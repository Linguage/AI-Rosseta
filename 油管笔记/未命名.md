


### 讲座介绍

在这场深度思考的演讲中，Ilya Sutskever探讨了无监督学习这一人工智能领域核心挑战的理论基础。与监督学习相对成熟的数学框架不同，无监督学习的内在机制和成功保证长期以来显得较为模糊。Sutskever提出，理解无监督学习的关键可能在于“压缩”这一概念。

他首先回顾了监督学习的成功要素，并指出了无监督学习在缺乏明确指导信号时面临的根本困惑。随后，他介绍了一种早期的、有理论保证的无监督学习方法——分布匹配，并承认其在实际应用中的局限性。演讲的核心论点是，一个理想的压缩器，特别是理论上的Kolmogorov压缩器，能够通过联合压缩未标记数据和有标记（或目标）数据，揭示它们之间的共享结构，从而实现有效的无监督学习。Sutskever进一步将这一高度抽象的理论与现实中的大型神经网络联系起来，认为随机梯度下降（SGD）在某种程度上是在进行程序搜索，而大型模型则是在逼近这种理想的压缩能力。

为了验证这一思想，他展示了OpenAI在2020年关于iGPT的工作，证明了在视觉领域通过“下一像素预测”（一种压缩形式）可以获得强大的无监督学习表征。尽管该理论为无监督学习提供了一个富有洞察力的视角，Sutskever也坦诚地讨论了其局限性，例如它并未完全解释线性表征的涌现，并对未来研究方向提出了展望。本次演讲为我们提供了一个重新审视和理解无监督学习本质的独特框架。

### 内容纲要

```
├── 引言与背景
│   ├── 演讲者与主题介绍 (Ilya Sutskever, 无监督学习的旧成果)
│   └── 当前研究重点 (AI对齐)
├── 学习理论基础回顾
│   ├── 学习的本质与可行性
│   └── 监督学习 (Supervised Learning)
│       ├── 定义与数学保证 (PAC学习, 统计学习理论)
│       ├── 成功条件 (低训练损失, 自由度 < 训练集大小, 同分布假设)
│       └── 对VC维度的评论 (处理无限精度参数, 实际有限精度)
├── 无监督学习的挑战与困惑
│   ├── 定义与目标 (发现隐藏结构)
│   ├── 缺乏类似监督学习的保证
│   └── 核心困惑 (优化一个目标，关心另一个；均匀分布下的失效)
├── 一种有保证的无监督学习方法：分布匹配 (Distribution Matching)
│   ├── 核心思想 (函数f使得F(X)分布 ≈ Y分布)
│   ├── 应用示例 (机器翻译, 密码破译)
│   └── 局限性 (设置理想化)
├── 无监督学习的核心思想：压缩 (Compression)
│   ├── 压缩与预测的等价性
│   ├── 核心思想实验 (联合压缩X和Y, $C(X \text{concat} Y)$ vs $C(X) + C(Y)$)
│   │   └── 共享结构/算法互信息
│   └── 通过压缩实现无监督学习的形式化：遗憾 (Regret)
│       └── 低遗憾意味着充分利用未标记数据
├── Kolmogorov复杂性：终极压缩器
│   ├── 定义 ($K(X)$ - 输出X的最短程序长度)
│   ├── 作为最佳压缩器的特性 (模拟其他压缩器, 不可计算性)
│   └── 与神经网络的联系 (SGD作为程序搜索, NN作为微型Kolmogorov压缩器近似)
├── 条件Kolmogorov复杂性：无监督学习的理论解
│   ├── 定义 ($K(Y|X)$ - 给定X输出Y的最短程序长度)
│   └── 作为无监督学习的解决方案 (终极低遗憾, 不可计算)
├── 联合压缩与条件压缩的等价性
│   ├── 应用条件Kolmogorov复杂性的挑战 (对大数据集条件化困难)
│   ├── 重要结果 ($K(X,Y)$ 在预测Y上 ≈ $K(Y|X)$)
│   └── 最大似然估计与联合压缩 (自然契合机器学习)
├── 理论的实证验证：iGPT
│   ├── GPT模型与压缩理论的关系 (可解释, 但非唯一解释)
│   └── 在视觉领域寻求直接验证 (iGPT, 2020)
│       ├── 方法 (图像转像素序列, Transformer进行下一像素预测)
│       └── 结果 (CIFAR-10, ImageNet上的表现, 显示可扩展性)
├── 关于线性表征的思考
│   ├── 压缩理论的局限性 (不直接解释线性可分性)
│   ├── 线性表征的普遍性
│   └── 观察与推测 (自回归模型 vs BERT, 长程结构的重要性)
└── 问答环节要点
    ├── 关于推测的详细说明 (下一像素预测 vs BERT掩码预测)
    ├── 鲁棒的二维下一像素预测 (扩散模型等概率模型)
    ├── Kolmogorov复杂性与神经网络训练动态的差异
    ├── 密码学中分布区分、预测与压缩的联系
    ├── VC维度的辩护 (Scott Aaronson的例子)
    ├── Transformer/SGD作为最佳压缩程序的理解 (数据集总损失)
    ├── 压缩框架、iGPT与线性表征的关系 (线性表征是额外好处)
    ├── 无监督理论对监督学习的启示 (函数类别, 忽略计算成本)
    ├── 自回归建模的重要性 (与其他最大似然方法比较)
    ├── GPT-4作为压缩器与压缩器自身大小的问题 (固定数据集 vs 无限测试集)
    ├── 使用gzip和k-NN进行文本分类的讨论 (gzip压缩能力有限)
    └── 课程学习效应 (与架构优化难度相关)
```


---

# Ilya Sutskever关于大型语言模型（LLM）与未来的演讲

## 一、 引言与背景

YEJIN CHOI: 它的引用次数达到了六位数，超过了139,000次，甚至更多。所以非常期待听到Ilya关于LLM和未来的分享。交给你了。

ILYA SUTSKEVER: 好的。大家好。[掌声]
大家好。感谢介绍。当Umesh邀请我参加这个活动时，我感到非常兴奋。我看到了所有的演讲者名单，心想，太好了，我会去讲点什么。然后我遇到了一个问题，那就是我们在OpenAI做的很多技术工作，我其实不能公开谈论。[笑声] 所以我真的绞尽脑汁思考我能讲些什么。

就在不久前，我把所有的研究重心都转移到了AI对齐（AI alignment）上。我们在这方面会有非常酷的结果展示，但还不是现在。所以我想，这部分内容就得留到下次演讲了。那么这次演讲讲什么好呢？我想到了一个点子。

我会告诉你们一些我们在OpenAI很多年前，甚至早在2016年，得到的一些非常古老的结果，这些结果确实影响了我对无监督学习的思考方式。我想和大家分享一下。很可能在座的各位会觉得这些内容显而易见，但也可能不全是。所以至少有那么一点点可能，你们会觉得它有趣。所以我想先设定一个适度的期望，然后希望能超出它。

那么，一个关于无监督学习的理论。

## 二、 学习理论基础回顾

这样的理论怎么可能存在呢？

在我们讨论无监督学习之前，我们想先谈谈一般的学习。什么是学习？学习为什么会起作用？计算机为什么能学习？我们现在已经习惯了这个事实，认为神经网络学习是理所当然的。但它们为什么能学习呢？从数学上讲，它们为什么应该能学习？为什么数据会具有我们的机器学习模型能够捕捉到的规律性？这不是一个显而易见的问题。

机器学习领域多年前由多人取得并形式化的一个重要概念进展是监督学习的发现。它通常被称为PAC学习（Probably Approximately Correct learning）或统计学习理论。监督学习的好处在于它给出了一个精确的数学条件，在此条件下学习必定成功。

你被告知，如果你有一些来自某个数据分布的数据，如果你设法达到了低的训练损失，并且你的自由度数量小于你的训练集大小，那么你将达到低的测试误差。并且你被保证能做到这一点。所以你有一个数学条件，你可以说，如果我确实从我的函数类中找到了一个函数，它达到了低的训练误差，那么学习就会成功。

你可以说，是的，这是一个我们可以推理的非常合理的数学事物。这就是为什么监督学习是容易的。然后你有了所有这些我认为很简单的定理。我发现它们很优雅。你们可能见过这样的定理。基本上，情况是这样的，如果你知道它，你会说，哦，是的，那个东西。如果你不知道，就不可能在30秒内解释清楚。不过，5分钟内是可以解释清楚的，只是30秒不行。这些东西都不复杂。

你有一个简短的证明，它说，如果你的函数类中有一定数量的函数，那么你的训练误差与测试误差对于至少一个函数而言相差很大的概率，这里有一些数学。数学很简单，这就是全部的数学——三行数学。所以三行数学就可以证明整个监督学习。嗯，这很好。这非常好。所以监督学习是相对而言被很好理解的。我们知道它为什么必须成功。所以我们可以继续收集大规模的监督学习数据集，并完全确定模型会持续变得更好。这就是那里的情况。

是的，我忘记提及这些结果中一个非常重要的部分。测试分布和训练分布需要相同。如果它们相同，那么你的监督学习理论就会生效并成功。所以从概念上讲，它是平凡的。我们对于为什么监督学习有效，为什么语音识别应该有效，为什么图像分类应该有效，都有了答案，因为它们都归结为监督学习，而监督学习是有效的，它有这种数学保证。所以这非常好。

这里我想对VC维度做一个小小的旁注，给那些关心这类事情的人。可能会有一个小子集。所以如果你想在接下来的30秒内走神，请随意。

很多关于统计学习理论的著述都强调VC维度是一个关键组成部分。但VC维度——实际上，VC维度被发明的唯一原因是为了让我们能够处理具有无限精度的参数。VC维度被发明出来是为了处理具有无限精度的参数。所以如果你有一个线性分类器，每个参数都是无限精度的。但是，当然，在现实中，我们所有的浮点数都是有限精度的，而且它们的精度还在缩小。所以你可以——所以你的——所以计算机实现的函数数量实际上是少的。你可以把它简化回那个小公式，它几乎能给你从监督学习中能得到的最好的界限。所以我发现这很酷。我发现这很有吸引力，因为你需要的证明行数更少。

## 三、 无监督学习的挑战与困惑

现在，我们来谈谈无监督学习。

首先，什么是无监督学习？它是什么？监督学习就像是，是的，你有了数据，数据告诉你，这里做这个，那里做那个，你拥有所有这些数据。你在训练误差上做得很好，你的训练误差很低。你的训练数据比函数类中的自由度（参数）要多。可能还有其他东西——比如比特这种自由度。然后你说，你的监督学习会成功。

但是什么是无监督学习？关于无监督学习，你能说些什么呢？我要说的是，至少我没有见过一个让我满意的关于无监督学习的阐述。如何从数学上推理它？我们可以直观地推理，但我们能从数学上推理吗？

作为一些背景，无监督学习的旧梦想是什么？顺便说一句，这个梦想已经实现了，但它是经验性地实现的。我们能否稍微超越经验结果一点点？比如，你只是看着图像，或者只是看着文本，而没有被告知如何处理它们，但不知何怎地，你仍然发现了数据中存在的真实隐藏结构，并且它以某种方式帮助了你。

但为什么会发生这种事呢？它应该发生吗？我们应该期待它发生吗？你没有任何与监督学习保证类似的东西。监督学习的保证说，是的，获得低的训练误差，你就能实现学习。这将是一个巨大的成功。所以无监督学习，看起来——看起来不是这样的。人们谈论它已经很久了，在80年代。玻尔兹曼机已经在谈论无监督学习了。无监督学习在小规模上也行不通，但所有的想法都在那里，比如去噪自动编码器，对于那些还记得它的人来说。它就像BERT或者扩散模型。在其中，它是一个微小的转折，最小的转折。旧时代的语言模型，它们也为它们那个时代生成了很酷的样本，但它们的无监督学习表现不像今天的那么令人印象深刻。

但我想说明的是，这令人困惑，因为——为什么令人困惑呢？你优化。无监督学习是如何工作的？你说，让我们优化某种重构误差，或者让我们优化某种去噪误差，或者某种自监督学习误差。你优化一个目标。哦，是的，我刚才说了。但你关心的是另一个不同的目标。那么这是否意味着你没有理由期望你会得到任何好的无监督学习结果，或者说你确实在经验上得到了它们，但是，它会是——我认为，神秘程度相当高。这似乎是一个完全无法触及的现象。你优化一个目标，但你关心的是另一个目标。然而它却有帮助。这怎么可能呢？魔法。

顺便说一句，另一件事是，无监督——好吧，我想我要说一些只有90%正确的话。无监督学习不是——你可以说，好吧，你只是学习了输入分布中的结构，它帮助了你。但是，如果你从均匀分布中训练呢？那么你所有的无监督学习算法都会失败。

你应该如何思考这个问题呢？那么我们能说些什么呢？我们需要做假设吗？什么样的假设？所以我想提出一种思考无监督学习的方式，我认为，我个人觉得它很有趣。也许你们也会觉得有趣。让我们拭目以待。

## 四、 一种有保证的无监督学习方法：分布匹配（Distribution Matching）

我想向你们展示一种进行无监督学习的方法，这种方法不一定广为人知，因为它从未成为主流的无监督学习方法。但它有一个很酷的特点，与监督学习类似，它必须有效。

那么，什么样的神秘无监督学习过程，在你没有给任何输入打上任何标签的情况下，仍然保证有效呢？

分布匹配，分布匹配。

那么什么是分布匹配呢？假设我有一些数据。我有 $X$ 和 $Y$，两个数据源。它们之间没有对应关系。我只有两个数据源，数据源 $X$ 和数据源 $Y$，比如语言一和语言二，文本和语音——它们之间没有对应关系。

让我们看看这个标准。找到一个函数 $f$，使得 $F(X)$ 的分布与 $Y$ 的分布相似。这是对 $F$ 的一个约束。在机器翻译和语音识别的例子中，例如，这个约束可能是有意义的。

你可以说，是的，如果我有长句子——如果你告诉我你有一个函数，使得你取英语句子的分布，对它们应用函数 $F$，然后你得到的东西与法语句子的分布非常相似，你可以说，好吧，我找到了关于 $F$ 的真实约束。如果 $X$ 的维度和 $Y$ 的维度足够高，那将会给你很多约束。事实上，你可能几乎可以从这些信息中完全恢复 $F$。

这是无监督学习的一个例子——它仍然保证有效，与监督学习保证有效的意义相同。

此外，替换密码，比如那些简单的小加密，也适用于这个框架。就是这样。所以我遇到了这个。我在2015年独立发现了这一点。我对此非常着迷，因为我想，哇，也许关于无监督学习，我们可以说一些有意义的、数学上有意义的东西。

但是让我们看看。这个设置的问题在于它仍然有点人为。它仍然——真实的机器学习设置不是这样的。而且我们喜欢思考无监督学习的方式也不是那样。

## 五、 无监督学习的核心思想：压缩（Compression）

现在我将向你们展示我要说的核心内容——一种思考无监督学习的建议方式，它让你——它将无监督学习与监督学习置于同等地位。那么好吧，它在数学上做了什么？你怎么能确定你的无监督学习是好的呢？压缩来拯救。

显然，众所周知——我不应该说显然。这并不明显，但众所周知，压缩即预测。每个压缩器都可以是一个预测器，反之亦然。所有压缩器和所有预测器之间存在一一对应的关系。然而，我认为，为了思考无监督学习，压缩的语言提供了一些优势，至少对我来说是这样。也许对你也会。

那么，考虑以下思想实验。

这个思想实验是最重要的幻灯片。假设你有两个数据集，$X$ 和 $Y$。你有两个数据集，在你巨大的硬盘上有两个文件。假设你有一个非常棒的压缩算法 $C$，它接收数据，然后输出压缩后的对象。假设你联合压缩 $X$ 和 $Y$。你将它们连接起来。你把这两个数据集连接起来，然后把它们喂给你的压缩器。

会发生什么呢？嗯，让我们看看。特别地，一个重要的问题是，一个足够好的压缩器会做什么？我的答案是，非常直观地，它会利用存在于 $X$ 内部的模式来帮助压缩 $Y$，反之亦然。你可以对预测提出同样的主张，但不知为何，当你说到压缩时，它更直观。我不知道为什么会这样，但我发现确实如此。

所以这是我们的线索。你可以写出这样的等式，你说，嘿，如果你的压缩足够好，如果它是一个真正伟大的压缩器，它应该表明，你连接后的巨大文件的压缩效果，不应该比你分别压缩两个文件差。所以通过连接获得的任何额外压缩都是你的压缩器注意到的某种共享结构。你的压缩器越好，它提取的共享结构就越多。这个差距就是共享结构，或者说算法互信息。

这很有趣，对吧？你可以看到我指的是什么。$Y$ 是你监督任务的数据。$X$ 是你的无监督任务。但突然之间，你有某种数学上的理由，让 $X$ 中的信息模式去帮助 $Y$。

还要注意它是如何推广分布匹配的。如果我们处于分布匹配的情况，其中你的 $X$ 是语言一，而 $Y$ 是语言二，并且你说——并且你知道存在某个简单的函数 $F$ 可以将一个分布转换为另一个分布，那么，你的压缩器，如果它足够好，肯定会注意到这一点，并加以利用，甚至可能在内部尝试恢复这个函数。

我认为这非常酷。我们形成了一个闭环。

那么我们如何将其形式化呢？无监督学习的形式化会是什么样的？让我们看看。让我们看看我能否做到。考虑一个机器学习算法。

顺便说一句，在接下来的内容中，我会有点不严谨，我会交替使用压缩和预测。假设你有一个机器学习算法 $A$。这是一个算法 $A$，它试图压缩 $Y$。并且假设它可以访问 $X$。$X$ 是文件一，$Y$ 是文件二。你希望你的机器学习算法，你的压缩器，去压缩 $Y$，并且它可以根据需要探测 $X$。目标是尽可能好地压缩 $Y$。

我们可以问自己，使用这个特定算法的遗憾是什么？你会明白我的意思。相对于什么而言的遗憾？如果我做得足够好，如果我拥有——低遗憾意味着我已经从这些未标记数据中获得了所有可能获得的帮助。这些未标记数据已经尽可能多地帮助了我。我对此感觉良好。我不觉得我把一些预测价值留在了桌面上，而其他人用更好的压缩算法本可以利用它。这就是它的意思。

特别是，就像，是的，你可以安心睡觉了，知道如果未标记数据中有一些信息可以帮助我的任务，没有人——没有人能比我做得更好。我在从我的未标记数据中获益方面做得最好了。所以我认为这是朝着思考无监督学习迈出的一步。你不知道你的无监督数据集是否真的有用。它可能超级有用。它可能包含答案。它也可能完全没用。它可能是均匀分布。但是如果你有一个低遗憾的无监督学习算法，你可以说，无论是第一种情况还是第二种情况，我知道我已经尽力了。我知道我已经尽力从我的未标记数据中获益了。没有人能比我做得更好。

## 六、 Kolmogorov复杂性：终极压缩器

现在，我想绕道去理论领域，这个领域有点晦涩。但我认为它很有趣。Kolmogorov复杂性作为终极压缩器，为我们提供了终极的低遗憾算法，尽管它实际上不是一个算法，因为它是不可计算的。但你很快就会明白我的意思。

所以Kolmogorov——首先，作为一些背景，这里有多少人熟悉Kolmogorov复杂性？好的，大约50%。Kolmogorov复杂性是那种很容易在一分钟内解释清楚的东西，所以我就这么做。这就像，想象一下，如果我给你一些数据，或者你给我一些数据。然后我会通过给你可能存在的最短程序来压缩它，那个存在的、如果你运行它就能输出你的数据的最短程序。

观众：这里有个笔误。[听不清] 不是Y，应该是X。
ILYA SUTSKEVER: 是的，你说得对。你逮到我了。[笑声]
它是输出 $X$ 的最短程序的长度。是的。

直观上，你可以看到这个压缩器相当好，因为你可以证明这个定理，这个定理也真的很容易证明，或者说，很容易感受到它。一旦你感受到了，你就能在某种程度上相信我它很容易证明。你基本上可以说，你的Kolmogorov压缩器，如果你用它来压缩你的字符串，你对你的压缩质量会有非常低的遗憾。

你可以证明这个结果。它说，如果你有一个字符串 $X$，你的数据集，数据库，无论什么，输出 $X$ 的最短程序比你的压缩器需要输出的任何东西都要短，也比你的压缩器压缩数据的好坏程度要短，再加上一个小项，这个小项是你实现压缩器所需的代码字符数。直观地，你可以看到这如何说得通，这是一个模拟论证。模拟论证，如果你告诉我，嘿，我有一个非常棒的压缩器 $C$，我会说，酷，它有计算机程序吗？你能把这个计算机程序给 $K$ 吗？$K$ 会运行你的压缩器。因为它运行计算机程序，你只需要为程序长度付费。所以，在不给你细节的情况下，我想我已经让你感受到了。Kolmogorov复杂性，Kolmogorov压缩器可以模拟计算机程序如何模拟其他压缩器。这也是它不可计算的原因。它不可计算，因为它模拟——它感觉非常自由地去模拟所有的计算机程序。

但它是现存最好的压缩器。我们之前谈论的是用于无监督学习的良好压缩。

现在，让我们推广Kolmogorov复杂性，一个Kolmogorov压缩器，使其能够使用旁路信息。

我会更详细地谈论。所以我会详细说明这一点。我会多次重申这一点，因为这一点很重要。显然，Kolmogorov压缩器是不可计算的。它是不可判定的。但是，嘿，因为它像是搜索所有程序，你知道吗，如果你在一个有100层的神经网络的参数上运行SGD，它实际上就像是在一台具有一定内存量、一定步骤数的计算机上进行程序搜索。这有点像微型微型微型同态加密，像是拟合一个神经网络。你有点能感受到那种感觉，那种相似性。

这有点神奇，对吧？神经网络可以模拟真实的程序。它们是小型计算机。它们是电路。电路是计算机，是计算机器。而SGD则在程序中搜索。所有深度学习都依赖于SGD这个奇迹，我们实际上可以用SGD训练这些计算机。这行得通。我们实际上从数据中找到了那些电路。因此，我们可以计算我们的微型Kolmogorov压缩器。顺便说一句，一个同化论证也适用于此，我只想提一下这个事实。我不知道你是否——如果你曾经尝试设计一个更好的神经网络架构，你会发现要找到一个更好的神经网络架构是相当困难的。你说，好吧，让我们添加这个连接。让我们添加那个连接。然后让我们修改这个和那个。为什么这么难？模拟论证，因为你的新架构可以被你的旧架构非常直接地模拟。除非它不能。那些是罕见的情况。在那些罕见的情况下，你会有很大的改进，比如当你从小型RNN切换到Transformer时。RNN有一个瓶颈，即隐藏状态。所以它很难实现Transformer。然而，如果我们找到一种方法来设计一个非常非常大的隐藏状态，也许它会再次变得和Transformer一样好。所以这就是联系。

你开始看到我们是如何从形式化的领域转换到神经网络的领域的。但你看到了相似之处。

## 七、 条件Kolmogorov复杂性：无监督学习的理论解

条件Kolmogorov复杂性作为无监督学习的解决方案——你基本上可以有一个类似的定理，我不会定义——嗯，我会定义什么是 $K(Y \text{ given } X)$。它就像是输出 $Y$ 的最短程序，如果它被允许探测 $X$ 的话。这是那个输出 $Y$ 的最短程序的长度，如果它允许探测 $X$。你可以证明同样的结果。你可以立即看到，是的，这在定义上就是无监督学习的解决方案。如果你使用它，你可以非常非常安心地睡觉——晚上睡得很香，知道没有人比你做得更好的无监督学习。

字面上就是这样。所以这是无监督学习的终极低遗憾解决方案，只是它不可计算。但我确实认为这是一个有用的框架。这里我们以数据集为条件，而不是单个样本。这个东西会从 $X$ 中提取所有价值来预测 $Y$，是数据集，数据集，而不是样本。所以这就是无监督学习的解决方案——完成了，成功了。

## 八、 联合压缩与条件压缩的等价性

这里有一个小小的技术细节，我需要花点时间谈谈，那就是我们之前谈论的条件Kolmogorov复杂性，你说的是那些能够看到，尝试压缩一件事物，同时又能访问另一件事物的压缩器。这在机器学习的背景下有点不自然，至少在今天，当你关心的是输入大型数据集时。虽然情况变化得相当快。但我认为公平地说，目前还没有真正好的方法来对大型数据集进行条件化。你可以拟合一个大型数据集，但你不能对它进行条件化，至少现在还不能，不是真正意义上的。

所以这个结果表明，嘿，如果你关心的是对你的监督任务 $Y$ 进行预测，使用老式的Kolmogorov压缩器，它只是压缩 $X$ 和 $Y$ 的串联，其效果将与使用你的条件压缩器一样好。我刚才说的内容还有更多细节和一些微妙之处。如果有人感兴趣，我很乐意在线下讨论它们，但这基本上是说，如果之前你说，嘿——上一张幻灯片说你可以使用这个条件Kolmogorov压缩器来解决无监督学习。这张幻灯片说你也可以使用你的常规Kolmogorov压缩器。只需把你所有的数据都放进去。把你所有的文件都拿出来。把它们连接起来。压缩它们。这会对你的监督任务，你关心的那个任务，做出一些很棒的预测。

关于为什么这是真的，有一些直觉。这个结果实际上——证明它稍微有点棘手，所以我不会去做。

所以无监督学习的解决方案，就是把它全部交给你的Kolmogorov复杂性——你的Kolmogorov压缩器。

最后一点，我想提一下，这种联合压缩在不过拟合的情况下就是最大似然。如果你有一个数据集，那么给定参数下似然的总和，就是压缩该数据集的成本。你还需要支付压缩参数的成本，但你可以看到，如果你现在想压缩两个数据集，没问题。只需向你的训练集、你的数据集中添加更多的点，并在总和中添加相应的项。所以这种串联，这种联合压缩，在机器学习的背景下非常自然，这就是为什么值得费力地说，是的，我们有条件Kolmogorov复杂性。然后我提出了一些论点。我提出了一些主张，但没有充分辩护，但是可以辩护，但使用常规的——只是压缩一切。Kolmogorov复杂性也有效。

所以我认为这很优雅。我喜欢它，因为它表明，嗯，如果你使劲眯着眼睛看，你可以说这解释了我们的神经网络在做什么。你可以说，嘿，大型神经网络上的SGD是我们的大型程序搜索。更大的神经网络越来越好地逼近Kolmogorov压缩器。所以也许这也是我们喜欢大型神经网络的原因，因为我们正在接近Kolmogorov压缩器那个遥不可及的理想，它真正没有遗憾，而我们只是想在训练越来越大的神经网络时，在提取预测价值方面，拥有越来越少的遗憾。

## 九、 理论的实证验证：iGPT

现在，这种理论如何应用于GPT模型——你知道，我认为这个理论也适用于GPT模型。但对于GPT模型来说，有点棘手的是，它们的行为理论也可以在不提及压缩或无监督学习的情况下得到解释。你只需说，不，这只是互联网上文本的条件分布。少样本学习（Few-shot learning），想象一个包含一些重复模式的文档，这种模式很可能会继续下去。所以GPT模型的行为，至少它们的少样本行为，肯定可以在不借助于这个理论的情况下得到直观解释。所以我想，如果我们能找到这个理论的其他直接验证，那会很好。

现在，我们能否找到一个不同的领域，比如视觉？因为视觉，你有像素。你能否证明在像素上这样做会导致好的无监督学习？答案是，是的，你可以。这是我们在2020年做的工作。我们称之为iGPT。这是一个昂贵的概念验证。它并非旨在——它并非旨在成为一种实用的程序。它旨在成为一篇论文，表明如果你有一个非常好的下一步预测器，你将会做出很棒的无监督学习。并且它在图像领域得到了证明。

在那里，我将详细说明。你有一张图像。你把它转换成一个像素序列。每个像素都应该被赋予一些离散的强度值。然后就做下一像素预测。使用相同的Transformer。就是这样——不同于BERT。只是下一词元预测（next token prediction），因为这最大化了似然，因此进行了压缩。

就像我们看到的，我们立即看到的一个结果——这些是CIFAR-10上的结果。你有不同大小的模型。这是它们在像素预测任务上，即它们的无监督学习任务上的下一步预测准确率。这是线性探针准确率（linear probe accuracy），线性探针，当你选择神经网络内部的某个层，最好的层，拟合一个线性分类器，然后看看它的表现如何。你得到了这些漂亮的曲线，它们开始变得越来越相似。这正是你想要的。就像它在起作用。与下一词预测、下一——不仅仅是像素预测，而是下一像素预测同类型的像素预测，会导致越来越好的无监督学习。我认为这非常酷。

我们尝试了——我们将其扩展到不同程度，并且我们表明，它确实学得相当好，我们接近了，但我们没有完全弥合我们的无监督学习与当时ImageNet上最好的无监督学习之间的差距。但很明显，它确实感觉像是可以扩展的，而且这只是计算资源的问题，因为那些东西使用大的、高分辨率的图像，而我们使用的是像$64 \times 64$这样的小图像，配上当时来说巨大的Transformer——以今天的标准来看很小，但在当时是巨大的，60亿参数。所以这就像，是的，你在一个大的图像数据集上进行无监督的下一像素预测，然后在ImageNet上拟合你的线性探针。你会得到很好的结果。就像，好吧，这很酷。

CIFAR，是的——我们得到的一个很酷的结果是，如果你用这个方法在CIFAR-10上得到99%的准确率，那很酷。2020年是不同的时代，但这很酷。

## 十、 关于线性表征的思考

我会以几个关于线性表征的评论来结束。

我喜欢压缩理论，因为我感觉它困扰了我很长时间，你无法严格地思考无监督学习。我认为现在你可以了，至少在某种程度上是这样。仍然需要很多手舞足蹈的比喻，但可能比以前少一些。但它并没有说明为什么你的表征应该是线性可分的。它并没有说线性[听不清]应该发生。但线性表征一直都在发生。其信息的原因必定是深刻而重要的。我认为我们可能能够在某个时候清晰地阐明它。

我觉得有趣的一件事是，这些下一像素预测模型，自回归模型，似乎比BERT具有更好的线性表征。蓝色的准确率是BERT对比自回归模型。我不确定为什么会这样，或者说我可以推测。我有一些推测。但我认为，如果能更深入地理解为什么，真正地理解为什么那些线性表征会形成，那会很好。

是的，这就是全部内容。感谢各位的聆听。
[掌声]

## 十一、 问答环节要点

问：您能详细说明一下那个推测吗？
答：是的，当然。我认为这个推测基本上是，如果你在做下一像素预测，你是从所有先前的像素来预测下一个像素。所以你需要观察长程结构。但是在BERT中，你有一个向量，假设你丢弃了25%的词元，或者说在这里是像素，那么你做的任何预测实际上都可以通过稍微看一下过去和稍微看一下未来就做得很好。所以你消除了所有困难的预测任务，或者使它们变得容易得多。下一像素预测中最难的预测任务比BERT预测情况中最难的预测任务要难得多。所以这是一个论点。如果我们尝试设计一个实验来测试它，我们甚至可能能够测试它。但是，是的，这就是那个推测。

问：下一像素预测是否有更鲁棒的二维版本？
答：我会说更鲁棒——我会说任何你做的，任何能将神经网络变成一个为不同输入分配概率的概率模型的事情，都是这样的。所以另一种进行下一词元预测的重要方法是扩散模型，扩散模型也——不是那些——人们在高质量图像生成器中使用的扩散模型并不真正最大化其输入的似然。它们有不同的目标。但最原始的公式是最大化似然。顺便说一句，扩散模型是反驳——或者说，我认为扩散模型也应该比下一词元预测模型有更差的表征，原因和BERT一样。所以我感觉这进一步在我脑海中增加了导致线性表征形成的神秘感。

问：是的。谢谢您的演讲。我喜欢您在Kolmogorov复杂性和神经网络之间做的类比。我想有一个地方它们似乎不太像，那就是神经网络有训练动态。而如果你只是把所有的计算机程序都拿来，那更像是内存。所以如果你用Kolmogorov复杂性进行压缩，数据的顺序并不重要。但对神经网络来说显然重要。你在训练早期学到的一些简单启发式特征可能会一直保留到训练后期。所以，我想知道您对如何在您这个框架下思考这个问题有什么看法？
答：是的。我的意思是，这个类比并不完美。这是真的。你描述的这个类比最不成立的地方在于搜索过程。这正是你所指的，比如，嘿，数据顺序很重要。为什么？因为我们使用的是一个弱搜索过程。Kolmogorov压缩器使用的是无限昂贵的搜索过程，即每次都从头开始枚举所有东西。所以，是的，我的意思是——我的意思是——我想说的是，对于这个特定的类比应该谨慎使用。它很明显不是普遍适用的。但我确实认为，在这种情况下——我确实认为这里对于解释无监督学习的来源有一些价值。

问：您现在在压缩和下一比特预测之间来回切换您的形式化方法，您正在将无监督学习与监督学习联系起来。那么，如果您从密码学的角度回溯，有一种理论，可以追溯到80年代，他们谈论压缩等同于下一比特预测，等同于能够区分两种分布。所以如果您有一个[听不清]，那么您就有一个可以预测的算法——那么您就有一个可以压缩的算法。我的意思是，密码学是另一种方式。您是说没有办法压缩[听不清]。因此，没有办法[听不清]。所以我想知道，这种能够区分的想法，如何能自然地转化为任何东西呢？
答：我想我明白你的问题。让我复述一下。我对你提到的领域不是很精通。但你是说，如果你能区分两种分布，你就能利用这种可区分性来进行一些预测。我想问题是，我有一些——
观众：你有一组来自一个分布的样本，另一组来自另一个分布的样本，你能区分它们[听不清]。
答：所以我想我的问题是，你能预测得多好——或者你指的是你能预测一点点？你指的是这个吗？
观众：是的，[听不清]。你可以预测下一个比特会是什么，比如说，在一个样本中。
答：所以——
观众：实际上，[听不清]，[听不清]你可以预测那些。
答：我的意思是，我可以提到一件相关的事情，那就是能量模型（energy-based models）。能量模型提供了另一种将神经网络转化为概率分布的方法，能量模型会说，只需给我你的向量配置，我就会告诉你它的感觉如何。然后你对所有这些进行归一化。我感觉，特别是对于能量模型而言，分布的比率对应于能量的差异。也许这与你说的有些关系。我想我可能没有精确地评论你所说的事情，但不幸的是，我没有更多要补充的了。

问：Ilya，也许我可以为VC维度的荣誉稍微辩护一下。
答：请讲。
观众：嗯，在2007年，我有一个关于量子态PAC学习的定理。这恰好是一个例子，即使你不关心无限精度，即使你离散化了状态空间，比如你看VC维度，或者fat shattering维度之类的，它也比假设类别大小的对数小指数级别。所以我确实认为有些情况下你仍然需要VC维度。
答：那是个很酷的例子。[笑声]

问：我没有完全理解您的符号。大写X是一个来自某个分布的样本吗？
答：是的。
观众：那些分布——
答：是数据集。
观众：那么Transformer，SGD——我不确定你是否能把它看作是压缩[听不清]的最佳程序。也许因为它一次只得到一个样本。
答：这是真的。我们还做出了另一个假设。如果你看一下总的训练损失——假设你有一些神经网络，它不一定是Transformer——一些为你的数据分配对数概率的神经网络。你有很多训练案例。你可以运行这个神经网络。你需要一些神经网络。你可以计算它在每个案例上的对数概率。然后你可以把它们加起来。这就是神经网络分配给整个数据集的对数概率。现在，神经网络无法——这种特殊的表述使得神经网络无法明确注意到数据顺序中可能存在的时序结构或任何类型的结构。但我仍然认为，说你可以计算整个数据集的对数概率是有意义的。这给了你负对数概率，也就是用这个神经网络作为压缩器来压缩这个数据集所需的比特数。

问：如果我可以复述一下，我认为您主张将压缩作为一个框架来理解或推动无监督学习。您在最后提出的一个观点是，如果将该框架应用于语言模型，应用于下一词预测，可能会感觉有点肤浅，因为下一词预测，任何文本任务都可以转换为下一词预测。因此，对于文本而言，无监督学习在表面上与监督学习是相同的。于是我们转向了iGPT，在iGPT中，您不能将任何给定的任务表述为下一像素预测。或许您可以，但我们就假设您不能。但是随后您可以使用线性表征作为一种方式来表明压缩是构建无监督学习的好方法。但是，存在一些高效的压缩器，它们并不会给您一个有用的线性表征。所以我想知道，是否存在某些情况，其中无监督学习和监督学习在表面上并不相同，[听不清]，但是您不需要您的压缩器给您一个有效的线性表征来证明压缩是一个好的无监督目标？
答：是的。这是一个非常——我对这个问题有想法。首先，我会说线性表征，一个好的线性表征是一个额外的好处。这个论点在任何时候都没有说，因此线性表征应该出现。然而，我认为这个理论确实表明好的微调（fine-tuning）应该出现，因为联合压缩有点像使用SGD这个糟糕的搜索算法进行的粗略的近似微调。现在，我们从这些旧实验的证据中知道，当在图像上运行时，BERT学习到的线性表征比下一像素预测学习到的要差。也许对于扩散模型也是如此。这似乎很有可能。我认为看看微调后的扩散模型如何比较会非常有趣。也许这里已经有人知道了。

问：所以我想知道您是否能将您关于无监督学习的想法带回来。在我看来，您似乎可以从中获得一些对监督学习[听不清]的见解。
答：嗯，我认为这能让你更深入地理解——所以你可能在这里得到的特定见解是关于函数类别，期望的函数类别。你希望你的神经网络有很多层。例如，如果你有很多层，它可以进行更多的思考。你希望那些层更宽，神经网络更大。我的意思是，基本上，这就是这个领域已经发现的东西。
观众：但是你可能不需要比[听不清]数量更多的样本。在监督学习中，这可能解释了为什么你不一定需要更多的参数。
答：没错。所以这可能，例如，是——所以这个理论的弱点——顺便说一句，这个理论有一个巨大的实践弱点，那就是它忽略了计算成本。它只关注信息。所以你们都熟悉通用Transformer，也许——基本上，它就像一个Transformer。我们在每一层都使用相同的权重。这是一个好主意，只是如果你想拥有很多参数，你需要为此付出大量的计算成本。没人想那么做。你不能忽略计算成本。这个理论忽略了计算成本。如果你忽略计算成本，它会给你一个如何进行的方案。

问：您认为自回归建模在概率分布的特定建模中有多重要？您可以将BERT视为在进行最大似然训练。我想您可以找到从BERT中采样的方法，例如。
答：所以某些扩散模型可以被设置为最大似然模型。因此，所有这些理论都预测，扩散模型也应该——应该有可能使扩散模型做出同样伟大的事情，也许只是有一些常数因子的差异——因为这个——正如之前的回答，这不是一个对计算敏感的理论。所以它会说，好吧，也许你需要大约10或15倍的计算量，然后自回归模型和扩散模型之间的事情就会一样了。自回归模型，它只是简单，方便。也许能量模型会做出更伟大的事情。但从那个角度来看，它们都是一样的。

问：看起来GPT-4目前可能是最好的压缩器，它大概也是目前最大的模型。所以一方面，它能够更好地压缩。另一方面，压缩器本身的大小也在增加。或者说，从[听不清]复杂性理论的角度来看，情况并非必然如此？
答：嗯，我的意思是重要的是——你真正想要的是——这个理论与现实不符的另一种方式是，这个理论说，你有一个固定的数据集想要压缩。但是在我们训练GPT模型的方式中，你说，你有一个大的训练集，然后你有一个测试集，这个测试集被假定为无限大的。如果测试集是无限的，并且你关心压缩测试集，那么你就不关心压缩器的大小，只要测试集的大小远大于它。所以我认为这也是，我想说，一个不相似之处，在这里需要更仔细的思考才能获得清晰的认识。

问：只要验证集是独立的，分开的，就足够了吗？那样能弥合差距吗？
答：足够吗？这是件好事。这样做是件好事。它是否真的——问题在于我认为它也是——实际上，在单轮训练（single epoch）的情况下，如果你处于单轮训练的情况下，那么你可以在训练时计算你的对数概率，那将类似于模型对数据集的压缩。这也为你提供了对验证集的一个良好估计。所以在单轮训练的情况下，我认为情况非常相似。

问：我只是想让您知道，上个月在[听不清]上有一篇论文。它使用了gzip，然后是k最近邻分类器进行文本分类。所以更多的训练，更多的参数。它是压缩字符串，就像您展示的那样，将两个字符串连接在一起[听不清]分别地，然后计算距离。有一种方法是使用gzip的输出作为[听不清]编码[听不清]。
答：是的。我对此唯一的评论是，gzip对于文本来说不是一个非常强大的压缩器。所以我认为它确实表明在某种程度上事情是可能的，但是所有真正有实质性的东西都发生在你挤压最后那些比特的时候，如果你明白我的意思。
观众：它看起来像一个玩具般的概念验证，但它确实有效。
观众：我想之前那里有一些bug——
答：它不起作用。
观众：[听不清]目前的结果。

问：既然Jacob提到了课程效应（curriculum effects），以及它们如何在神经网络中出现，但对于Kolmogorov复杂性你却看不到。我们午餐时还在讨论，关于课程效应实际有多重要的当前经验情况是什么。我们都不知道答案。
答：嗯，这有点——我对这个有自己的看法，那就是越容易——我们作为一个领域做了很多工作来使Transformer，比如我们正在使用的架构，相对容易优化。我们找到了好的初始化方法。我们找到了好的超参数。我们对架构进行了一些更改，以便训练尽可能容易。训练优化问题越容易，你就越不容易受到课程效应的影响。这是已知的。例如，那些训练各种奇特架构的人，比如神经图灵机，它们是非常复杂的东西。而且有超级超级多的异构层，它们是不同的。对于那种东西，你需要对你的课程非常非常小心，因为如果你马上给它完整的数据集，它就会失败。

YEJIN CHOI: 好的，让我们再次感谢Ilya。
[掌声]
[场外交流声]





---

# 要点回顾

**一、 引言与背景**
- Ilya Sutskever被介绍，提及他在OpenAI的工作。
- Ilya表示由于OpenAI的技术工作很多无法公开，他将分享一些关于无监督学习的旧成果（2016年左右），这些成果影响了他对无监督学习的看法。
- 他目前的研究重点已转向AI对齐（AI alignment）。

**二、 学习理论基础回顾**
- **学习的本质与可行性**
    - 提问：学习为什么会起作用？计算机为什么能学习？数据为何具有机器学习模型可以捕捉的规律性？
- **监督学习（Supervised Learning）**
    - 被认为是相对容易理解和解决的，有精确的数学条件保证其成功（PAC学习，统计学习理论）。
    - 核心保证：如果能在训练数据上实现低训练损失，并且模型的自由度（如参数数量）小于训练集大小，那么就能在测试集上获得低测试误差。
    - 数学证明简单，仅需几行数学公式。
    - 关键假设：测试分布和训练分布相同。
    - 应用实例：语音识别、图像分类等。
    - 对VC维度的评论：VC维度主要用于处理具有无限精度的参数；在计算机中，参数（如浮点数）精度有限，因此函数数量有限，可以用更简单的公式来分析。

**三、 无监督学习的挑战与困惑**
- **无监督学习的定义与目标**
    - 什么是无监督学习？它与监督学习有何不同？
    - 旧的梦想：在没有标签的情况下，仅通过观察数据（如图像、文本）来发现数据中隐藏的真实结构，并从中受益。这个梦想在经验上已实现，但缺乏理论解释。
- **缺乏类似监督学习的保证**
    - 无法提供与监督学习相似的成功保证。
- **核心困惑**
    - 优化一个目标（如重构误差、去噪误差、自监督学习误差），但关心的是另一个不同的目标。为什么这种优化会有帮助？
    - 如果从均匀分布中训练，无监督学习算法会失败。
    - 需要什么样的假设才能使其工作？

**四、 一种有保证的无监督学习方法：分布匹配（Distribution Matching）**
- 一种不广为人知但具有理论保证的无监督学习方法。
- **核心思想**
    - 给定两个数据源 $X$ 和 $Y$（无对应关系，如两种语言的文本）。
    - 寻找一个函数 $f$，使得 $F(X)$ 的分布与 $Y$ 的分布相似。
    - 这是一个对 $F$ 的约束。
- **应用示例**
    - 机器翻译：将英语句子分布转换为法语句子分布。
    - 密码破译（如替换密码）。
- 如果 $X$ 和 $Y$ 的维度足够高，这个约束会非常强，可能足以恢复 $F$。
- Ilya在2015年独立发现了这个想法，认为它为无监督学习的数学理解提供了可能。
- 局限性：这种设置在真实的机器学习场景中可能有些理想化或不自然。

**五、 无监督学习的核心思想：压缩（Compression）**
- **压缩与预测的等价性**
    - 压缩即预测，反之亦然。压缩的语言为理解无监督学习提供了优势。
- **核心思想实验（“最重要的幻灯片”）**
    - 假设有两个数据集（文件） $X$ 和 $Y$。
    - 使用一个非常好的压缩算法 $C$。
    - 联合压缩 $X$ 和 $Y$（将它们连接起来）。
    - 一个足够好的压缩器会利用 $X$内部的模式来帮助压缩 $Y$，反之亦然。
    - 数学表述：联合压缩的长度应不大于分别压缩 $X$ 和 $Y$ 的长度之和。 $C(X \text{concat} Y) \le C(X) + C(Y)$ (此为理想情况，实际是 $C(X \text{concat} Y)$ 与 $C(X) + C(Y)$ 之间的差值代表共享结构)。
    - 这个差值（压缩增益）代表了 $X$ 和 $Y$ 之间的共享结构或算法互信息。
    - $Y$ 可以是监督任务的数据，$X$ 可以是无监督任务的数据。这为 $X$ 中的模式帮助 $Y$ 提供了数学理由。
    - 这个框架也推广了分布匹配。

- **通过压缩实现无监督学习的形式化：遗憾（Regret）**
    - 考虑一个机器学习算法 $A$，它尝试在可以访问 $X$ 的情况下压缩 $Y$。
    - 目标是尽可能好地压缩 $Y$。
    - 使用特定算法的“遗憾”：低遗憾意味着该算法已从无标签数据 $X$ 中获得了所有可能的帮助。
    - 如果算法实现了低遗憾，意味着没有其他算法能更好地利用 $X$ 来预测/压缩 $Y$。

**六、 Kolmogorov复杂性：终极压缩器**
- **Kolmogorov复杂性 ($K(X)$)定义**
    - 输出字符串 $X$ 的最短程序的长度。 (幻灯片中曾有笔误，后更正)
- **作为最佳压缩器的特性**
    - 任何压缩器 $C$ 压缩 $X$ 的结果加上 $C$ 本身的程序长度，不会短于 $K(X)$。 $K(X) \le \text{length_of_C_compressed_X} + \text{length_of_C_program}$
    - 通过模拟论证：Kolmogorov压缩器可以模拟任何其他压缩器。
    - 不可计算性：因为它需要模拟所有可能的计算机程序。
- **与神经网络的联系**
    - 大型神经网络上的随机梯度下降（SGD）可以看作是在进行程序搜索。
    - 神经网络是小型计算机/电路，SGD在这些电路上搜索程序。
    - 深度学习依赖于SGD能够训练这些网络的“奇迹”。
    - 神经网络可以被视为Kolmogorov压缩器的微型、可计算的近似。
    - 模拟论证也解释了为什么改进神经网络架构通常很困难（除非新架构不能被旧架构轻易模拟，如RNN到Transformer的转变）。

**七、 条件Kolmogorov复杂性：无监督学习的理论解**
- **$K(Y|X)$ 定义**
    - 在给定 $X$ 的情况下，输出 $Y$ 的最短程序的长度。
- **作为无监督学习的解决方案**
    - $K(Y|X)$ 定义上是无监督学习的终极低遗憾解决方案。
    - 它能从 $X$ 中提取所有对预测 $Y$ 有价值的信息。
    - 同样不可计算。

**八、 联合压缩与条件压缩的等价性**
- **在机器学习中应用条件Kolmogorov复杂性的挑战**
    - 直接对大型数据集进行条件化是不自然的或困难的。
- **一个重要结果**
    - 使用普通的Kolmogorov压缩器来压缩 $X$ 和 $Y$ 的串联 ($K(X,Y)$)，对于预测 $Y$ 来说，与使用条件压缩器 ($K(Y|X)$) 的效果“一样好”（存在一些细节和微妙之处）。
    - 这意味着只需将所有数据（无监督 $X$ 和监督 $Y$）连接起来并进行压缩。

- **最大似然估计与联合压缩**
    - 联合压缩在无过拟合的情况下等同于最大似然估计。
    - 数据集的对数似然总和（给定参数）即为压缩该数据集的成本（还需加上压缩参数的成本）。
    - 在机器学习中，联合压缩（通过将数据点添加到训练集）非常自然。
    - 大型神经网络通过SGD优化，可以看作是在逼近Kolmogorov压缩器，从而实现有效的无监督学习。

**九、 理论的实证验证：iGPT**
- **GPT模型与压缩理论**
    - 该理论适用于GPT模型，但GPT模型的行为（如few-shot learning）也可以通过互联网文本的条件分布来直观解释，无需明确引用压缩理论。
- **在视觉领域寻求直接验证：iGPT (2020年工作)**
    - 目标：证明一个优秀的“下一步预测器”能实现出色的无监督学习。
    - 方法：
        - 将图像转换为像素序列（每个像素具有离散强度值）。
        - 使用Transformer进行“下一像素预测”。
        - 这等同于最大化似然，从而实现压缩。
    - **结果**
        - 在CIFAR-10上：模型在下一像素预测任务上的准确性越高，其在线性探针（linear probe）分类任务上的准确性也越高。
        - 扩展到ImageNet：使用大型Transformer（当时算大，如60亿参数）处理小图像（如$64 \times 64$），学习效果良好，接近当时ImageNet上最好的无监督学习结果，显示了可扩展性。
        - 在CIFAR-10上达到99%的准确率（在2020年是很好的结果）。

**十、 关于线性表征的思考**
- **压缩理论的局限性**
    - 压缩理论本身并不直接解释为什么会学到线性可分的表征。
- **线性表征的普遍性**
    - 线性表征在实践中经常出现，其背后的原因可能非常深刻。
- **观察与推测**
    - 自回归模型（如iGPT的下一像素预测）似乎比BERT类型的模型（如掩码语言模型）能学到更好的线性表征。
    - 推测原因：下一像素预测需要模型关注长程结构；而BERT的掩码预测任务通常可以通过局部信息较好地完成，可能不会迫使模型学习到同样丰富的全局表征。
    - 形成线性表征的真正原因仍然是一个有待深入理解的问题。

**十一、 问答环节要点**
- **更鲁棒的二维下一像素预测**：任何将神经网络转化为概率模型（为不同输入分配概率）的方法都可以，如最大化似然的扩散模型。扩散模型可能也因类似BERT的原因，其表征不如自回归模型。
- **Kolmogorov复杂性与神经网络的差异**：神经网络的训练动态（如数据顺序影响）与Kolmogorov复杂性的理想搜索（每次从头枚举）不同。类比不完美，尤其是在搜索过程方面。
- **密码学中区分分布、预测与压缩的联系**：Ilya提到能量模型中分布的比率与能量差异相关，但未深入探讨。
- **VC维度的辩护**：Scott Aaronson举例说明在某些情况下（如量子态的PAC学习），即使离散化后，VC维度依然重要。
- **Transformer/SGD作为最佳压缩程序**：虽然模型一次处理一个样本，但整个数据集的总训练损失（对数概率之和）代表了用该模型压缩整个数据集所需的比特数。
- **压缩作为框架，iGPT和线性表征**：线性表征是“额外的好处”，理论主要表明好的微调（fine-tuning）应该会出现。
- **无监督理论对监督学习的启示**：可能解释为什么参数比样本多的模型有时也能工作（与函数类别有关），但Kolmogorov理论忽略了计算成本。
- **自回归建模的重要性**：其他最大似然方法（如某些扩散模型）理论上也应可行，但自回归模型简单方便。计算效率可能不同。
- **GPT-4作为最佳压缩器与压缩器自身大小**：理论上，如果关心的是压缩一个无限大的测试集，那么压缩器本身的大小相对不那么重要。这与Kolmogorov复杂性中固定数据集的设定有所不同。单轮训练（single epoch）可能更符合理论。
- **用gzip和k-NN进行文本分类**：gzip不是一个强大的文本压缩器，真正有价值的结构信息在于压缩最后一些比特。
- **课程学习（Curriculum effects）**：对于易于优化的架构（如现代Transformer），课程学习不那么重要；对于复杂或奇特的架构则可能非常关键。

---