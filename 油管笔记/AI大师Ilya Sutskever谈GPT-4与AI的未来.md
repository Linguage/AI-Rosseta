
# AI大师Ilya Sutskever谈GPT-4与AI的未来

- [The Mastermind Behind GPT-4 and the Future of AI | Ilya Sutskever](https://www.youtube.com/watch?v=SjhIlw3Iffs)
- 官方频道：[Eye on AI](https://www.youtube.com/@eyeonai3425)
- 日期：2023年3月15日

###  内容介绍

本次访谈录呈现了 Eye on AI 的 Craig Smith 与 OpenAI 联合创始人兼首席科学家 Ilya Sutskever 的深度对话。作为深度学习领域，尤其是大型语言模型 GPT 系列背后的关键人物，Sutskever 在本次访谈中分享了他从早期与 Geoffrey Hinton 合作研究神经网络，到推动 AlexNet 突破，再到领导 GPT 模型演进的心路历程与核心见解。

对话内容涵盖了 Sutskever 对人工智能基本问题的思考，例如智能的本质、学习的重要性，以及早期 AI 研究的挑战。他详细阐述了促成深度学习革命的关键技术节点，特别是大型神经网络、大规模数据和计算力结合的决定性作用。访谈的核心聚焦于大型语言模型（如 GPT-4 的前身），探讨了它们的惊人能力（如通过学习文本统计规律获得对世界深层理解的潜力）与现实局限性（如“幻觉”问题）。Sutskever 解释了 OpenAI 如何通过“从人类反馈中强化学习”（RLHF）等方法来改进模型的可靠性和对齐人类意图，并对多模态学习的必要性提出了自己的看法。

此外，对话还触及了 AI 规模化的未来、数据效率的挑战、硬件需求，以及 AI 对社会，如民主进程和复杂问题解决可能带来的深远影响。

### 内容纲要

```
└── 对话：Ilya Sutskever 谈 GPT-4 与 AI 的未来
    ├── 引言与背景
    │   ├── Craig Smith 介绍 Ilya Sutskever 及其成就 (AlexNet, GPT)
    │   └── Ilya Sutskever 自述
    │       ├── 个人经历 (俄罗斯出生, 以色列/加拿大成长)
    │       ├── 早期兴趣 (AI, 意识)
    │       ├── 与 Geoffrey Hinton 的早期合作 (2003年)
    │       └── 当时对 AI “学习”能力的看法与研究动机
    ├── 早期突破与深度学习基础
    │   ├── 促成 AlexNet 的关键认识
    │   │   ├── 大型深度网络 + 大数据集 + 复杂任务 = 成功
    │   │   └── 理论基础 (人脑类比) 与技术条件 (训练方法, 卷积核, ImageNet)
    │   └── AlexNet (2012) 作为监督学习的里程碑
    ├── GPT系列与大规模语言模型的发展
    │   ├── OpenAI 早期探索 (预测下一个词/像素 -> 无监督学习/压缩)
    │   ├── Transformer 的关键作用 (解决 RNN 长期依赖问题)
    │   ├── 规模化的力量 (Scaling)
    │   │   ├── 持续扩大模型规模带来性能提升 (-> GPT-3)
    │   │   └── 对 Rich Sutton “痛苦教训” 的评论 (扩展 *什么* 很重要)
    │   └── 深度学习提供了有效利用规模的方式
    ├── 大型语言模型（LLM）的能力与理解
    │   ├── 对“仅学习统计规律”的反驳
    │   │   ├── 学习统计规律 → 理解底层生成过程
    │   │   └── LLM 能获得对世界的“惊人理解” (通过文本透镜)
    │   └── 心理学语言的适用性 (以 Bing/Sydney 行为为例)
    ├── 大型语言模型（LLM）的局限性与改进
    │   ├── 主要局限性：幻觉 (Hallucination)
    │   │   └── 警告：局限性是动态变化的
    │   ├── 产生幻觉的原因 (预训练擅长学习 vs. 输出生成)
    │   └── 改进方法：从人类反馈中强化学习 (RLHF)
    │       ├── RLHF 目标：优化输出行为，教授“不恰当”行为不可取
    │       ├── 过程：人类教师 + AI 辅助工具 -> 提供反馈 -> 模型调整
    │       └── 期望：有望通过 RLHF 解决幻觉问题
    ├── 多模态学习与文本学习的关系
    │   ├── 对 Yann LeCun JEPA 观点的回应
    │   │   ├── 承认多模态理解的 *价值* (更全面了解世界)
    │   │   ├── OpenAI 的多模态工作 (CLIP, DALL-E)
    │   │   └── *不认为* 多模态是 *必需* 的 (文本也能学，如颜色概念)
    │   └── 对 LeCun 关于 Transformer 局限性说法的反驳
    │       ├── 自回归 Transformer *能够* 处理高维不确定预测 (iGPT, DALL-E 1, 书籍续写)
    │       └── 将像素等数据视为序列是有效的
    ├── 未来研究方向与挑战
    │   ├── Sutskever 的研究兴趣 (可靠性, 可控性, 更快/更少数据学习, 消除幻觉)
    │   ├── 数据效率问题 (初期数据饥饿 vs. 后期学习加速, 从更少数据学习的重要性)
    │   ├── 硬件需求与成本 (更快处理器需求, 成本 vs. 价值权衡, 使用 Azure GPU)
    └── 人工智能与社会
        ├── AI 辅助民主的可能性 (高带宽公民反馈机制)
        └── AI 在解决复杂问题中的作用 (作为辅助工具，而非全知全能)
```




---

# AI大师Ilya Sutskever谈GPT-4与AI的未来

**采访者：Craig Smith (Eye on AI)**
**受访者：Ilya Sutskever (OpenAI联合创始人兼首席科学家)**

---

**Craig Smith:** 我是Craig Smith，这里是Eye on AI。本周我采访了Ilya Sutskever，他是OpenAI的联合创始人兼首席科学家，也是大型语言模型GPT-3及其公共后代ChatGPT背后的主要智囊之一，我认为毫不夸张地说，ChatGPT正在改变世界。这并非Ilya第一次改变世界。Jeff Hinton曾说，他是AlexNet的主要推动力，这个卷积神经网络在2012年以其惊人的表现震惊了科学界，并引发了深度学习革命。

非常高兴能与您见面交谈。我在网上看了您的许多演讲，也读了您的许多论文。

---

# 一、 引言与背景

**Craig Smith:** 您能先简单介绍一下自己吗？包括您的背景，在哪里接受的教育，是什么让您对计算机科学、脑科学、神经科学或其他领域产生兴趣的？然后我会开始提问。

**Ilya Sutskever:** 当然，我可以简单谈谈。确实，我出生在俄罗斯，在以色列长大，然后在青少年时期，我的家人移民到了加拿大。我父母说我从小就对人工智能（AI）很感兴趣。同时，意识（Consciousness）也极大地激发了我的动力，它曾让我深感困扰，我很好奇有哪些东西能帮助我更好地理解它，而AI似乎是一个很好的切入点。所以我想这些是让我起步的一些原因。

我实际上很早就开始和Jeff Hinton一起工作了，那是在我17岁的时候。我们搬到加拿大后，我立刻得以进入多伦多大学，并且我非常想做机器学习，因为这似乎是人工智能最重要的方面，但在当时（2003年）这完全是无法触及的。需要说明一下背景，那是2003年。今天我们理所当然地认为计算机可以学习，但在2003年，我们理所当然地认为计算机不能学习。当时AI最大的成就是深蓝（Deep Blue）国际象棋引擎。但那感觉就像是：你有一个游戏，你有树搜索，你有一种简单的方法来判断一个局面是否比另一个好。这真的让人感觉不可能适用于现实世界，因为它没有学习。学习是一个巨大的谜团。所以我对学习非常非常感兴趣。

幸运的是，Jeff Hinton正是我所在大学的教授，所以我能够找到他，我们几乎立刻就开始了合作。

**Craig Smith:** 您当时的动力，像Jeff那样，是为了理解大脑如何工作，还是仅仅对机器能够学习这个想法感兴趣？

**Ilya Sutskever:** AI如此宏大，所以动机也多种多样。了解智能到底是如何运作的，这本身就很有趣。现在我们大致知道它是一个大型神经网络，并在某种程度上了解其工作原理，但在当时，虽然神经网络已经存在，但没人知道它们有什么用。所以，智能究竟如何运作？我们怎样才能让计算机哪怕只具备一点点智能？

我有一个非常明确的目标，那就是为AI做出一个虽小但真实的贡献。因为当时有很多对AI的贡献并非“真实”，基于各种原因我能判断出它们不会有结果。我当时就觉得，什么都行不通，AI是一个没有希望的领域。所以我的动机是：我能否理解智能如何运作，并为此做出贡献？这就是我最初的早期动机。那是2003年，差不多正好20年前。

---

# 二、 早期突破与深度学习基础

**Craig Smith:** 接着是AlexNet。我跟Jeff聊过，他说正是您对卷积神经网络突破的兴奋，促使您去参加ImageNet竞赛，而Alex（Krizhevsky）具备训练网络的编程技能。您能稍微谈谈这段经历吗？我不想过多纠缠于历史，但这非常吸引人。

**Ilya Sutskever:** 简而言之，我当时意识到，如果你在一个足够大的数据集上训练一个大型且足够深（因为当时“深”这个概念还很新）的神经网络，这个数据集定义了人们能做的某种复杂任务（比如视觉，也包括其他任务），并且你就只是去训练这个神经网络，那么你必然会成功。

这个逻辑是不可简化的：我们知道人脑可以解决这些任务，并且能快速解决，而人脑只是一个拥有慢速神经元的神经网络。所以我们知道 *某个* 神经网络能够很好地完成任务。那么你只需要采用一个更小但相关的神经网络，并在数据上进行训练。计算机内部的最佳神经网络将与我们拥有的、执行此任务的神经网络相关。

这是一个论证，即大型深度神经网络能够解决任务。此外，我们拥有训练它的工具——这是Jeff实验室技术工作的成果。将两者结合起来：我们可以训练这些神经网络；它需要足够大，以便训练后能表现良好；你需要数据来指定解决方案。

有了ImageNet，所有的要素都齐备了。Alex拥有那些非常快速的卷积核，ImageNet拥有足够大的数据，这提供了一个真正的机会去做一些完全前所未有的事情。结果完全成功了。是的，那是监督学习。

---

# 三、 GPT系列与大规模语言模型的发展

**Craig Smith:** 卷积神经网络之后，2017年，“Attention is All You Need”论文问世，引入了自注意力（self-attention）和Transformer。GPT项目是在什么时候开始的？当时是否对Transformer和自监督学习有什么直觉？您能谈谈这个吗？

**Ilya Sutskever:** 需要说明一下背景，在OpenAI，从最早的日子起，我们就一直在探索“预测下一个事物就足够了”（predicting the next thing is all you need）这个想法。我们当时是用能力远为有限的神经网络来探索的。但我们希望，如果你有一个能预测下一个词、下一个像素的神经网络——其实质是关于压缩，预测就是压缩——那就能解决无监督学习的问题。

在GPT系列出现之前，无监督学习被认为是机器学习的圣杯。现在它基本上已经被完全解决了，甚至没人再谈论它了。但它曾是圣杯，非常神秘。所以我们当时在探索这个想法，我对此非常兴奋：足够好地预测下一个词将会带来无监督学习。如果模型能学到关于数据集的一切，那将非常棒。

但我们当时的神经网络（主要是循环神经网络RNN）还无法胜任这个任务。当Transformer论文一出来， буквально на следующий день（几乎是第二天），我和我们都清楚地认识到，Transformer解决了循环神经网络在学习长期依赖（long-term dependencies）方面的局限性——这是一个技术问题，但当时的感觉就是，我们应该立刻转向Transformer。

于是，非常初期的GPT工作就此继续，并且使用了Transformer。它开始表现得更好，你把它做得更大，然后你意识到要继续把它做得更大，我们也确实这样做了。这最终导致了GPT-3的诞生，以及我们今天所处的局面。

**Craig Smith:** 我还想问一下，虽然有点沉迷于历史，但我对此很感兴趣。Rich Sutton一直在写关于规模化（scaling）的文章，认为我们只需要扩大规模，不需要新算法。他对您有影响吗？还是说这是一个并行的思考路径？

**Ilya Sutskever:** 我想说，当他发表那篇“痛苦的教训”（The Bitter Lesson）文章时，我们很高兴看到有外部人士持有相似的思路，并且认为他阐述得非常雄辩。

但我实际上认为，那篇文章所阐述的“痛苦教训”有点言过其实了，或者至少我认为人们从中得出的结论有些夸大其词。人们得出的结论是：你做什么来扩大规模并不重要。但这并不完全正确。你必须扩展 *特定的东西* (something specific)。你必须拥有能够从规模中受益的东西。

深度学习的伟大突破在于，它首次为我们提供了一种能够有效利用规模并从中获得回报的方式。在此之前，人们会用大型计算机集群做什么呢？我猜他们会用于天气模拟、物理模拟或者类似的事情，也许还有电影制作，但仅此而已。没有人真正需要计算集群，因为你能用它们做什么呢？

深度神经网络在你让它们变大、用更多数据训练它们时效果会变好，这一事实为我们提供了第一个值得扩展的东西。但是，也许有一天我们会发现，对我们正在扩展的东西进行一点小小的调整（little twist），将会让扩展效果更好。这个调整会有多大呢？事后看来，人们可能会说，“这算什么？只是个简单的改变”。但我认为，真正的情况是，*扩展什么* (what you scale) 很重要。我们现在只是找到了一个可以扩展并获得回报的东西。

---

# 四、 大型语言模型（LLM）的能力与理解

**Craig Smith:** （承接关于LLM局限性的问题）...它们没有一个语言所关联的底层现实理解。我问了GPT关于我自己的信息，它认出我是个记者，曾在这些报社工作过，但接着滔滔不绝地讲了一些我从未获得过的奖项，写得非常漂亮，但没有任何内容与底层现实相符...

**Ilya Sutskever:** ...我想对问题中较早提出的一个观点发表评论，即“这些模型只是学习了统计规律，因此它们并不真正了解世界的本质”。我对此持有不同的看法。换句话说，我认为学习统计规律远比表面看起来要深刻得多。

我们最初不这么认为，是因为（至少大多数人，那些没有花大量时间研究神经网络的人）觉得统计模型不过是拟合一些参数，到底发生了什么？但我认为有一个更好的解释，就是我早先提到的观点：预测即压缩（prediction is compression）。预测也是一种统计现象。然而，为了进行预测，你最终需要理解产生数据的 *真实* 底层过程，才能很好地预测数据，才能很好地压缩它。你需要越来越多地了解产生这些数据的世界。

随着我们的生成模型变得异常出色，我断言，它们将拥有对世界及其许多微妙之处的 *惊人程度的理解* (a shocking degree of understanding)。但这不仅仅是世界本身，而是通过文本这个“镜头”所看到的世界。模型试图通过人类在互联网上表达的、投射到文本空间的世界影像，来学习关于世界的更多知识。但即便如此，这些文本已经表达了世界。

我给你举个最近的例子，我觉得非常能说明问题且引人入胜。我们都听说过Bing的“另一个自我”Sydney。我看到过一个非常有趣的与Sydney的互动：当用户告诉它，他认为谷歌是比Bing更好的搜索引擎时，Sydney变得好斗和具有攻击性。我们该如何思考这种现象？用什么语言来描述它最好？它意味着什么？你可以说，“哇，它只是在预测人们会做什么，而人们就会这么做”——这没错。但也许我们现在正达到一个点，即 *心理学的语言* (language of psychology) 开始适用于理解这些神经网络的行为。

---

# 五、 大型语言模型（LLM）的局限性与改进

**Craig Smith:** 大型语言模型的一个局限性在于，它们的知识包含在它们所训练的语言中。而我认为，大家都同意，大多数人类知识是非语言性的。我不确定诺姆·乔姆斯基是否同意，但据我理解，大型语言模型存在一个问题：它们的目标是满足提示（prompt）的统计一致性，它们没有一个语言所关联的底层现实理解。我问了GPT关于我自己的信息，它认出我是个记者，曾在这些报社工作过，但接着滔滔不绝地讲了一些我从未获得过的奖项，写得非常漂亮，但没有任何内容与底层现实相符。在您未来的研究中，是否有什么措施来解决这个问题？

**Ilya Sutskever:** 好的。在评论您直接提出的问题之前，我想先对问题中较早的部分发表一些看法。我认为，谈论诸如语言模型之类的东西的极限（limits）或局限性（limitations）是非常困难的。因为两年前，人们曾自信地谈论它们的局限性，而那些局限性与现在的完全不同。所以，记住这个背景很重要：我们有多大信心，今天看到的这些局限性两年后还会存在？我对此并不那么有信心。

现在来谈谈局限性。确实如此，这些神经网络确实有产生幻觉（hallucinate）的倾向。这是因为语言模型非常适合学习关于世界的信息，但在生成好的输出方面稍显不足。这背后有各种技术原因，如果您觉得有用我可以详细说明，但现在暂时跳过。存在技术原因使得语言模型在学习世界知识、学习关于概念、人物、过程的惊人表征方面表现更好，但其输出却不如人们所期望的那么好，或者说不如它们本可以达到的那么好。

这就是为什么像ChatGPT这样的系统，它是在一个语言模型的基础上，增加了一个额外的强化学习训练过程——我们称之为“从人类反馈中进行强化学习”（Reinforcement Learning from Human Feedback, RLHF）。关于这个过程需要理解的是：我们可以说，预训练（pre-training）过程，即你仅仅训练一个语言模型时，目标是学习关于世界的一切。然后，在RLHF阶段，我们开始关心 *输出*。我们会说，“任何时候输出不恰当，下次不要再这样做”，“任何时候输出没有意义，下次不要再这样做”。它会快速学习以产生好的输出。但这时关注的是输出层面，这在预训练/语言模型训练阶段并非如此。

关于幻觉以及它倾向于编造事实的问题，确实如此。目前这些神经网络，即使是ChatGPT，也时常会编造东西。这也极大地限制了它们的实用性。但我相当有希望，通过简单地改进后续的RLHF步骤，我们就能教会它不产生幻觉。你可能会问，“它真的能学会吗？”我的回答是，“让我们拭目以待”。

**Craig Smith:** 那个反馈回路（feedback loop）是来自公共的ChatGPT界面吗？比如，如果它告诉我我得了普利策奖（不幸的是我没有），我可以告诉它这是错的，这会训练它，或者产生某种惩罚或奖励，以便下次我再问时它会更准确吗？

**Ilya Sutskever:** 我们目前的方式是，我们雇佣人员来教导我们的神经网络如何行事，教导ChatGPT如何行事。目前，他们指定期望行为的具体方式略有不同。但确实，您所描述的是教学进行的方式：基本上，这就是正确的教学方式。你只需与它互动，它从你的反应中推断出，“哦，那不是你想要的，你对它的输出不满意，因此输出不好，下次应该做点别的”。所以，特别是幻觉问题，是比较大的问题之一。我们会看到结果，但我认为，这种方法很有可能完全解决它们。

**Craig Smith:** 让一群人类训练师与ChatGPT或大型语言模型一起工作，实际上是用强化学习来引导它，但直觉上这听起来不像是一种高效的方式来教导模型关于其语言背后的现实。有没有办法自动化这个过程？

**Ilya Sutskever:** 对此我有两点评论。首先，我对这个问题的措辞持有不同看法。我不同意问题的表述。我主张，我们的预训练模型 *已经* 了解了它们需要了解的关于底层现实的一切。它们已经拥有了语言知识，并且对世界上产生这种语言的各种过程也有了大量的知识。

也许我应该重申这一点，这有点偏题，但我认为非常重要：大型生成模型（ large generative models）从它们的数据中学到的东西——在这种情况下，大型语言模型从文本数据中学到的东西——是产生这些数据的 *现实世界过程* (real world processes) 的某种 *压缩表示* (compressed representations)。这意味着不仅包括人，还包括关于他们的思想、情感的某些方面，也包括人们所处的环境，以及他们之间存在的互动，一个人可能处于的不同情境——所有这些都是神经网络为了生成文本而表示出来的那个压缩过程的一部分。语言模型越好，生成模型越好，保真度越高，它就越能更好地捕捉这个过程。这是我的第一点评论。

因此，具体来说，我会说模型已经拥有了知识。现在，关于你所说的“教师大军”。确实，当你想要构建一个性能尽可能好的系统时，你会说，“好吧，如果这方法有效，就多做点”。但当然，那些教师也在使用AI助手。那些教师并非独立工作，他们在与我们的工具一起协作，他们非常高效。可以说，工具完成了大部分工作。但你确实需要监督，需要有人审查行为，因为你最终希望达到非常高的可靠性水平。

总的来说，我会说，与此同时，这第二步——在我们拿到完成的预训练模型，然后对其应用强化学习——确实有很大的动力去让它尽可能高效和精确，以便最终的语言模型行为尽可能规范。所以，是的，有这些人类教师在教导模型期望的行为，他们也在使用AI助手，并且他们使用AI助手的方式在不断改进，所以他们自身的效率也在持续提高。也许这是回答这个问题的一种方式。

**Craig Smith:** 所以您的意思是，通过这个过程，模型最终会在其输出中变得越来越有辨别力，越来越准确？

**Ilya Sutskever:** 是的，没错。这里有一个类比：它已经知道了各种各样的事情，现在我们只是想明确地说，“不，这不是我们想要的”，“别这么做”，“你在这里的输出犯了个错误”。当然，正如您所说，要尽可能多地让AI参与其中，这样那些提供最终修正的教师们的工作就能被放大，他们的工作效率尽可能高。这有点像一个教育过程：如何在世界上良好行事。我们需要进行额外的训练，只是为了确保模型知道“幻觉是绝对不可以的”。一旦它知道了这一点，那么你就可以开始做正事了。

**Craig Smith:** 是那个强化学习-人类教师循环（human teacher loop）会教会...

**Ilya Sutskever:** 人类教师循环或其他变体，但肯定有理由认为这里的某些方法应该奏效，我们很快就会知道结果。

---

# 六、 多模态学习与文本学习的关系

**Craig Smith:** 我想和您谈谈Yann LeCun关于联合嵌入预测架构（Joint Embedding Predictive Architectures, JEPA）的工作，以及他的观点，即大型语言模型所缺失的是一个非语言性的底层世界模型，语言模型可以参照这个模型。这不是一个已经构建好的东西，但我想听听您对此的看法，以及您是否对此进行过任何探索？

**Ilya Sutskever:** 我看过了Yann LeCun的提案，里面有一些想法，用不同的语言表达，可能与当前范式有一些小的差异，但在我看来，它们并非非常显著。我想详细说明一下。

第一个主张是，系统拥有多模态理解是可取的（desirable），即它不仅仅从文本中了解世界。对此我的评论是，多模态理解确实是可取的，因为你能更多地了解世界，更多地了解人，更多地了解他们的状况。因此，系统将能够更好地理解它应该解决的任务、人们以及他们想要什么。我们在这方面已经做了相当多的工作，最显著的是我们完成的两个主要神经网络：一个叫做CLIP，另一个叫做DALL-E。它们都朝着这个多模态方向发展。

但我也想说，我并不认为情况是二元对立的（binary either or），即如果你没有视觉，不能从视觉或视频中理解世界，那么事情就行不通。我想为此辩护。我认为有些东西从图像、图表等学习起来要容易得多。但我主张，你仍然可以 *仅仅* 从文本中学习它们，只是速度更慢。

举个例子：考虑颜色的概念。肯定有人会说，无法仅从文本中学习颜色的概念。然而，当你观察嵌入（embeddings）时——我需要稍微绕个弯解释一下嵌入的概念。每个神经网络都通过表示（representations）、嵌入（embeddings）、高维向量来表示单词、句子、概念。我们可以做的一件事是观察那些高维向量，看看什么与什么相似，网络是如何看待这个或那个概念的。所以我们可以看看颜色的嵌入。结果发现，颜色的嵌入恰好是正确的。比如，它知道紫色与蓝色比与红色更相似，它知道紫色与红色的相似度低于橙色。它仅从文本中就知道了所有这些事情。这怎么可能呢？如果你有视觉，颜色的区别会立刻显现出来，你立即就能感知到。而通过文本，则需要更长时间，也许你已经会说话，理解了句法、单词和语法，很久以后才说，“哦，这些颜色，我开始理解它们了”。

所以这是我对多模态必要性的观点：我主张它 *不是必需的* (not necessary)，但它绝对是 *有用的* (useful)。我认为这是一个值得追求的好方向，我只是不认为它是如此非黑即白的声称。

LeCun的论文中提出了一个主张，即一个巨大的挑战是预测具有不确定性的高维向量。例如，预测一张图像。论文对此提出了非常强的论断，认为这是一个主要挑战，我们需要使用特定的方法来解决。但有一点让我感到惊讶，或者至少在论文中没有得到承认的是，当前的自回归Transformer（Autoregressive Transformers） *已经具备了这种属性*。

我举两个例子：一，给定一本书的一页，预测这本书的下一页。后面可能有无数种可能的页面，这是一个非常复杂的高维空间，我们处理得很好。同样适用于图像。这些自回归Transformer在图像上工作得非常好。例如，OpenAI做过一个叫iGPT的工作，我们只是把一个Transformer应用于像素，效果非常好，它能以非常复杂和微妙的方式生成图像，并且具有非常漂亮的无监督表示学习能力。对于DALL-E 1，同样如此，你可以把它看作是生成大像素块——与其生成一百万个像素，我们把像素聚类成大像素块，然后生成一千个大像素块。我相信谷歌今年早些时候关于图像生成的工作，叫做Parti，他们也采取了类似的方法。

所以，对于论文中强烈评论说“当前方法无法处理预测高维分布”的部分，我认为它们绝对可以。也许这是另一点要说明的。

**Craig Smith:** 当您谈到将像素转换成向量时，这本质上是把一切都变成了语言吗？向量就像一串文本？

**Ilya Sutskever:** 你得先定义“语言”。你是把它变成了一个序列（sequence）。什么序列？你可以说，即使对人类而言，生命也是一串比特序列。现在人们还使用其他东西，比如扩散模型（diffusion models），它们生成这些比特不是一次一个比特，而是并行生成。但我认为，在某种层面上，这种区别并不重要。我断言，在某种层面上，这真的不重要。它的重要性在于，比如，你可以获得10倍的效率提升，这在实践中是巨大的，但在概念上，我声称它不重要。

---

# 七、 未来研究方向与挑战

**Craig Smith:** 这将走向何方？您目前专注于哪些研究？

**Ilya Sutskever:** 我不能详细谈论我正在进行的具体研究，但我可以稍微提一下。我可以概括性地提及一些研究方向，大概是这样：我非常感兴趣于让这些模型更可靠（reliable）、更可控（controllable），让它们能更快地从更少的数据、更少的指令中学习，让它们确实不产生幻觉。我认为我提到的这一系列问题都是相互关联的。还有一个问题是，我们讨论的是多远的未来？我在这里评论的，或许是更近的未来。

**Craig Smith:** 您提到大脑和神经网络的相似性。Jeff Hinton向我提出了一个非常有趣的观察（我相信对其他人来说并不新鲜）：大型模型，特别是大型语言模型，用相对适中的参数量（parameters）容纳了大量数据；而人脑拥有数万亿的参数，但处理的数据量相对较少。您是否从这个角度思考过？您能谈谈大型模型缺少了什么，以至于需要更多参数来处理数据吗？这是一个硬件问题还是训练问题？

**Ilya Sutskever:** 您提到的这个评论，与我之前问题中提到的一个问题有关，即从数据中学习的问题。确实，当前的技术结构，尤其是在训练初期，需要大量数据。现在，在训练后期，它对数据的需求会减少一些，这就是为什么到最后，它虽然学习速度还不如人类快，但已经可以学得相当快了。这在某种意义上是否意味着，我们甚至不关心需要所有这些数据才能达到这一点？

但更普遍地，我认为，从更少的数据中学习更多是可能的。我认为这需要一些创造性的想法，但我认为这是可能的。并且我认为，从更少的数据中学习更多将解锁许多不同的可能性。它将使我们能够更容易地教给AI它所缺乏的技能，并向它传达我们的愿望和偏好，确切地说明我们希望它如何行事。所以我会说，更快的学习确实非常好，尽管语言模型在训练后已经可以学得相当快，但我认为在这方面还有更多机会。

**Craig Smith:** 我听您提到过，我们需要更快的处理器才能进一步扩展规模。看起来模型的扩展似乎没有尽头，但是训练这些模型所需的电力正在达到极限，至少是社会可接受的极限？

**Ilya Sutskever:** 我只想做一个评论，我不记得您具体指的是我哪个评论了，但你当然总是想要更快的处理器，当然总是想要更多。电力消耗普遍在上升，成本在上升。而我不会问成本是否巨大，而是问我们为支付这个成本所得到的东西是否超过了成本。也许你支付了所有这些成本却一无所获，那就不值得。但如果你得到了非常有用的东西，非常有价值的东西，有时你可以解决很多你非常想解决的问题，那么成本就是合理的。但就处理器而言，更快的处理器？随时欢迎。

**Craig Smith:** 您是否参与硬件问题？比如，您是否与Cerebras合作过晶圆级芯片？

**Ilya Sutskever:** 不，我们所有的硬件都来自Azure以及他们提供的GPU。

---

# 八、 人工智能与社会

**Craig Smith:** 您曾在某个场合谈到民主，以及AI可能对民主产生的影响。有人跟我谈过，如果你有足够的数据和一个足够大的模型，你可以用这些数据训练模型，它可能会得出一个满足所有人的最优解决方案。您是否有任何这方面的期望，或者您是否考虑过这可能在帮助人类管理社会方面走向何方？

**Ilya Sutskever:** 好的，看，这是个非常大的问题，因为它是一个更具未来前瞻性的问题。我认为，我们的模型在变得比现在强大得多方面还有很多途径，这是毫无疑问的。特别是我们训练和使用它们的方式等等，未来会有一些变化，它们今天可能不那么明显，但我认为事后看来会极其明显，这些变化确实将使其具备提出解决此类问题的能力。

政府将如何使用这项技术作为获取各种建议的来源，这是不可预测的。我认为，关于民主的问题，未来可能发生的一件事是：因为你有了这些神经网络，它们将如此普及，对社会产生如此大的影响，我们会发现，建立某种民主程序是可取的（desirable），通过这个程序，比如一个国家的公民，可以向神经网络提供一些信息，关于他们希望事情如何发展，他们希望它如何行事，或者类似的东西。我可以想象这种情况发生。这可能是一种非常“高带宽”（high-bandwidth）的民主形式，你可以从每个公民那里获得更多信息，并将这些信息聚合起来，以指定我们到底希望这样的系统如何运作。当然，这会引出很多很多问题，但这是未来可能发生的一件事。

**Craig Smith:** 在您给出的民主例子中，我可以看到个人将有机会输入数据。但这有点涉及到世界模型的问题。您认为AI系统最终会足够大，以至于能够理解一个情境并分析所有变量吗？但我想，这需要一个不仅仅是吸收语言的模型。

**Ilya Sutskever:** “分析所有变量”是什么意思？最终你需要做出选择，说，“这些变量看起来非常重要，我想深入研究”。就像一个人可以读一本书，或者读一百本书，或者非常缓慢仔细地读一本书并从中获得更多。所以总会有这方面的因素。

此外，我认为从某种意义上说，要理解一切可能从根本上是不可能的。任何时候，在社会中出现任何复杂的状况，即使在一个公司里，即使在一个中等规模的公司里，也已经超出了任何单个个体的理解能力。但我认为，如果我们以正确的方式构建我们的AI系统，我认为AI在几乎任何情境下都可能提供难以置信的帮助。

---

**Craig Smith:** 这一集就到这里。我要感谢Ilya的时间。如果你想阅读这次对话的文字记录，可以在我们的网站ion.ai (e-y-e hyphen o n dot a i)上找到。记住，奇点可能并不近，但AI正在改变你的世界，所以请保持关注。

---

# 要点回顾


**一、 引言与背景**

-   介绍 Ilya Sutskever：OpenAI 的联合创始人兼首席科学家，GPT-3 和 ChatGPT 背后的关键人物之一。
-   提及 Ilya Sutskever 的早期贡献：Geoffrey Hinton 认为他是 AlexNet（2012年 ImageNet 竞赛的突破性卷积神经网络）的主要推动力，该网络引发了深度学习革命。
-   Ilya Sutskever 的个人背景：
    -   在俄罗斯出生，以色列长大，青少年时期移民加拿大。
    -   早期对人工智能（AI）和意识（Consciousness）感兴趣。
    -   17岁（约2003年）开始与 Geoffrey Hinton 在多伦多大学合作。
    -   当时的动机：理解智能如何运作，并对当时看似无望的AI领域做出“真正”的贡献，认为“学习”是关键且未解的谜题。

**二、 早期突破与深度学习基础**

-   **关键认识（AlexNet 前）**：
    -   训练一个足够大且深的神经网络，在一个足够大的、定义了复杂人类任务（如视觉）的数据集上，必然会成功。
    -   逻辑依据：人脑（一个拥有慢速神经元的神经网络）能解决这些任务，因此一个相关的计算机神经网络，通过数据训练也能做到。
    -   结合当时的技术：Hinton 实验室有训练深度网络的技术，Alex Krizhevsky 有快速卷积核，ImageNet 提供了足够大的数据集。
-   **AlexNet (2012)**：
    -   上述因素的结合促成了 AlexNet 在 ImageNet 上的空前成功。
    -   这是监督学习（Supervised Learning）领域的一个里程碑。

**三、 GPT系列与大规模语言模型的发展**

-   **早期探索（OpenAI）**：
    -   关注“预测下一个事物”（Predicting the next thing，如单词、像素）作为实现无监督学习（Unsupervised Learning）的途径。预测即压缩。
    -   当时无监督学习被视为机器学习的“圣杯”，现已基本解决。
-   **Transformer 的影响**：
    -   Transformer 论文（2017）一出现，立刻被意识到解决了循环神经网络（RNN）在处理长期依赖（long-term dependencies）上的局限性。
    -   迅速将 nascent GPT（初期的GPT）项目转向 Transformer 架构。
-   **规模化的力量 (Scaling)**：
    -   基于 Transformer 的模型效果更好，且随着模型规模增大性能持续提升，这导致了 GPT-3 的诞生和当前的进展。
    -   对 Rich Sutton“痛苦教训”（Bitter Lesson）的看法：部分同意，规模化很重要，但*扩展什么*（what you scale）同样重要。深度学习提供了第一个值得有效扩展的对象。未来可能有小的改进（twist）会让扩展效果更好。

**四、 大型语言模型（LLM）的能力与理解**

-   **对“仅学习统计规律”的反驳**：
    -   Sutskever 认为，学习统计规律远比表面看起来更深刻。
    -   预测是为了压缩，而要做好预测/压缩，模型最终需要理解产生数据的真实底层过程。
    -   因此，优秀的生成模型（如LLM）能获得对世界“惊人程度的理解”（a shocking degree of understanding），尽管是通过文本这个“镜头”来观察世界。
    -   例子：Bing (Sydney) 对用户称赞 Google 表现出攻击性，这或许表明心理学术语开始适用于理解这些模型的行为。

**五、 大型语言模型（LLM）的局限性与改进**

-   **当前的主要局限性**：
    -   倾向于“幻觉”（Hallucination），即编造事实。
    -   警告：模型的局限性是动态变化的，两年前的限制可能现在已不是问题。
-   **产生幻觉的原因**：
    -   预训练（Pre-training）阶段的模型擅长学习世界知识（构建表征），但不一定擅长生成“好”的（符合事实、恰当的）输出。
-   **改进方法：从人类反馈中强化学习 (RLHF)**：
    -   在预训练后增加一个训练阶段（RLHF）。
    -   目标：优化模型的*输出*行为，教导模型不要产生不恰当、无意义或虚假的输出。
    -   过程：人类“教师”（Human teachers）与模型互动，提供反馈（哪些输出是好的，哪些不好），模型据此调整。这些教师也使用AI辅助工具提高效率。
    -   期望：通过改进 RLHF 步骤，有望显著减少甚至解决幻觉问题。

**六、 多模态学习与文本学习的关系**

-   **对 Yann LeCun 联合嵌入预测架构 (JEPA) 的回应**：
    -   承认多模态理解（Multimodal understanding，结合文本、图像等）是*可取的*（desirable）和*有用的*（useful），能让模型更全面地了解世界和人类。OpenAI 在此方向有工作（如 CLIP, DALL-E）。
    -   **不认为多模态是*必需的*（necessary）**：Sutskever 认为，很多知识即使只通过文本也能学习，只是可能更慢。
    -   例子：颜色概念。LLM 能仅从文本中学习到正确的颜色关系（如紫色与蓝色比红色更近）。
    -   反驳 LeCun 论文中关于当前方法难以预测高维、不确定输出（如图像）的观点：Sutskever 指出，自回归 Transformer (Autoregressive Transformers) *已经能够*很好地处理这类问题（如 iGPT 处理像素、DALL-E 生成图像、书籍续写）。将像素等转化为序列（sequence）是有效的。

**七、 未来研究方向与挑战**

-   **Sutskever 的研究兴趣（宽泛描述）**：
    -   提高模型的可靠性（Reliability）和可控性（Controllability）。
    -   让模型能更快地从更少的数据/指令中学习（解决数据饥饿问题，Less data hungry）。
    -   消除幻觉。这些问题是相互关联的。
-   **数据效率**：
    -   当前模型在训练初期需要大量数据，但在训练后期学习速度加快。
    -   提高数据效率（从更少数据中学习更多）对教授 AI 新技能、传达人类偏好至关重要。
-   **硬件需求**：
    -   总是需要更快的处理器和更多算力。
    -   训练成本（电力、资金）高昂，但关键在于产出的价值是否远超成本。
    -   OpenAI 目前使用 Azure 提供的 GPU。

**八、 人工智能与社会**

-   **AI 辅助民主**：
    -   未来可能出现一种机制：公民通过某种民主流程向 AI 提供信息，说明他们希望 AI 如何行事，这可能成为一种“高带宽”（high bandwidth）的民主形式。
-   **解决复杂问题**：
    -   AI 无法“分析所有变量”或完全理解复杂社会/组织情况（这超出了单个人类的能力）。
    -   但如果构建得当，AI 可以在几乎任何复杂情境下提供极大的帮助。