---

---

# 「开放课程」MIT 计算思维（Julia）- 3：变换与自动微分

- 视频链接：[Transformations & AutoDiff | Lecture 3 | MIT Computational Thinking Spring 2021](https://www.youtube.com/watch?v=AAREeuaKCic&list=PLP8iPy9hna6T56GkMHEdSrjCCheNuEwI0&index=3)
- 官方频道：[链接](https://www.youtube.com/@TheJuliaLanguage)


### 讲座介绍

本次讲座内容源自 MIT 计算思维课程，主题为“变换与自动微分”。讲座旨在通过编程实践，特别是利用 Julia 语言，来加深对基础数学概念的理解，这些概念通常涉及多元微积分与线性代数。讲座的核心目标是展示计算工具如何能够帮助学习者，无论其背景如何（初学者或专家），建立或巩固对抽象数学理论的直观认识。

讲座首先通过生动的图像处理实例，可视化地介绍了二维空间中的各种变换，包括线性变换（如剪切、旋转）和非线性变换（如扭曲）。重点探讨了如何从视觉上区分它们，并将线性变换与矩阵的概念紧密联系起来，强调矩阵作为变换而非仅仅是数字数组的本质。

随后，讲座介绍了在 Julia 中定义函数的不同方式，为后续的自动微分计算奠定基础。核心部分深入探讨了自动微分（Autodiff）这一关键技术，阐述了其工作原理并非传统的符号微分或数值近似，而是能精确计算导数。内容覆盖了从简单的单变量函数导数，到多变量函数的梯度（用于标量输出函数）和雅可比矩阵（用于向量输出函数/变换）的计算。

最后，讲座联系了这些概念在机器学习（如优化损失函数）中的应用，并从几何角度解释了行列式（以及雅可比行列式）作为面积或体积缩放因子的直观意义。

### 内容纲要 

```
讲座：变换与自动微分
├── 一、 引言与目标
│   ├── 讲座目的：展示编程语言能力，结合数学实践
│   ├── 核心目标：通过计算加深数学（多元微积分、线性代数）的理解与直觉
│   └── 主要议题：变换 (Transformations) 与自动微分 (Autodiff)
├── 二、 图像变换：可视化与概念
│   ├── 直观演示：使用 Julia 和图像进行交互式变换
│   ├── 线性变换 (Linear Transformations)
│   │   ├── 一般线性变换 (矩阵控制)
│   │   ├── 剪切变换 (Shear)
│   │   ├── 旋转 (Rotation)
│   │   └── 核心观点：矩阵即变换，可视化理解几何意义
│   ├── 非线性变换 (Non-Linear Transformations)
│   │   ├── 扭曲/变形 (Warp)
│   │   └── 其他示例 (xy, rθ)
│   ├── 变换的复合
│   └── 视觉区分：线性 vs 非线性 (网格线变化，全等平行四边形)
├── 三、 Julia 中的函数定义
│   ├── 定义方式：短格式、匿名函数、长格式
│   ├── 函数参数：默认参数、关键字参数
│   └── 处理向量输入：直接接收、元组解构、多重派发
├── 四、 自动微分 (Automatic Differentiation - Autodiff)
│   ├── 核心概念：精确计算导数的技术
│   ├── 重要区别：非符号微分，非数值微分 (有限差分)
│   ├── 单变量函数 (Scalar -> Scalar)：基本导数计算
│   ├── 多变量标量值函数 (Vector -> Scalar)
│   │   ├── 导数：梯度 (Gradient)
│   │   └── 计算与直观理解 (有限差分对比)
│   └── 向量值多变量函数/变换 (Vector -> Vector)
│       ├── 导数：雅可比矩阵 (Jacobian Matrix)
│       └── 计算与直观理解 (有限差分对比各列)
├── 五、 应用与解释
│   ├── 机器学习优化：损失函数与梯度下降
│   └── 行列式 (Determinant) 的几何意义
│       ├── 线性变换：面积/体积缩放因子
│       └── 非线性变换：雅可比行列式的局部缩放意义
└── 六、 资源与总结
    ├── 提及补充资源 (Autodiff 视频)
    └── 讲座结束与预告
```


---

# 变换与自动微分 | MIT 计算思维 2021春季 第3讲

## 一、 引言与目标

大家好，向互联网上的每一位朋友问好。看起来我们已经有了成千上万的观众，这总是让我感到难以置信，因为即使在最好的情况下，MIT 的课堂里也只有少数学生。所以在互联网上有成千上万的观众真是令人兴奋。

今天的讲座一如既往地展示编程语言能做什么，但同时我们也会有很多乐趣。你会在这里看到，我们将要玩一些图像变换。我们会做这样的事情，还有这样的事情。

我这次讲座的真正目标是……该怎么说呢？有很多像多元微积分和线性代数这样的数学课程，这些是许多人都会学习的基础大学数学课程。也许你已经学过这些课程，或者你将在未来几年内学习，或者你可能是这些领域的专家，又或者你以前从未接触过这些。我的信念是，无论你以前没见过这些，还是已经是专家，没有什么比在计算机上亲自实践更好的了。

所以，让我开始向你展示一些东西。今天讲座的标题是“变换与自动微分”。自动微分（Automatic Differentiation）通常缩写为 Autodiff。Autodiff 技术是一项非常非常令人兴奋的技术，它在近几年才真正成熟起来，并且它能做到的事情持续让我惊叹。

但为了做好铺垫，我想向下滚动一下，我希望你能和我一起做。让我们先玩几个变换来熟悉一下概念。

## 二、 图像变换：可视化与概念

我已经预定义了一系列变换，比如一般的线性变换——你马上就会明白这些词的意思——剪切变换、扭曲（Warp）、旋转等等。这里，你可以选择任何你喜欢的图片。这是一只长长的柯基。我们可以用通常的柯基图片。你可以选择一张图片，我们可以从互联网上加载任何你喜欢的。我们也有这张奥胡斯（Aarhus）的图片，这实际上一直是我们的测试图案，但这里……嗯，让我们用这三只柯基吧。

好，这是一张图片，我们要做的就是运行一个线性变换。这个一般的线性变换，通过选取这个矩阵的坐标来实现。这是一个所谓的“可拖拽”矩阵。如果你在自己的 notebook 里运行，你可以点击这里，然后你就能看到，“哦，看那些柯基被剪切了。”

你可能注意到那些方格发生了什么？

> **问：** （来自助教/聊天）有人想告诉我吗？（方格发生了什么？）
> 
> **答：** （可能来自学生/聊天）当你进行剪切时，正方形变成了平行四边形，对吧？

所以这是一个剪切变换。我们可以做其他的线性变换，我们还可以放大、缩小，从左上角放大，或者让它消失。你有很多控制选项。实际上制作这类东西非常容易。让我们把它设置回大约 1 左右，以便稍后我们还可以玩。

好的。这是一个一般的线性变换。坦白说，在我看来，这才是你应该看待矩阵的方式。太多人把矩阵看作是数字的数组，就像一个表格，就像看电子表格一样，只是一个枯燥的数字表。而且有太多的线性代数课程会让你认为线性代数就是对这些数字表进行行操作和列操作。你永远不会从中获得关于矩阵真正是什么的直觉。但就我而言，这就是一个 $2 \times 2$ 矩阵的本质，并且这可以推广到 $3 \times 3$ 乃至 $n \times n$ 维的矩阵。

我们稍后会详细说明。但这次讲座的一个要点是：矩阵是什么？如果你认为它是一堆数字的集合，如果你一生都是这样看待矩阵的，那么现在你有机会真正了解“大人物们”是如何看待矩阵的。这是一种线性变换。

我们稍后再谈。但趁现在，让我们再玩几个变换。我已经玩过剪切了。还有一个很有趣的是扭曲（Warp）。

这是一个非线性变换。让我们来扭曲这些可怜的柯基。它们变形了，看那个。正如你可能想象的，我们可以把柯基放进洗衣机里转，就像 Photoshop 能做的那样。这个扭曲是一个非线性变换。这些是在线性代数课上你看不到的变换类型，因为在前计算机时代，要在现实生活中看到这些东西真的很难。我的意思是，你可以一个像素一个像素地画，但这会非常非常困难。

> **问：** （来自助教/聊天）Allan，你能放大你的浏览器吗？
> 
> **答：** 放大。好主意。技巧是，我需要保留控制参数在屏幕上。我不知道能不能做到。这大概是我能放大的极限了，同时还能操作。看看能不能再调一下。好，我可以再放大一级。好了。这就是被扭曲的柯基。

我们还有一些其他的图像，例如复合图像，你可以... 比如，先旋转柯基。旋转它们。它们就在那儿。现在 `alpha` 控制旋转。我想这是一个相当简单的变换。这个是线性的。

然后我们可以复合事物。例如，我们可以先进行旋转，然后进行扭曲。它在旋转。

> **问：** （来自助教/聊天）你能提一下如何从视觉上区分一个变换是线性的还是非线性的吗？
> 
> **答：** 嗯，我希望同学们先思考一下这个问题。当然，如果有人想在 Zoom 聊天或 Discord 里告诉我，他们可以告诉我。想想看是什么让它看起来是线性的，而什么是非线性的，然后我们可以讨论这意味着什么。

好的。我只是想先设定好这个想法，让事情开始。我本可以在讲座后面再展示这些，但我想让你们立刻看到二维的变换，线性的和非线性的。

我们还有几个变换。也许我该把它们拿出来。有 `xy` 变换。这是一个非常奇怪的变换。还有一个我准备的变换，你可以定义自己的，只要写你自己的函数就行，但是 `rθ` 变换是另一个我们准备好的、看起来很有趣的变换。就是这个像扇形展开的。

好的。第一个要点是变换，线性和非线性变换。就像我说的，我的目标是让你用新的方式看待线性代数和多元微积分。我真的相信——我很想在讲座后听听你们的意见——但我确实相信，即使你没有学过这些课程，在 Julia 里看到它，你也能获得直觉。或者你可以先上课，然后从这个讲座中获得更好的直觉。而且我仍然相信，即使是专家，即使是那些认为自己对多元微积分和线性代数无所不知的人，仍然能从能够实时操作这些东西中有所收获。仅仅因为你理解这些东西，并不意味着你不能进一步巩固你的直觉。

好了，那有点像电影的预告片。现在，让我们正式开始讲座。我们将学习一点三角学，学习一点现代计算机科学，像往常一样，一并进行。

让我们从一些非常简单的事情开始。还记得上次我们定义了图像的线性组合吗？你记得我们通过乘以一个常数来缩放图像。我们还通过将每个像素的颜色相加来组合图像。我们说过，如果你组合这两种操作，你通常会得到线性组合。今天我们从另一个角度来看重组，但让我们从真正真正简单的，几乎是高中水平的东西开始。

## 三、 Julia 中的函数定义

数学和 Julia 中的函数。我写下了几个单变量函数。这里有平方函数 $f_1(x) = x^2$，正弦函数 $f_2(x) = \sin(x)$，以及平方函数的推广 $f_3(x, \alpha) = x^\alpha$，其中 $x$ 可以取任何指数 $\alpha$。

在 Julia 中，你可以用多种方式表达这些。我来展示给你看。有所谓的**短格式**，你写 $f_1(x) = x^2$。并且，为了遵循数学的下标，我只想告诉你，如果你输入 `f/_1` 然后按 Tab 键，你会得到下标 1，或者我可以输入 7，得到下标 7。这是让你的代码看起来更像数学的一种方式。我的意思是，我本可以写 `f1`，但我看到人们这样写 `alpha`（指希腊字母 $\alpha$），而不是写 `alpha`（指英文单词）。这似乎是一个明显的迹象，表明你是在计算机上，而不是在做数学。所以这里，我们通过用一种漂亮的方式书写事物，模糊了数学和计算之间的界限。

回到函数调用。我可以简单地通过说 $f_1(x) = x^2$ 来创建平方函数，然后你可以看到 $f_1(5)$ 是 25。这是一种方式。这是写函数的短格式方式。正如你可能预料到的，当函数不是很复杂，可以用一个或两个表达式完成时，短格式是一种方便的定义函数的方法。

然后有所谓的**匿名形式**。对于了解 Lisp 的人来说，这就是 lambda 表达式，但现在它们通常只被称为匿名形式。所以这里，不是给它一个像 `f2` 这样的名字，"匿名"这个词意味着没有名字。函数就是那个将 $x$ 映射到 $\sin(x)$ 的东西。就是这样。函数没有名字。它只是那个映射 $x$ 到 $\sin(x)$ 的事物。所以你可以拿这个东西在 $\pi/2$ 处求值，它当然会计算 $\sin(\pi/2)$，结果是 1。当然，我可以继续给它一个名字。它就不再是匿名的了，但我可以说 `my_sine` 是这个函数 $x -> \sin(x)$，然后我可以继续在 $\pi/2$ 处求值，它当然会工作。所以我得到了 1。这是匿名的，但我可以给它一个名字。

好的。我们有了函数的短格式、匿名形式。然后是**长格式**，你之前已经见过几次了。你用 `function` 关键字开始，用 `end` 结束，并在中间放入返回值。你不必写 `return` 这个词，但它让代码更容易阅读。我只是加入了另一个知识点，你可以有一个默认参数 `alpha=3`。它的工作方式是，如果我计算 $f_3(5)$，它会计算立方。所以指数 `alpha` 在我不指定时被理解为 3。但如果我确实包含了它，比如 $f_3(5, 2)$，那么就会使用那个值。所以 $5^2$ 是 25。

这里是 `f3`。让我还快速提一下**关键字参数**的概念，因为它们很方便。我们用分号 `;` 代替逗号。这意味着如果你想指定指数是多少，你必须输入 `alpha=...`。这样，关键字（也许称之为 `exponent` 会更好，例如 $f_3(x; exponent=3) = x^exponent$）会更易读。当你调用它时，你写 `f3(5, alpha=2)`，它就会运行。

好的。三种不同的写函数的方式。也许不是那么令人兴奋，但它们各有用途。短格式、匿名形式或长格式。每种都有它的用处，每种都有它的位置。当你写 Julia 一段时间后，你就会开始知道选择哪种。与此同时，可能真的不重要。你可以自由选择。你可以查看 Julia 的函数文档了解更多。

## 四、 自动微分 (Automatic Differentiation - Autodiff)

但让我进入更有趣的部分，那就是自动微分。这是一个热门话题。它因为机器学习而变得非常非常火热。在某种意义上，自动微分已经存在了几十年。我的意思是，我知道有人在二三十年前就在做这个了。但如今越来越多的人以多种方式应用它，而且编程语言本身也在做或者让你用微分做一些……我不知道，我觉得你几乎无法想象的事情。

为了把事情说清楚，让我们先实际运行一下，以确保我们知道我们在做什么。我们将使用 `ForwardDiff` 包。还有一种叫做反向微分（Backward Differentiation），但我们下次再说。我将加载 `ForwardDiff` 包，并开始进行一些数值求导。

`f1`，你可能记得是 $x^2$。所以它的导数是 $2x$。如果我在 5 处计算导数，我应该得到数字 10。这很好。

还有，我们之前定义的 $f_3(x, \alpha=3) = x^\alpha$，默认计算的是 $x^3$。让我们记住这个。它的导数当然是 $3x^2$。

> **问：** （来自助教/聊天）你能再放大一点吗？
> 
> **答：** 再放大一点。好的。甚至可能再放大一次。好主意，谢谢。在这个特定的 notebook 里，我可能需要根据情况放大和缩小。你知道通常的权衡。如果我只看一小部分，我可以一直放大。但如果我需要同时看到一堆东西，我就必须缩小。我会尽量处理好。

所以 $3x^2$，在 $x=5$ 时是 $3 \times 5^2 = 75$。好了。没什么大不了的。

我会告诉你什么是大不了的。大不了的是，这个导数是以一种不同于几乎所有人预期的方式计算的。我认为这才是关键。当你想到微分时，你可能想到你是怎么学它的。你可能学了所有的规则。规则并不多，比如 $\sin$ 的导数是 $\cos$。乘积的导数是 $udv + vdu$。你学了一堆规则，然后计算导数就变得很机械化了。我称之为**符号微分 (Symbolic Differentiation)**。这是 Mathematica 非常擅长的事情。你甚至可以用 Julia 做，但我们今天不打算这样做。我的观点是，这里发生的**不是**符号微分。这不像你在上微积分课时做的那样。

如果不是那样，那么你可能会认为它像你在数值计算课上做的，或者有时人们在微积分课上这样做，也就是做**有限差分 (finite difference)**。你把函数在稍微远一点的地方的值减去当前位置的值，然后除以你的位移，这就是对斜率的近似。所以这可能就是这里发生的事情。很多很多人会这么猜。有趣的是，这里发生的也**不是**有限差分。

这里发生的是一种不同的事情。如果这次讲座结束时有时间，我会告诉你更多关于发生了什么。但现在，我希望你记住的是，计算机现在正以新的、令人兴奋的方式计算导数。我的意思是，这种方式已经为人所知多年，但现在它正在大规模地发生，它真的让你能做一些我认为人们以前根本不知道能做的事情。这在机器学习中经常发生，但在其他地方也是如此。

我们稍后会看到更多，但现在我只想让你记住，对于一个标量函数，你所要做的就是写下函数，指定你想要计算导数的点，然后它就发生了。

我决定用有限差分做一次数值微分，只是为了给你们展示一下。这里，我有一个很小的 `epsilon` ($\epsilon$)。这里我将要——以防你从未知道导数是什么——这是在数学课上会怎么做。再次强调，`ForwardDiff` **不是**这样工作的，但这只是为了给你等价的数学概念。如果我想知道 $\sin$ 函数在点 1 处的导数，这个语法告诉你我在做什么：`ForwardDiff.derivative(sin, 1.0)`。我们知道这（解析解）是 $\cos(1)$，因为我们学过微积分。而这是那个有限差分的东西： `(sin(1.0 + epsilon) - sin(1.0)) / epsilon`，也就是 $\Delta y / \Delta x$ 那种东西。你可以看到，如果 $\epsilon$ 很大，比如 0.1，你会得到一个近似值。如果你把 $\epsilon$ 变小，你会得到越来越精确的数字，正如你所期望的。当 $\epsilon$ 是 $10^{-6}$ 时，在显示的数字位数内，它看起来与精确值完全匹配。在某种意义上，这就是导数的直觉。当然，这里（指 Autodiff）发生了一些神奇的事情，关于它是如何计算的。

标量函数。标量函数很简单。这只是热身。你们都知道那个。让我们继续讨论你在多元微积分课上可能看到的东西。MIT 的课程编号是 1802。MIT 对所有的课程都有这些奇怪的编号。据我所知，只有本科生和课程的教授知道课程的编号，而且本科生知道所有的编号，教授只知道他们教的课程的编号。

好的。让我们讨论**多变量标量值函数 (Scalar Valued Multivariate Functions)**。这在多元微积分里也是一个简单的概念。让我们定义一个函数 $f_5(x)$，它是一个三个变量的函数，就像我说的。我只是随便选了一个函数，它没有实际意义： $f_5(x_1, x_2, x_3) = 5 \sin(x_1 x_2) + 2x_2 / (4x_3)$。

在 Julia 中，我们现在有一个选择。我的意思是，在数学中，你也有选择，但你可能没想那么多。我们可以把它写成一个接受**向量**的函数。这是你如何把它写成一个接受向量的函数：`f5(v) = 5*sin(v[1]*v[2]) + 2*v[2]/(4*v[3])`。这是短格式，它表示“输入一个向量，然后取出它的分量”。这里是 $x_1, x_2, x_3$ 分量，或者我这里叫它 $v$，$v_1, v_2, v_3$ 分量。

或者，我发现更可读的方式是，你可以把它写成一个接受**三个参数**的函数：`f5(x1, x2, x3) = ...`。但这样做的缺点是它不适用于向量。所以你可以写成三个参数的函数，或者一个接受向量的函数——这个（指多参数版本）肯定比那个（指向量版本）更可读。我想每个人都会同意。但在实践中，你真正想做的是处理向量函数。我稍后会告诉你如何解决这个问题。但这里有一个折中的步骤，那就是，用可读的格式写它。我们现在叫它 `f6`，只是给它一个新名字。你可以用这种可读的格式写它。然后当你需要处理向量时，你可以就这样调用它（指定义一个接受向量的方法，内部调用多参数版本）。Julia 会允许你这样做。Julia 说没关系。如果你用三个参数调用这个东西，这个函数会被调用。如果你用一个参数（向量）调用它，这个（接受向量的）方法会被调用。能够让函数仅根据参数数量进行派发（dispatch）是非常非常方便的。这样，你可以为同一个函数编写一堆不同的方法，它就能以两种方式工作了。

你可以看到这里——我可以带三个参数调用它 `f6(1, 2, 3)`，或者你记得第一次讲座，这是定义向量的方式 `[1, 2, 3]`。所以这里是 `f5` 直接应用于向量 `f5([1, 2, 3])`。也许在这里加点空格能强调重点，这是向量函数，这是三个标量参数的函数。这里我用 `f6` 做同样的事情，只是为了展示它也能工作 `f6([1, 2, 3])`。

顺便说一下，有一个技巧可以让你一次性定义它，既有可读性，又能处理向量。我曾犹豫是否应该在课程的第三讲就向你们展示这个，但我认为值得展示，因为它确实让代码更具可读性。这里，这个双括号 `f7((x, y, z)) = ...`，基本上意味着一个接受元组（tuple）的函数。这是一个微不足道的事情。但别担心它是如何工作的。只需指出，当我这样写时，它很可读 $x, y, z$ 就像数学一样。但它实际上接受向量参数。所以你可以写一个既可读又接受向量参数的函数，以防你想这样做。

回到函数 `f5`。你可能会问，“嗯，它是这个接受三个参数的函数，还是这个接受向量参数的函数？” 嗯，如果你调用 `methods(f5)`，你可以看到它实际上——读起来有点丑，但你可以看到 `f5` 的方法，有一个单参数（向量）版本和一个三参数版本。如果你把它和 `methods(+)` 比较，你会发现加号有非常多的方法，多到几乎不好意思承认。加号被如此重载，以至于你几乎看不清。你可以加矩阵，加向量，加数字，加布尔值，加实数和浮点数。所有这些不同的东西都出现在加法里。

好的。但这里的底线是，只是几种不同的写函数的方式。

我们现在在哪里？让我们看看目录。好的，我们正在讨论多变量标量值函数。现在我想讨论的是**多变量标量值函数的导数**。

好的，让我们求一些导数。有一个词叫**梯度 (Gradient)**。再次，也许这对某些人来说有点吓人。也许你们中的一些人已经计算梯度很多很多年了。但如果你有一个标量函数，你计算它的梯度，无论那是什么，梯度**就是从向量到标量的函数的导数**。它真的就是这个。

所以如果我计算这个 `f5` 函数——它的定义在哪里？在上面。它被定义成什么并不重要，但这里是 $5 \sin(x_1 x_2) + 2x_2 / (4x_3)$。你现在在一个向量上计算它，你会得到一个向量输出（指梯度）。哦，这里有个提醒，它是什么。

再次强调，这不是自动微分的工作方式。但以防你没有真正抓住梯度的直觉，或者你以前从未见过它，这小段代码就是你获得梯度直觉所需要的全部。你可能还记得我之前的 $\epsilon$。你所要做的就是，我们有一个三个变量的函数。你只需扰动第一个参数，并与你开始的地方比较。所以这就像是 $\Delta f / \epsilon$ 对于第一个参数。然后你对第二个参数做同样的事情。$\epsilon$ 在这里（第二个位置）。然后你对第三个参数做同样的事情。然后梯度就是结合了所有这些的向量。你可以看到——这里，让它竖排显示。你可以看到，无论自动微分器做了什么魔法，它给出的答案在足够多的小数位上是令人信服的。我想这里是 7，这里是 8，但它非常令人信服地表明这就是梯度。

也许我在重复自己，但我觉得这是一个非常重要的点。如果你上过多元微积分课，你可能练习过计算梯度，也许你很擅长。但对我来说，仍然有一些奇妙之处。也许只有我这样。我不知道我是否能与互联网上的观众和在场的 MIT 学生分享这一点。但仅仅是看到“哦，我稍微改变了 x，我稍微改变了 y，我稍微改变了 z，我得到了和那个魔法（Autodiff）一样的答案”，这真是太好了。对我来说，我喜欢再次看到这些东西，如果我能分享这份喜悦。但我当然希望，这个数值方法能让梯度的概念更直观。

现在左右都在发生的事情是机器学习或其他一些优化背景下，每个人都想优化包含许多参数的标量函数。在机器学习世界里，这被称为**损失函数 (Loss Function)**，每个人都在沿着**负梯度**方向走。你所做的就是，你有一个标量函数，你的目标是让那个函数尽可能小。所以你从某个向量开始，从一个点开始，你计算这个方向（负梯度），然后你沿着那个方向走一小步，然后你一次又一次地这样做。这是最小化函数的最简单方法。可能也是当你有很多变量时（比如机器学习中）最好的方法。当你只有几个变量时，我要说有比沿着梯度走好得多的技术，但如今人们有如此多的变量，你真的负担不起那些花哨的技术，所以你就用沿着梯度走的简单技术。

好了。我想你看到我要去哪里了。我们做了单变量函数（标量到标量）。然后在接下来的部分，我们做了向量输入，标量输出。现在，对于真正的大场面，让我们做**向量输入，向量输出 (Vector Valued Multivariate Functions)**。这才是真正有意思的地方。

所以，向量值函数，或者说向量值多变量函数。输入的是一个向量，输出的也是一个向量。这里有一些函数，其中一些在讲座一开始就展示了。但这里，你可以看到一堆函数，其中一些带有参数，它们都被设计用来处理向量。让我把这一点说得非常清楚：下面所有的函数都被设计用来处理大小为 2 的向量。

让我给你看几个函数。有**恒等函数 (identity function)**。世界上最无聊的函数，可能仅次于零函数。你输入向量 `(x, y)`（用了我之前展示的技巧），输出向量 `(x, y)`。但这是一个典型的**线性函数**的例子：`linear_transform((x, y)) = [2x + 3y, x - 4y]`。这里的系数 2, 3, 1, -4 没有什么特别之处，但这是你典型的线性函数。

这里是在 x 方向按 $\alpha$ **缩放 (scale)**：`scale_x((x, y), alpha) = [alpha*x, y]`。注意到这里发生了什么——我现在有了一个带参数 $\alpha$ 的函数。但这没关系。`scale_x` 是一个函数，它接受一个向量和一个参数 $\alpha$，并在 x 方向上将该向量缩放 $\alpha$ 倍。当然，`scale_y` 是另一个函数，它接受向量 `(x, y)` 和参数 `alpha`，并在 y 方向上缩放。

这是一个**旋转 (rotation)**：`rot((x, y), theta) = ...`（这里用了具体的 $\pi/6$，即 30 度）。这是**剪切 (shear)**，它也有一个参数 $\alpha$，$\alpha$ 是剪切的强度。然后这是一般的线性变换，我只是用了任意系数 a, b, c, d：`general_linear((x, y), a, b, c, d) = [ax + by, cx + dy]`。

只是为了展示一下，如果我调用 `rot( (3, 4), pi/2 )`，也就是旋转向量 (3, 4) 90 度，你会得到 `(-4, 3)`。（注：视频脚本这里似乎计算错了或口误，(3,4) 逆时针旋转90度应为(-4,3)，他得到的是(4,-3)可能是顺时针或坐标系不同，但此处按标准逆时针旋转结果写出）。

我打赌你注意到了，这些函数中的**所有**函数都可以用矩阵来定义。如果你学过一点线性代数，这甚至在 MIT 的 1802（多元微积分）课程中，在线性代数课之前就会出现。你可能已经注意到，我们可以——而不是用这种冗长笨拙的形式写出来——数学家们已经发明了矩阵表示法来书写。它变成了**矩阵乘以向量** $A \mathbf{v}$。在数学中，它看起来像这样：
$$
\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} ax + by \\ cx + dy \end{pmatrix}
$$
这种表示法有一些好处，因为它不是把系数分散在各处，而是将系数（决定函数的那些东西）和向量 $ (x, y) $ 分开。

这些都是线性的。是的，我刚才列出的每一个函数都可以用一个 $2 \times 2$ 矩阵来写。所有这些都可以用矩阵定义。

但是，如果你还记得之前的“扭曲柯基”（Warp Corgi），那个就不能用矩阵定义了。所以我们必须把它写出来。这里有**扭曲 (warp)**，还有那个 `rθ` 和 `xy` 函数。你不能用矩阵来写这些，但它们仍然是向量到向量的函数。这些是**非线性函数**，但它们仍然接受向量并输出向量。

扭曲函数可能看起来有点复杂，但它实际上并不那么复杂。我会稍后解释为什么。你所做的是，你计算点 `(x, y)` 到原点的距离 $r = \sqrt{x^2 + y^2}$，然后你用这个距离乘以你的参数 `alpha` 来得到旋转角度 `theta = alpha * r`。然后你旋转向量 `(x, y)` 这个角度 `theta`。所以发生的是，你的旋转不仅取决于你的参数 `alpha`，还取决于你的输入参数 `(x, y)`。

你们中有些人可能认出 `rθ` 和 `xy` 是从极坐标到笛卡尔坐标的典型转换。我想你们很多人在高中或者更晚的时候见过这个。我不知道他们现在高中还做不做这些。也许你们中有人可以告诉我他们是否还在高中做这些。但这些是极坐标和笛卡尔坐标之间的转换。

既然我在谈论写函数的酷方法，我想提一下在 Julia 中写相同函数的另一种有趣的方式。再说一次，这些是你可能需要看五次才能明白怎么回事的东西。但一旦你学会了如何做这些事情，你就可以自己做了，你会开始意识到 Julia 最终可以多么优雅。例如，如果我想写一个 `warp` 函数，并且我不在乎输入是否是向量... 我们看到 `warp` 依赖于一个参数 `alpha` 和一个输入参数 `(x, y)`。所以，让我们先不担心输入的格式，只用一个扁平的结构来做，只看 `alpha`, `x` 和 `y`。我们看到，因为我们已经定义了一个旋转函数 `rot(v, angle)`，我们可以创建这样一个函数：它旋转的角度是 `parameter * distance_to_origin`。这能工作，只是它不接受 Julia 向量作为输入，它接受三个参数。但没关系。我们现在知道该怎么做了。我们可以将函数定义为这样一个匿名函数：它接受一个向量 `v`，并应用我们刚刚定义的那个需要三个参数的函数 `warp_internal(alpha, v[1], v[2])`。
（注：讲者这里的解释有点绕，更简洁的做法可能是定义一个高阶函数 `warp(alpha)` 返回一个接受向量 `v` 的函数，该函数内部计算 `r` 和 `theta` 并调用 `rot`。）
我只是想检查一下它是否有效，我用冗长复杂的方式写的 `warp` 函数和我这样写的 `warp2` 函数是否给出相同的答案。结果表明它们是正确的。

现在，最有趣的部分来了：**变换的自动微分 (Automatic Differentiation of Transformations)**。我将把这些（向量到向量的函数）称为**变换 (transformations)**。我想“变换”这个词……我几乎觉得我应该查一下，但是函数和变换之间有什么区别？可能除了词语的使用方式外没什么区别。我想我们用“变换”这个词时，通常是指输入和输出都是多维的。否则，我不认为我们称它们为变换。一个标量函数到一个标量函数严格来说可能也是一个变换，就像一个正方形严格来说是一个矩形，但你不会在指正方形时说矩形。你就是不这样做。同样地，你不会在指一个（标量值）函数时说变换。而且我不认为人们对向量到标量的函数经常使用“变换”这个词。我想向量到向量，或者更高维到更高维，那是这些词汇使用的地方。

对于更高维导数的词是**雅可比矩阵 (Jacobian)**。所以在某种程度上，词汇是：导数（derivative），那是初等微积分；梯度（gradient），那是当你从向量到标量时得到的；而向量到向量，词是雅可比矩阵（Jacobian）。但它们在某种程度上都是雅可比矩阵。它们都是同一件事，只是推广了。

让我们取 `warp(3)`。`warp` 有一个参数，所以我们取 `alpha=3` 的 `warp`。让我们称之为 `w = warp(3)`。让我们在点 `(4, 5)` 处求它的导数。现在，记住，`warp` 接受一个二维向量输入，并输出一个二维向量。别忘了这个。所以 `warp` 接受一个二维向量输入，输出一个二维向量。在你意识到之前，`warp` 在这一点的**导数是一个 $2 \times 2$ 矩阵**。它有四个数字。

再次，如果你不……也许你从未见过雅可比矩阵，也许你不知道数学定义，或者你知道，但在这里很容易看到定义。我们将要做的是，我们将取 `warp` 函数，并在第一个参数上稍微增加一点 `epsilon` 来计算它 `w((4+epsilon, 5))`，然后减去原始值 `w((4, 5))`，再除以 `epsilon`。这就像任何导数一样，我们先对第一个参数做。然后我们对第二个参数做同样的事情 `(w((4, 5+epsilon)) - w((4, 5))) / epsilon`。

也许值得单独看一下这些部分。这是 `wdx = (w((4+epsilon, 5)) - w((4, 5))) / epsilon`。我们让它垂直显示，更有启发性。但请记住，如果这个函数接受一个二维向量输入并输出一个二维向量，那么 `wdx` 这里就是一个二维向量。我用一个二维向量减去另一个二维向量，结果将是一个二维向量。看那个！这就是雅可比矩阵的第一列。当然，我可以得到雅可比矩阵的第二部分 `wdy = (w((4, 5+epsilon)) - w((4, 5))) / epsilon`，你会看到这个东西是第二列。当然，你只需将两者连接起来，就得到了这个 $2 \times 2$ 矩阵。所以从数学的角度来看，这就是雅可比矩阵。再次，你们中有些人可能已经知道了。我仍然喜欢在数值上看到它。我就是喜欢能够看到我只需做这个差分，答案就在那里。

## 五、 应用与解释

让我们回到之前，现在我们玩得更多了。让我们再玩一下这个。我回到一般的线性变换，我要把这个（控制矩阵）移到图片下面。让我们重新运行这段代码，让它变成单位矩阵。

我重复一下我在讲座开始部分说过的话。选择一个矩阵就选择了一个变换。这就是我希望你们看待矩阵的方式。一个矩阵就是一个变换。当人们告诉你如何做矩阵乘以向量时，他们实际上是在告诉你，你可以把每个像素当作一个向量。你知道我可以做什么来强调这一点吗？我可以用我的 Zoom 画笔。

这里，让我们举个例子。让我们取……哦，我不知道……中间柯基耳朵的尖端，在这个点。我会称之为 `(0.2, 0.3)`。向右 0.2，向上 0.3，不完全是尖端，但我在这里，在柯基的耳朵上。现在，如果我应用一个矩阵 $A$ 到它上面，我要做矩阵乘以向量 $A \begin{pmatrix} 0.2 \\ 0.3 \end{pmatrix}$，我会得到一个新的点。现在，我稍微骗了你们一下，有人会抓住我，因为我实际上是在变换坐标系，但我们先不担心这个。所以一次矩阵乘以向量会告诉我——我不知道它会显示成什么样，这取决于矩阵。但这里，让我画一张图，让你想象它会是什么样子。这将是点 $A \begin{pmatrix} 0.2 \\ 0.3 \end{pmatrix}$。这是图像最终到达的地方。这就像你有一个被压扁的坐标系。

所以所有矩阵乘以向量所做的就是，它取每一个——它不是做一次矩阵乘向量。而是，你对整个图片同时做这个操作。这张图片中的每个点都有一个坐标。然后当你挤压东西时——让我现在清除这个——当我开始移动我的矩阵时，但每个点，整个图像都会移动到另一个地方。所以它不是一次矩阵乘向量。它是矩阵乘以你这里的**每一个**向量，每一个点。这就是线性变换所做的。

> **问：** （回顾之前的问题）那么有人想告诉我，你怎么能看着网格线告诉我这个变换是否可以写成矩阵乘以向量的形式？
> 
> **答：** （来自 Discord/聊天）Discord 上有回应。直线输入变成直线输出，反之亦然。共线的点保持共线。
> 
> **答：** （教授回应）好的。所有这些都是对的，但这实际上还不够。我可以向你保证，如果你没有这些特性，那么你的线性变换就不是线性的。然而，仅凭这些还不够。例如，如果你有一种情况，比如说你的网格线开始变成这样（画图示意，例如水平线等距，垂直线不等距）。我认为你可以组织它，使得所有的线……实际上，我不太确定非垂直线会发生什么。也许它们会以奇怪的方式拉伸。我只是认为这不完全够。嗯，也许够了。也许我错了。让我思考一下。实际上，它可能够了。我不认为够了。我必须想一个反例。（有人提示双线性变换）好吧，现在我得思考了。我不能在直播电视上实时思考得很好。（有人提示仿射变换）是的，但我们先不担心仿射变换。无论如何，这不是我想要的答案。它是否正确，我得想想。还有其他人给出答案吗？
> 
> **答：** （来自 Discord/聊天，另一条）它们还必须保持等距。
> 
> **答：** （教授回应）是的。这几乎就是我想要的了。我想要一个两个词的答案。也许我只是——我寻找的是：**网格线变成全等的平行四边形 (congruent parallelograms)**。这是我想要的答案。我不确定其他答案是错的。它们可能也是对的，但我想表达的方式是“全等的平行四边形”。所有这些（线性变换）都具有全等的平行四边形。这就是使它成为线性的原因。

顺便说一下，你们中有些人可能知道有个东西叫做**行列式 (Determinant)**。如果你有一个矩阵，提醒一下，如果你有一个矩阵 $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$，有一个叫做行列式的函数，它是 $det(A) = ad - bc$。在线性代数课上，你学了很多关于行列式如何告诉你矩阵是否奇异，是否可以找到逆矩阵。但是你们中有多少人首先学到的是，**行列式是柯基的鼻子（或任何区域）被缩放的程度？**

说清楚点，让我们回到单位矩阵。你看，这里有——你可以取任何——鼻子显然没有什么特别的。你可以取这只柯基的耳朵。这是它的耳朵。你可以称之为变换前的面积。然后你应用一个变换。我现在把耳朵变换到了一个新的地方。也许它到了这里。这个矩阵（指剪切矩阵示例）的行列式是 1。所以即使它被扭曲了（剪切了），面积实际上保持不变，因为这个矩阵的行列式是 1。

但**行列式通常是缩放因子**。所以如果你取 变换后的面积 / 变换前的面积 —— 你想要的任何面积，只要它是一个区域 —— 对于线性变换，这就是行列式的几何解释。我认为这通常比矩阵是否奇异有趣得多。

好了。我快要到时间了。让我看看在结束前还能快速告诉你些什么。我们下周一可以再多做一点这个。关于矩阵，网上有一些评论，实际上是矩阵的逆在起作用。如果我继续做……是的，我想我需要更多时间。所以我可能会在周一继续这个话题。

但我想提一下，如果你有一个**非线性变换**，当你足够仔细地看时，你确实能看到**平行四边形（近似）**，但当你放大看时，它们不再是全等的了。但是在局部，在一个小邻域内，**每个（光滑）函数都是线性的**（可以通过线性函数很好地近似），然后**雅可比矩阵的行列式**会告诉你面积是如何缩放的。我会在周一详细说明这一点。

## 六、 资源与总结

对于那些想在周一之前做些事情的人，如果你们想知道——我告诉过你们自动微分是神秘而有趣的——我在几年前做了一个关于这个的 10 分钟视频（好吧，实际上是 11 分钟）。你们可能想看一下，因为它真的会向你展示你学习微积分的方式和计算机现在计算导数的方式之间的区别。

我还对超过三维的情况做了一些评论。如果你们喜欢，可以自己阅读。

好了。那么我要和互联网世界的朋友们说再见了。MIT 的学生如果愿意可以留下来。我们周一会带来更多可以用 Julia、数学和计算思维做的有趣的事情。

好的。现在，谁有终止的权力？谁是主持人？是我。那么我们停止吧。

---

# 要点回顾

**一、 引言与目标**

-   本讲座旨在展示编程语言（Julia）的能力，并结合数学概念（多变量微积分、线性代数）进行实践。
-   核心目标：通过计算机实践加深对数学概念的理解和直觉，无论学习者背景如何（初学、已学或专家）。
-   主要议题：变换（Transformations）与自动微分（Automatic Differentiation, Autodiff）。

**二、 图像变换：可视化与概念**

-   **变换的直观演示：**
    -   使用图像（如柯基犬图片）展示各种二维变换效果。
    -   交互式演示允许实时调整参数并观察效果。
-   **线性变换 (Linear Transformations):**
    -   **一般线性变换：** 通过一个 $2 \times 2$ 矩阵控制，可以通过调整矩阵元素实现拉伸、缩放等。
    -   **剪切变换 (Shear Transformation)：** 线性变换的一种，使正方形变为平行四边形。
    -   **旋转 (Rotation)：** 线性变换的一种，通过角度参数控制旋转。
    -   **核心观点：** 矩阵不仅是数字数组，其本质是线性变换。可视化操作有助于理解矩阵的几何意义。
-   **非线性变换 (Non-Linear Transformations):**
    -   **扭曲/变形 (Warp)：** 坐标变换依赖于位置本身（例如，旋转角度与到原点的距离有关），网格线会弯曲。
    -   **其他非线性变换示例：** `xy` 变换，`rθ` 变换（类似极坐标变换）。
    -   这类变换无法用单一矩阵乘法表示。
-   **变换的复合 (Compose Transformation):**
    -   可以将多个变换按顺序叠加（例如，先旋转再扭曲）。
-   **区分线性与非线性变换（视觉线索）：**
    -   观察网格线的变化。
    -   线性变换：直线保持为直线，平行线保持平行，网格方块变成**全等的平行四边形**。
    -   非线性变换：直线可能变弯曲，平行关系可能破坏，网格方块可能变成各种形状且大小不一。

**三、 Julia 中的函数定义**

-   **函数定义方式：** (作为实现变换和微分的基础)
    -   **短格式 (short form):** 如 $f_1(x) = x^2$。
    -   **匿名函数 (anonymous form):** 如 $x -> \sin(x)$，没有固定名称，可以赋值给变量。
    -   **长格式 (long form):** 使用 `function ... end` 结构，可包含更复杂逻辑和 `return` 语句。
-   **函数参数：**
    -   位置参数。
    -   默认参数：如 `f(x, alpha=3) = x^alpha`。
    -   关键字参数 (keyword arguments)：使用分号 `;` 分隔，调用时需指定名称，如 `f(x; exponent=2) = x^exponent`。
-   **处理向量输入：**
    -   函数可直接接受向量作为参数 $f(v)$，并在内部访问其元素 $v[1], v[2]$。
    -   可利用元组解构 (tuple destructuring) `f((x, y))` 来提高可读性，同时接受向量输入。
    -   多重派发 (Multiple Dispatch) 允许为同一函数名定义处理不同参数类型/数量的方法（例如，同时定义 $f(x,y,z)$ 和 $f(v)$）。

**四、 自动微分 (Automatic Differentiation - Autodiff)**

-   **核心概念：**
    -   一种强大的计算导数的技术，近年因机器学习而变得非常热门。
    -   **重要区别：**
        -   它**不是**符号微分（Symbolic Differentiation）：不像数学推导那样操纵表达式。
        -   它**不是**数值微分（Numerical Differentiation）：不像有限差分法那样用 $$(f(x+\epsilon) - f(x)) / \epsilon$$ 近似导数。
    -   Autodiff 通过追踪计算过程（计算图）和应用链式法则来精确计算导数值（达到机器精度）。
-   **单变量函数 (Scalar -> Scalar):**
    -   输入标量，输出标量。
    -   使用 `ForwardDiff` 等包计算函数在某点的导数值。
    -   示例：计算 $f(x) = x^2$ 在 $x=5$ 处的导数，结果为 $10$。
-   **多变量标量值函数 (Vector -> Scalar):**
    -   输入向量，输出标量。
    -   其导数称为**梯度 (Gradient)**，是一个向量，包含函数对每个输入变量的偏导数： $∇f = [\partial f/\partial v_1, \partial f/\partial v_2, ..., \partial f/\partial v_n]$。
    -   使用 `gradient(f, v)` 计算。
    -   可以通过有限差分逐个扰动输入变量来验证梯度计算的直观含义。
-   **向量值多变量函数/变换 (Vector -> Vector):**
    -   输入向量，输出向量。通常称为**变换 (Transformation)**。
    -   其导数称为**雅可比矩阵 (Jacobian Matrix)**，是一个矩阵。
    -   雅可比矩阵 $J$ 的元素 $J_{ij}$ 是第 $i$ 个输出分量对第 $j$ 个输入变量的偏导数： $J_{ij} = \partial u_i / \partial v_j$。
    -   使用 `jacobian(T, v)` 计算。
    -   可以通过有限差分观察每个输入变量的微小变动对整个输出向量的影响，其结果对应雅可比矩阵的列向量。

**五、 应用与解释**

-   **机器学习中的优化：**
    -   通常需要最小化一个**损失函数 (Loss Function)**，这是一个关于模型参数（很多变量）的标量函数。
    -   **梯度下降法**利用负梯度方向 (`-∇f`) 迭代更新参数以寻找最小值。Autodiff使得计算复杂模型的梯度变得高效可行。
-   **行列式 (Determinant) 的几何意义：**
    -   对于一个线性变换（由矩阵 $A$ 表示），其行列式的绝对值 $|det(A)|$ 代表了**面积（二维）或体积（高维）的缩放因子**。
    -   即：变换后的面积 / 变换前的面积 $= |det(A)|$。
    -   这比仅用行列式判断矩阵是否可逆提供了更直观的几何理解。
    -   对于非线性变换，在某一点的**雅可比矩阵的行列式**描述了该点邻域的**局部面积缩放因子**。

**六、 资源与总结**

-   提及了一个关于 Julia 自动微分的 10 分钟介绍视频资源。
-   简要提到了更高维度的情况。
-   讲座结束，预告后续内容。