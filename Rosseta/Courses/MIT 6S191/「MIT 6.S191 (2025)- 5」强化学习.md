

> - 视频链接：[MIT 6.S191: Reinforcement Learning](https://www.youtube.com/watch?v=to-lHJfK4pw&t=109s)
 >- 官方：[链接](https://www.youtube.com/@AAmini)


### 导言

本讲座实录源自 MIT 深度学习系列课程的第五讲，核心主题为深度强化学习（Deep Reinforcement Learning, DRL），旨在系统性地介绍如何将深度学习的表征能力与强化学习的决策能力相结合。讲座首先明确了 DRL 相较于传统监督学习和无监督学习的独特性，强调其通过智能体与环境的动态交互进行学习，而非依赖静态数据集。为了奠定基础，讲座详细梳理了强化学习的基本构成要素，包括智能体、环境、状态、动作、奖励、回报以及关键的 Q 函数概念，并解释了它们在决策过程中的作用。

随后，内容深入探讨了实现 DRL 的两大核心方法：基于价值的 Q-Learning 和基于策略的 Policy Learning。针对 Q-Learning，重点介绍了 Deep Q-Networks (DQN) 的原理、网络结构、训练机制（目标 Q 值与损失函数），并通过 Atari 游戏实例展示了其处理离散动作任务的能力与局限。接着，讲座转向 Policy Learning，阐述了策略梯度（Policy Gradients）方法如何直接学习策略函数、输出动作概率、通过采样进行决策，并特别强调了其处理连续动作空间（如自动驾驶控制）的优势及其训练流程。

最后，讲座结合了如自动驾驶的高保真仿真训练、以及 AlphaGo 在围棋领域的突破性进展等案例，进一步揭示了 DRL 在解决复杂决策问题上的巨大潜力与实际应用中的挑战。

### 内容纲要

```
深度强化学习 (MIT 深度学习第五讲)
├── I. 深度强化学习 (DRL) 导论
│   ├── 定义：深度学习 + 强化学习
│   ├── 核心特点：动态交互、超越监督学习范式
│   └── 应用领域：自动驾驶、机器人、游戏
├── II. 学习算法范式回顾
│   ├── 架构 vs. 算法
│   ├── 监督学习 (Supervised Learning)
│   │   └── 输入(X, Y)，目标(映射)，例子(苹果识别)
│   ├── 无监督学习 (Unsupervised Learning)
│   │   └── 输入(X)，目标(结构/相似性)，例子(苹果聚类)
│   └── 强化学习 (Reinforcement Learning)
│       └── 输入(状态-动作对)，目标(最大化奖励)，例子(苹果生存)
├── III. 强化学习核心术语与流程
│   ├── 智能体 (Agent)
│   ├── 环境 (Environment)
│   ├── 动作 (Action) - 离散 vs. 连续
│   ├── 状态 (State) / 观察 (Observation)
│   ├── 奖励 (Reward - r_t) - 即时反馈
│   ├── 回报 (Return - R_t) - 未来折扣奖励总和
│   │   └── 折扣因子 (Discount Factor - γ)
│   └── Q函数 (Q-function - Q(s, a))
│       ├── 定义：状态s下执行动作a的预期未来回报
│       └── 作用：用于选择最优动作 (argmax_a Q(s, a))
├── IV. 深度强化学习的两大类方法
│   ├── 基于价值的学习 (Value Learning / Q-Learning)
│   │   └── 学习Q函数 -> 推导策略
│   └── 基于策略的学习 (Policy Learning)
│       └── 直接学习策略函数 π(s) -> 输出动作/概率
├── V. 深入 Q-Learning (Deep Q-Networks - DQN)
│   ├── 示例：Atari Breakout 游戏
│   │   └── 智能体、动作、目标
│   ├── Q值估计挑战：人类直觉 vs. 最优策略 (侧面击球)
│   ├── 用神经网络学习 Q 函数 (DQN)
│   │   ├── 网络结构：输入(状态/图像)，输出(各动作Q值)
│   │   ├── 训练目标：预测Q值 ≈ 目标Q值
│   │   ├── 目标Q值计算 (贝尔曼思想)
│   │   └── 损失函数 (Q Loss) 与反向传播
│   ├── DQN 训练流程总结
│   ├── DQN 的成功与局限性
│   │   ├── 成功：超越人类水平 (Atari)
│   │   └── 局限：仅限离散动作、确定性策略
│   └── 问答环节 (Q&A)
│       ├── Q1: 未来动作假设与大动作空间处理
│       ├── Q2: 游戏规则对动作空间的约束
│       └── Q3: 奖励定义与校准 (主观性/情境化)
├── VI. 深入 Policy Learning (Policy Gradients)
│   ├── 目标：直接学习策略网络 π(s) -> 输出动作概率
│   ├── 动作选择：从概率分布中采样
│   ├── 主要优势
│   │   ├── 处理连续动作空间
│   │   └── 随机性策略 (探索、随机环境)
│   ├── 问答环节 (Q&A)
│   │   ├── Q4: 探索性
│   │   ├── Q5: 评估时间/复杂度
│   │   └── Q6: 直接学习策略 vs. 难算的Q函数
│   ├── 处理连续动作：参数化分布 (高斯分布 - 均值/方差)
│   ├── 训练方法：策略梯度 (Policy Gradients)
│   │   ├── 核心思想：增加高回报动作概率，降低低回报动作概率
│   │   ├── 训练流程示例 (自动驾驶)
│   │   │   ├── 初始化 -> 运行策略 -> 收集轨迹 -> 计算回报 -> 更新策略
│   │   │   └── 关键：增加好动作概率，减少坏动作概率
│   │   ├── 损失函数 (直观形式：-log(P(a|s)) * R_t)
│   │   └── 梯度更新
│   ├── 真实世界应用挑战：终止条件 (撞车)
│   └── 解决方案：高保真度仿真环境
├── VII. 高级应用案例: AlphaGo
│   ├── 背景：围棋的复杂性 (巨大状态/动作空间，稀疏奖励)
│   ├── AlphaGo 结合的技术
│   │   ├── 监督学习 (SL) 策略网络 (模仿人类)
│   │   ├── 强化学习 (RL) 策略网络 (自我对弈优化)
│   │   ├── 价值网络 (Value Network) (评估棋局胜率)
│   │   └── 蒙特卡洛树搜索 (MCTS) (结合策略/价值网络搜索)
│   └── AlphaGo Zero：移除监督学习，完全自我对弈
└── VIII. 总结与展望
    ├── 回顾 Q-Learning 与 Policy Learning
    └── DRL 应用潜力
```





## 讲座实录

### I. 深度强化学习 (Deep Reinforcement Learning - DRL) 导论

大家好，欢迎回到 MIT 深度学习的第五讲。今天，我们将学习如何将深度学习——我们这整个课程一直在学习的领域——与另一个历史悠久的领域，强化学习，结合起来。这两者的结合将是今天讲座的重点。

深度强化学习是两个领域一个非常奇妙的交叉点，因为它实际上超越了我们迄今为止在课程中所见、所学的一切。具体来说，它摆脱了那种试图基于我们已看到的数据进行学习，以做出模仿这些数据的预测或推断的范式。相反，强化学习与深度学习的结合，其重点在于构建模型（Agent）与其环境（Environment）和数据之间的**动态交互**。这不仅仅是拿一个预先收集好的数据集来训练深度学习模型，而是要有一个动态的环境，模型将与该环境互动以便学习。

我们的目标是，理想情况下，尝试在没有任何监督、没有任何人类标签的情况下完成这一点，或者尽可能少地使用人类标签。这在现实世界中有着许多显而易见的意义。我们看到这类技术的应用遍及从自动驾驶、机器人技术一直到游戏玩法等各个领域。事实上，深度强化学习的许多最初突破都来自于游戏玩法，我们也会看到一些始于这个领域的策略或近期工作，它们极大地推动了该领域的发展。

### II. 学习算法范式回顾

现在，我想先做一点基础铺垫，为我们今天接下来课程中将要看到的所有内容奠定基础。让我们先退一步，尝试理解强化学习是如何融入我们迄今为止在课堂上看到的各种不同类型学习算法的更大范式中的。

我们已经见过两种类型的东西，对吧？我们见过**架构 (architectures)**。第一讲我们讨论了全连接网络，第二讲是循环神经网络，第三讲是卷积神经网络。这些都是架构。然后我们开始讨论**学习算法 (learning algorithms)**：监督学习、无监督学习，以及今天的强化学习。算法与架构是无关的。

我们看到的第一个算法是**监督学习 (Supervised Learning)**。监督学习关注的是这类问题：给定 X（输入）和 Y（期望输出），我们的目标是学习一个模型，能在给定 X 时学会预测 Y，这是通过大量 X 和 Y 的示例来学习这种映射的。具体来说，一个类比或者直观的例子就是这个苹果：如果我向模型展示大量苹果的图片，并告诉它“这是苹果”，“这也是苹果”，那么当我展示一个看起来略有不同的新苹果时，它应该仍然能够识别出这还是一个苹果。

接下来，我们讨论了**无监督学习 (Unsupervised Learning)**，这是我们昨天第四讲看到的内容。在无监督学习中，我们只有数据，也就是只有 X，但没有 Y，没有标签。现在，用苹果的例子来说，就是如果我只给你看很多这类相似的东西，我不需要告诉你它们是苹果，但如果我再给你看这个类别里的另一个东西，模型应该能够检测到，即使你不知道这是什么或苹果意味着什么，仅仅通过观察许多这样的 X，你也应该能检测到特征上的相似性。

最后，在今天的课程中，**强化学习 (Reinforcement Learning)** 是我们将要涵盖的第三类算法。它将关注点从 X 和 Y 移开，现在我们得到的是所谓的**状态 (States)** 和**动作 (Actions)** 对。状态就是我们的模型在与环境交互时所观察到的信息，而动作是它如何与环境互动，如何做出决策以接收新的观察结果。我们将在接下来的几张幻灯片中让这个概念更具体，但强化学习的真正目标是利用所有这些状态-动作对——不再是像数据-标签对那样——而是利用状态-动作对，学习如何在这个环境中采取行动，从而进入**期望的状态 (desirable states)**。我们将讨论如何衡量“期望性”，这涉及到我们随时间赋予模型的**奖励 (rewards)**。一个很好的例子还是苹果场景：我们不必说“这是苹果”或“这不是苹果”，但仅仅通过与这些物品互动，模型应该能够学会它需要吃苹果来获取营养以求生存。

所以今天的讲座，我们将专注于这第三类算法，并看看我们如何在这个类型的世界中构建模型。

### III. 强化学习核心术语与流程

在我们继续深入之前，我想详细解释一下我在上一张幻灯片中介绍的所有术语。我将首先介绍我们整个讲座都会用到的词汇。这部分实际上是讲座中非常重要的一部分，因为课程的其余所有内容都将建立在这部分之上。

我们将从强化学习流程图的左侧开始：**智能体 (Agent)**。这是环境中的主要行动者，是做出决策、采取行动的主体。这可以是超级马里奥中的马里奥，可以是导航或送货场景中的无人机。在生活中，我们都是智能体，我们在我们的环境中行动并做出决策。

然后是**环境 (Environment)**。这仅仅是智能体存在并做出决策的场所或世界。

当然，这两者之间的互动是：智能体可以向环境发送**动作 (Actions)**。动作对应于该智能体可能执行的不同操作集合。例如，我们可以想象一个简单的二维世界，一个二维环境，我们的智能体只能在四个方向之一移动：上、下、左、右。你可以想象动作就存在于这个**离散空间 (discrete space)** 中。所以，你可以想象一个非常简化的世界，你只有四种可能的动作可以采取：上、下、左、右。或者，你可以想象一个更**连续的空间 (continuous space)**，就像我们所有人的生活一样，我们有连续且无限数量的动作可以定义我们的未来。

最后，这个反馈循环在另一侧也闭合了：环境向智能体发回**观察 (Observations)**。基于智能体采取的行动，环境会反馈回来，你可以将其视为一种回应，告知世界因其采取该行动而发生了怎样的新变化。这里还有一步：环境不仅以观察结果回应，它还以一种**奖励信号 (Reward Signal)** 的形式回应。这体现了智能体现在进入的状态有多么“令人满意”或“期望”。这就是奖励，我们称之为 R 在时间 t (r_t)。

最终，我们真正关心的不仅仅是单个时间点 t 的奖励，我们关注的是智能体在整个生命周期中能够累积的**总回报 (Total Reward / Return)**。智能体从时间 1 一直存活到时间 T，它随时间累积奖励，这通常是所有这些奖励从 1 到 T 的总和。

但通常情况下，我想强调这一点，仅仅考虑所有奖励的总和（即你随时间累积的每个奖励的聚合总和）并不总是最有用的，因为并非所有奖励都生而平等。例如，**未来的奖励 (future rewards)** 通常远不如**当前的奖励 (current rewards)** 有价值。你可以想象这样一个例子：如果我提议今天给你 5 美元，或者 10 年后给你 5 美元，即使不考虑通货膨胀或其他因素，这 5 美元的价值在那段时间里也是不同的。你仍然会更喜欢今天的 5 美元，因为这是你今天就可以花费并决定如何使用的 5 美元。因此，在开发这些奖励函数时，我们通常考虑的不仅仅是所有奖励的总和，我们实际上会**对未来的奖励进行折扣 (discount future rewards)**，它们距离现在越远，折扣就越重。这强制执行了某种形式的短期反馈，使模型能够优先考虑更早到来的奖励，并抑制那些稍晚到来的奖励。

最后，强化学习中有一个非常重要的函数叫做 **Q 函数 (Q-function)**。让我们看看 Q 函数是如何定义的，它是什么，特别是将其置于我们迄今为止学到的所有定义和词汇的背景下。

记住这个方程：总回报 (Return)，大写的 R(t)，就是从时间 t 开始一直到未来的所有折扣奖励的总和。从当前时间开始，直到未来的最远时间点。

我之前提到的 Q 函数，它是一个接收两个输入的函数：它接收智能体当前所处的**状态 (state, s_t)** 和它将在此状态下采取的**动作 (action, a_t)** 作为输入。Q 函数的作用就是返回通过在当前状态下采取这个动作，该智能体预期将看到的**总未来回报 (expected total future reward)**。所以，如果智能体在时间 t，给定它当前所处的状态 s(t)，执行这个动作 a(t)，那么通过采取这个行动，该智能体可能获得的总未来期望回报是多少？

现在的问题是，如果我们被赋予了这个神奇的 Q 函数——我不会告诉你我们是如何得到它的，我只是告诉你你拥有了这个 Q 函数——那么，给定一个函数，如果你给它一个状态和动作对，它就能告诉你你可以期望获得多少回报。让我问大家一个问题：如果你能访问这样的函数，并且我问你，“给定当前状态，你会选择哪个动作？”，你会如何使用这个 Q 函数来确定你的**最佳下一个动作 (best next action)**？

（等待回答）

完全正确，是的。所以，你最终要做的是，你想要确定在这个状态下你能采取的最佳行动是什么。你可以使用 Q 函数，基本上就像你建议的那样：好的，让我评估我的 Q 函数在所有这些不同动作上的值。对于每个动作，我将计算我期望获得的回报是多少，然后我将取所有这些回报中的最大值，也就是选择那个基于所有可能动作能给我带来最大期望回报的动作。

**问：**（学生提问，关于 Q 函数如何处理未来奖励，即使当前动作次优但未来更好）...这有点像...？
**答：** 是的，是的。所以问题基本上是关于，你如何不仅仅基于当前的奖励进行优化，也要基于未来的奖励。这确实是个好问题。我想强调这一点的原因是，请记住，Q 函数优化的是大写的 R(t)，也就是回报 (Return)，而不是小写的 r(t)，后者是时间点 t 的奖励 (Reward)。所以，大写的 R(t)——让我回到上一张幻灯片，因为这确实是个好点——回报与奖励的关系在于，回报是所有未来奖励的**折扣总和 (discounted sum)**。因此，这也包括了那些你现在可能为了较少奖励而采取次优行动，但这能让你在未来获得更大成功的情况。这种情况会被包含在大 R 中，但在小 r 中则会丢失。但请记住，在 Q 函数中，你实际上捕捉了通过采取这个行动所期望获得的**全部未来回报**。你假设，如果你采取了这个行动，并且之后采取所有最优的未来行动，那么你可能获得的总期望回报是多少。这就是你决定行动的方式。

**IV**. 深度强化学习的两大类方法

现在，请记住，我们真正关心的不仅仅是观察所有这些，我们真正想要的是选择能够**最大化未来回报**的动作。那么这看起来像什么呢？正如我们所说，我们可以将其表述为：对于任何给定状态的最优动作，可以通过将我们所有的动作代入这个 Q 函数，评估 Q 函数在所有这些动作上的值，然后选择那个能给我们带来最佳回报的动作来计算。

好的。

现在，在这个讲座中，我们将主要关注两种不同的方法来学习这个“动作决策器”（如果你愿意这么称呼它的话）。基本上有两种不同的方法我会告诉你们。

**第一种方法 (基于价值的学习 / Value Learning)** 是，我们首先尝试获得这个 Q 函数。我们已经看到，如果我们拥有了 Q 函数，我们就能有一个好的动作决策器，我们知道如何基于那个 Q 函数来采取行动。秘诀基本上就是你必须找到 Q 函数，这是一个神奇的函数。我们将讨论一种方法，基本上是通过尝试学习该 Q 函数的**价值 (value)** 来实现的。这被称为**价值学习 (Value Learning)**。所以你尝试找到 Q，然后就像我们刚才看到的那样使用 Q，只需对动作取 argmax 即可。

然后是**另一种学习方法 (基于策略的学习 / Policy Learning)**，我们今天也会涵盖，它的重点是，与其尝试学习 Q 然后使用 Q，不如直接尝试学习这个**策略函数 (policy function)**？因为最终你的策略函数是一个不需要评估所有动作的函数，它只需要评估你的状态，而这实际上是你真正关心的函数。你想要这个函数 pi，pi(s)，它接收你的状态作为输入，并告诉你动作。不一定需要说“我要评估我的 Q 函数在所有可能的动作上”，而是直接告诉我动作。然后我该如何使用它呢？我可以简单地从该策略中采样最优动作，以实现...我们稍后会更详细地讨论这个。

### V. 深入 Q-Learning (Deep Q-Networks - DQN)

但我想先深入探讨讲座的第一部分，也就是只关注我们看到的第一种技术：尝试找到 Q 函数，然后利用该 Q 函数来找到最优动作。让我们更深入地探讨一下，因为我认为随着我们看到一个例子，很多问题会开始得到解答。

首先，我将介绍这个 Atari Breakout 游戏（左侧）。对于那些不熟悉 Atari Breakout 的人来说，这是一款游戏，其中的行动者是底部的这个挡板。这是一个水平挡板，只能向两个方向移动：左或右，只有两种动作。它的目标是确保它朝一个方向移动，以便能击中正在接近它的球，并让球从挡板上反弹出去，以击碎顶部的彩色砖块。目标基本上是在它漏掉球并且球通过之前，尽可能多地打掉顶部的砖块。如果球通过而挡板没有击中它，游戏就输了。所以它实际上想要不断地击中球，以便打碎砖块。

现在，让我向观众提出一个有趣的场景。我想强调的是，估计甚至凭直觉理解 Q 函数有时对人类来说非常困难，而且人类通常并不擅长估计最优的 Q 函数。让我举个例子。这里有两个不同的场景，状态-动作对。有两个不同的状态-动作对。

这里是 A 类场景：球正朝着挡板飞来，挡板决定不移动，直接把球打回去，打碎背景中的一些彩色砖块。
然后我们有另一个场景 B：球实际上是朝侧面飞来，挡板必须尝试向它移动，才能追上球。

有没有人能想到，如果你估计这两个状态-动作对的 Q 值，你认为哪个会有更高的 Q 值？

**学生 A:** 我认为可能是选项 B，因为奖励会更高，尽管风险更大，因为向侧面反弹...
**主持人:** 好的。还有其他人认为 A 更好吗？
**学生 B:** （举手）是的。
**主持人:** 请讲。
**学生 B:** 是的，因为我以前玩过这个游戏（笑声）。是的，不管那是多久以前的事了。但我认为 B 的问题在于，当你移动到侧面时，这东西（挡板）出于某种原因会变得有点慢。所以我认为... 嗯。
**主持人:** 好的，好的。我们来举手表决一下吧。认为 A 会更好的人请举手。
（部分人举手）
**主持人:** 好的。认为 B 更好的呢？
（更多人举手）
**主持人:** 好的。是的，我想大概 60% 的人选了 B。好的，那我们来看看。

实际上，我们先来看 A。我们会看一个实际执行这两种策略的例子，这样我们就能进行定性比较。A 基本上是由非常**保守的 (conservative)** 动作定义的，它试图将球直接打向挡板的中心。这看起来像什么呢？这里我们可以看到这个策略在实践中执行的一个例子。它非常保守，通常试图将球正好打在挡板的中心，它被训练成这样做。但结果是，你会看到它也确实开始主要打掉游戏中心区域的很多砖块。

现在，让我们看看策略 B。你会在这里看到一些有趣的事情：挡板实际上会**故意远离 (move away from)** 球，只是为了能够回来从侧面以一个角度击打它。它为什么要这样做？它移开，你看它实际上藏在左手边，只是为了能回来从角落里击打它，试图突破顶部，并在游戏顶部获得这个“秘籍”（cheat code）。它已经学会了，如果它打入一个角落，就会像得到所有这些免费砖块一样，存在这种“秘籍”。

这只是一个例子，试图说明为什么凭直觉判断哪些 Q 函数或 Q 值是好的，以便确定我们在特定状态下可以采取的最佳行动，通常是非常困难的。这就是为什么需要如此多的与这些系统进行游戏的示例，才能遇到像那样的一个场景，并在学习过程中将其强化。

现在，让我们问自己这个问题：我们如何训练我们的模型来学习像我们在上一张幻灯片中看到的那样的 Q 函数？因为这最终是这里的目标：给定左侧我们看到的两个东西——给定一个状态（我们游戏当前帧，我们此刻所处的位置）和我们想要的一个动作——让我们评估这个动作有多好。

好的。那么现在我们可以做什么呢？我们可以创建两种不同形式的网络。我们可以有一个网络，它接收单个动作作为输入，我们对每个动作调用它多次，以评估该动作的 Q 值。或者，我们可以像右侧那样，基本上将动作移到另一侧，移到输出侧，然后说：好的，对于单个状态，我将输出**每个可能动作 (every possible action)** 的 Q 值。这两边是等价的，没有区别。唯一的区别实际上是你改变了你调用这个模型的方式的参数化。与其在左侧调用模型 N 次（N 是动作的数量），你在右侧调用模型一次，得到 N 个输出。所以你基本上是重新定义了模型，说：好的，与其调用模型 N 次，我不如直接输出 N 次。这实际上是相同的情况。

但我们将关注的是右侧，因为通常调用模型的成本非常高。所以我们想要最小化我们对模型的调用次数，我们想说：好的，我们如何能从一次模型调用中基本上一次性获得所有答案？

**问：** （学生提问关于状态信息）所以状态不仅要捕捉挡板的状态，还要捕捉球的状态、球的角度以及它在哪里，所以状态捕捉的信息远不止挡板在哪里？
**答：** 是的，是的。状态捕捉的是**环境 (environment)**。所以状态捕捉的是，比方说，屏幕的整个图像，完全正确，是的。这基本上意味着，你在这里尝试做的是，你试图训练一个网络，给定屏幕的图像——这基本上就是我们人类玩这个游戏时会看到的东西——那么哪个动作会导致最高的回报 (return)？

现在，如果我们让一个智能体总是采取所有最佳行动，会发生什么？那会是什么样子？嗯，这基本上相当于**最大化任何给定状态的目标回报 (maximizing the target return for any given state)**。所以，给定一个状态，我们总是想最大化目标回报。因此，给定所有这些可能回报的输出，即 Q 值，我们想要在每个可能的步骤中最大化那些 Q 值。这实际上也意味着，我们真正想做的只是采取所有最佳行动，以产生那个目标回报。

为了做到这一点，我们可以将这个期望回报 (expected return) 表述为，如果我们打算采取所有那些最佳行动，它将是两个东西的组合：首先是我们在当前这个瞬间获得的**初始奖励 (initial reward) r(t)**，再加上我们从此刻起在所有未来步骤中获得的**期望的最大未来回报 (expected maximum future return)**。当然，我们还必须应用**折扣因子 (discounting factor)**，以便我们对所有未来的回报进行折扣，因为它们不如我们当前的回报有价值。这就是我们的**目标 (target)**，这是我们试图优化的目标值。

我们也有**预测值 (predicted value)**，因为我们可以调用我们的网络，问网络：“你认为预测的 Q 值会是多少？” 那就是我们的预测值。

现在，你应该自动地看到这两样东西，这应该看起来与我们在课程其他部分看到的内容非常相似。我们有一个预测值，我们有一个目标值。我们现在可以将这两者相减，并计算它们之间的范数（norm），比如它们之间的绝对值或平方值。然后这就给了我们我们描述为**损失 (loss)** 的那个项。这就是我们可以用于**反向传播 (back-propagating)** 的损失，这样在下一步我们调用这个 Q 值时，希望预测值会更接近我们本应达到的目标回报。

这正是 **Q 损失 (Q loss)**。这就是所谓的 Q 损失。

让我总结一下这个，然后在我们进入下一部分之前，我暂停一下回答问题。好的，让我们总结一下这整个端到端的过程。

这是我们的深度神经网络，也就是我们的 **Q 网络 (Q network)**。它将接收我们的状态作为输入，并将为该智能体可能看到的每个可能动作输出一个 Q 值。我们假设这里有**离散动作 (discrete actions)**，比如有 K 个不同的动作智能体可以采取，它将输出 K 个不同的 Q 值。这些只是 K 个不同的数字。

这里是这个模型可以采取的动作。比方说，还是在 Breakout 游戏中，我们有三个动作：左、右或保持不变。对于这些动作中的每一个，我们可以让我们的模型预测 Q 值。比如说，在左边的这个给定状态下，向左走的 Q 值是 20，保持在原地不动的 Q 值是 3，向右走的 Q 值是 0。为什么会这样？因为你可以看到球正向左移动，而挡板已经在右边了。所以如果它向右走，它知道肯定不会再获得任何未来的回报。如果它保持在原地，也很不可能，也许它还有几步的时间来获取那些回报并向左移动，但时间不多了。

现在，有了所有这些，让我们回顾一下我们之前在课堂上讨论过的内容。给定所有这些 Q 值——我们现在有三个数字——你如何确定要采取的最佳行动？你会看这三个数字，你会说：“向左走的期望回报是 20，保持不动的期望回报是 3，向右走的期望回报是 0。我应该向左走。” 对吧？因为我想最大化我获得未来回报的机会。

然后我采取那个行动，我实际上向左移动了。我把这个（动作）发送给游戏。游戏根据我向左的一次移动，反馈给我一个新的状态，游戏的状态改变了。然后这整个过程一遍又一遍地重复。

你知道，这个确切的范式，这个确切的算法，已经在实践中被广泛使用，有很多不同的例子。一个例子是谷歌 DeepMind，他们如何使用这些称为 Q 网络的网络，来解决各种不同类型的这类 Breakout Atari 游戏。基本上在左手边，你可以看到状态总是作为输入进来，但现在输出的不是三个，而是该智能体在任何时间点可能看到或可能做出的非常广泛的动作范围。你看到中间这个非常大的网络，它试图确定对于这个动作，所有这些状态的所有 Q 值是多少。

你知道，实际上，一个非常优雅且非常简单——就其核心而言——的算法，它能工作得这么好，真是令人惊叹。因为他们在如此多的 Atari 游戏上进行了测试。在这里你可以看到这些游戏的表现。超过 50% 的游戏，用这种基本方法——没有涉及额外的技巧和窍门，这真的只是他们使用的方法——超过 50% 的游戏超越了人类水平的表现。当然，其他游戏，在最右边，你看到这个尾部，显然有很多游戏比这种简单技术能处理的要更具挑战性得多。但对我来说，这个算法如此简单和干净，却能在这样的规模上工作，处理如此多、如此复杂的行为，仍然是令人惊叹的。

**Q-Learning 的缺点:**

还有一件事，然后我会在 Q-learning 部分暂停回答问题。我想评论一下 Q-learning 的**缺点 (downsides/disadvantages)**，这将有助于引出我们过渡到下一部分的内容。

第一，我们只讨论了**离散动作空间 (discrete action spaces)**。离散动作空间意味着你的动作是 N 个不同动作中的一个，你必须选择其中一个。所以到目前为止我们所讨论的只能模拟那种类型的空间。这显然是一个非常大的缺点，我们将讨论如何将来缓解这个问题，以便我们可以更多地转向**连续动作空间 (continuous action spaces)**。这是第一点。

第二，关于这些算法的**灵活性 (flexibility)**。如果我两次给它相同的状态，它会两次给我相同的答案。无论我给它多少次这个状态，我可以给同一个状态 100 次，它总是会告诉我下一个动作是相同的。因为这是一个**确定性算法 (deterministic algorithm)**，这个算法中不涉及随机性。我们也会看到为什么这有问题，因为它不允许我们同样好地处理**随机环境 (stochastic environments)**。我们的环境并不总是确定性的，所以我们需要能够也是随机的策略和动作网络。

**问答环节:**

**主持人:** 是的，你请讲。
**问：** 首先，有一个假设是所有未来的行动也是正确的…… （后续提问关于大动作空间的可行性）
**答：** 是的，我们会看到一些例子，说明这些如何扩展到非常大的动作空间。当你拥有非常大的动作空间时，这肯定是存在问题的。我想说，在某些情况下，比如国际象棋，对吧？国际象棋的轨迹空间或解空间是巨大的，但动作空间实际上并不像人们想象的那么大。在任何时刻，可以采取的可能动作集合相对来说是可管理的。但是如果你考虑，比如说，仅仅构建未来三步的动作树，这会呈指数级增长，那就成问题了。但请记住，你的 Q 网络只需要管理，你知道的，一次性的，就像给定当前状态，在这一步采取的动作。所以你的动作空间是由每一步的动作定义的，而不是联合考虑整个未来轨迹的所有动作。

**问：** （学生提问关于游戏规则对动作空间的约束）
**答：** 完全正确。所以问题是关于你是否有游戏规则施加在你的动作空间上的限制。是的，你想要确保当你选择一个动作时，你只允许那些是**有效动作 (valid actions)**。

**主持人:** 好的，还有一个问题。嗯，是的，你已经等了很久了。
**问：** 关于奖励，是否存在奖励可能是情境化和主观的情况？在这个例子里…（后续提问关于如何定义成功和奖励）…后果…
**答：** 完全正确。所以问题基本上是关于你如何定义成功，对吧？是的，我们完全没有触及这一点，但实际上，所有这一切的基础假设是，你有一个**奖励模型 (reward model)**，它是根据你认为是真实奖励的东西进行**校准 (calibrated)** 的。所以如果某件事说，好的，你收到了一个奖励，比如 100 分，它实际上对那个个体来说校准为 100 单位的奖励。定义那个校准度量是一个非常困难的问题。我们完全忽略了这一点，对吧？但是谁来定义这个呢？是游戏制造者，是设计游戏的人。这不是一个学习得到的东西。谁设定了游戏，谁就定义了奖励是什么，因为这是游戏规则的一部分。

### VI. 深入 Policy Learning (Policy Gradients)

好的，太棒了。为了时间考虑，让我们继续推进。

我将回到我们一直在讨论的强化学习算法的这两个部分。到目前为止，我们只讨论了左边这一侧，对吧？我们专注于那些我们想要学习 Q 函数，然后使用该 Q 函数来确定最优动作的情况，通过选择那个能给我们带来最大未来 Q 值或最大未来回报的动作。

现在，让我们关注**策略学习算法 (Policy Learning algorithms)**，它们不旨在学习这个中间的 Q 函数，它们旨在**直接学习策略 (directly learn the policy)**。策略，再次强调，是这个接收状态作为输入的函数，这样如果你从这个策略函数中**采样 (sample)**，你就直接得到一个动作。它不一定需要是一个最优动作，但如果你从中采样足够多次，它应该是好的动作之一，期望上应该是最佳动作。我们将在下一部分更详细地讨论这一点。

快速回顾一下。我们刚刚在 Q 网络中看到了什么？我们将基于所有这些来进入策略网络。所以我们刚刚看到的是：深度 Q 网络接收状态作为输入，输出所有动作的所有 Q 值。然后我们对这些动作取 argmax。

对于**策略网络 (Policy Networks)**，我们现在要做的唯一区别是，我们将直接输出的**不是 Q 值**，而是对于那些动作中的每一个，我们将输出一个**概率 (probability)**，即该动作根据我们的（潜在）Q 值会是最佳动作的概率。

好的，让我们稍微分解一下。让我们回到这个 Atari 的例子。与其为每个动作输出 Q 值，我现在要输出这个**概率得分 (probability score)**。好的。所以在给定这个状态下，我向左走的概率，当我们评估我们的策略网络时会看到，是 0.9。有 90% 的概率，我们基于这个网络相信，向左的动作是最佳动作。有 10% 的概率，我们看到保持不动可能是最佳动作。有 0% 的概率，我们看到向右走是最佳动作。所以我们改变了输出内容的定义。

我们为什么要这样做？嗯，这是因为如果我们在输出空间上有一个概率函数，这意味着现在我们可以选择我们的最佳动作，**不仅仅是通过取这些数字的最大值**，而是我们可以从那个**概率分布中采样 (sample from that probability distribution)**。为了确定我们下一步要采取的行动，我们可以获取所有这些概率并从中采样。这样，你知道，90% 的时间我们会得到向左走的动作，但 10% 的时间我们仍然会得到保持不动的动作。

当然，我们必须强制执行，因为我们现在处理的是概率，模型输出有更多的约束。所以现在，第一，最大的约束是，概率**必须加起来等于 1 (probabilities have to add up to one)**。所以我们必须强制执行我们模型的输出首先要遵守这个约束。我们可以做到这一点，例如，使用 **softmax**。基本上我们几乎是在处理一个分类问题，对吧？我们试图分类，给定一个状态，我们想从这个状态采取的最优下一个动作是什么。

现在我想开放一下，征求一些想法，也许房间里有两个人可以告诉我，到目前为止，你认为这种表述的主要优势是什么？即使没有深入太多。

**问答环节:**

**学生 C:** 有了这种方法，你会采取...有时...
**主持人:** 好。
**学生 C:** ...
**主持人:** 是的。所以有时我们会得到一些**探索 (exploration)**，对吧？探索那些可能不是最优或不是最大化收益的动作。是的。
**学生 D:** 是的。因为你在采样，你也可以根据这个动作看到你要去的方向，比如下一个动作是正面的还是负面的。
**主持人:** 对，对。是的，这回到了一个相似的点，对吧？也是探索。是的。还有一个。
**学生 E:** 你降低了时间复杂度，因为现在你不用计算嗯… 每个…
**主持人:** 嗯，你仍然需要计算它们中的每一个，因为它仍然是…你正在输出所有的概率，对吧？所以这个网络的时间复杂度仍然会是相同的。让我试试叫一个新人。是的。
**学生 F:** 我们不再需要等待评估，就像我们之前必须使用的那样？
**主持人:** 对。所以评估时间某种程度上减少了…
**学生 F:** （补充）
**主持人:** 是的，是的。请讲。
**学生 G:** 这里的主要优势在于，获取 Q 函数可能在那个时间段内非常难以计算和估计…
**主持人:** 完全正确。是的，完全正确，完全正确。所以你的观点是，你正在获得一个更直接通向你期望目标的函数，对吧？这正是重点。我们实际上会看到，我们会让它更具体，你会更清楚为什么你会想这样做。

**主要优势总结:**

一个优势，实际上如果你把所有这些点结合起来，一个隐含的尚未说明的事情是，你现在可以同时处理**离散 (discrete)** 和**连续 (continuous)** 的动作了。之前我们只能处理离散动作，因为它只允许我们估计有限集合的不同动作的 Q 值。我们如何也能为连续动作做到这一点呢？

例如，这里是一个离散动作空间：左、右或保持在中间。我们可以看到，基本上，好的，我们可以有一个概率，表明做那些动作中的每一个是最佳动作。这是一个离散空间上的概率。

但是，如果我们想把它变成一个连续空间呢？比如，与其说左、右或保持不动，如果我说向左移动**多快 (how fast)** 或向右移动**多快**？现在突然之间，我们不再有三个不同的动作了。这现在是一个**无限的动作空间 (infinite action space)**，可以是 x 轴上实数线上的任何数字。我们如何能推广这种方法来处理这个问题？

Q 网络无法做到这一点。但通过策略网络，我们可以做到。因为现在我们实际上可以模拟输出上的一个**连续概率函数 (continuous probability function)**，然后说：好的，对于这个概率质量 (probability mass)，我们能用我们的模型来估计它吗？然后我们能从那个质量中采样来产生我们的最佳动作吗？

所以现在我们想说的是：好的，我在任何时间点应该采取的最佳动作，只是从我可以用我的网络预测的这个**估计的概率分布 (estimated probability distribution)** 中进行的一次采样。

让我们看看我们如何能实际地用策略梯度（一种策略网络类型的方法）来模拟这些连续动作空间。与其像我们之前看到的那样预测那三个不同类别的概率，或者让我这样说：与其预测**每一个可能存在**的动作的概率——我们当然不能对无限数量的动作这样做，即使我们只有一个数字，比如速度，那也是这个模型可以采取的无限数量的速度——所以我们已经排除了那种方法。现在我们该如何为那些基本上具有无限数量动作的输出分布做到这一点呢？

我们可以通过用一个**分布 (distribution)** 来**参数化 (parameterizing)** 我们的动作空间来做到这一点。比如一个分布，就像你在这里看到的，这是一个**高斯分布 (Gaussian distribution)**。你需要多少个数字来生成一个高斯分布，任何高斯分布？是的，只需要两个数字。所以两个数字就能描述这整个概率质量。你需要一个**均值 (mean)**，它告诉你高斯分布的中心，你还需要一个**方差 (variance)**，它告诉你高斯分布的宽度或散布程度。所以现在你只需要预测两个数字，而不是无限数量的动作。你预测一个均值和一个方差，这就告诉了你对于这个模型可能采取的每个可能速度的概率得分。

因此，如果我们想采样，你知道的，然后说：“好的，我的模型应该以什么速度行动？”嗯，我可以只看我的均值和方差，然后我可以从那个分布中采样，那个分布会精确地告诉我，我的最优均值和方差应该在这一点，接近均值，但也许带有一些散布。

当然，你知道，再次强调，就像以前一样，我们必须有这个概率总和为 1 的约束，我们可以做到，因为我们实际上明确地使用这种高斯公式来参数化我们的模型。

**策略梯度实际应用示例 (自动驾驶):**

好的。现在让我们把所有这些与一个在实践中使用策略梯度的具体例子联系起来。让我们思考一下我们如何能采用这种方法，并用它在一个简化的环境中使用强化学习和这种策略方法来训练一辆自动驾驶汽车。

所以，这里的智能体将是一辆我们想要在世界中导航的车辆。实际上，让我们进一步简化它，就说：保持在你的车道内，不要撞车。这是标准。环境将是，你知道的，它在车道上的位置，距离中心线有多远等等，以及它在那个世界中的移动。它可以采取的动作将只是**方向盘角度 (steering wheel angle)**。在任何时刻，它都可以执行一个方向盘角度。方向盘角度是连续的，对吧？你可以在很宽的角度范围内取任何角度。最后是奖励，让我们保持简单。你不会在每个时间步都得到奖励，你只会在**撞车时得到惩罚 (penalty if you crash)**，并且你想要**最大化撞车前的时间 (maximize the time before you crash)**。这就是奖励。

好的。让我们把所有这些整合起来，看看我们如何在这个背景下训练这个网络。

我们将首先**初始化我们的智能体 (initializing our agent)**，我们在这个环境中初始化汽车。然后我们将**运行我们的策略 (run our policy)**。在每个时刻，每一步，我们是如何运行这个策略的？在每一步，我们计算我们的策略概率，我们从那些概率中采样来计算汽车认为它此时应该执行的动作，然后它执行那个动作，并在一个循环中重复这个过程。当然，在开始时，它从未被训练过，所以策略非常糟糕，概率与什么是安全的非常不相关。所以它行驶得非常……你知道的，行驶不了多远，最终会撞车。

但我们要做的是，对于那些步骤中的每一步，我们将**记录它执行和看到的每一个状态和动作对 (record every state and every action pair)**。并且，我们还将记录它在那些阶段中看到的**奖励 (rewards)**。所以在那些步骤中的每一步，我们将有一个包含三个数字的三元组：一个状态、一个动作和一个奖励对。

最后，我们将回到我们的策略网络，我们将只做两件事：我们将**不鼓励 (discourage)**，我们将降低在这个序列中**低回报部分 (low-reward parts)** 发生的每个动作的概率——也就是接近撞车的部分。大致上，我们将有两个部分：接近撞车的部分和远离撞车的部分。对于接近撞车的部分，我们将降低发生在那部分附近的所有事情的概率。我们将**增加 (increase)** 重复发生在远离那次撞车的所有动作的概率。

然后我们将从头开始**重复这整个循环 (repeat this entire loop)**。我们将重新初始化智能体，我们将启动它，重新运行策略，我们将重新记录新的状态-动作-奖励三元组。然后再次，我们将做同样的事情：我们将降低后来发生的事情的概率，并增加更早发生的事情的概率。（澄清：不要纠结于后来与更早，重点实际上是我们正在降低受惩罚动作的概率，并增加好动作的概率。好动作实际上更早而坏动作更晚只是这个特定例子的结果，不一定总是如此。）

现在我们一遍又一遍地重复这个过程，最终你会学到，智能体，你知道的，可以不断地从它所有的错误中学习，不断增加做得好的事情（的概率），减少做得不好的事情（的概率），最终它开始有点沿着这些车道行驶而不会撞车了。

这真的令人惊叹，因为：
第一，我们从未向这个模型展示任何关于环境的信息。
第二，我们从未教过它驾驶规则，或者它必须保持在车道内。我们只给了它一个极其**稀疏的奖励信号 (extremely sparse reward signal)**，即当它撞车、当它离开车道时我们惩罚它。
第三，我们从未明确告诉它任何事情，我们从未给它任何人类驾驶的例子来让它学习这个任务。
但它能够仅仅通过与环境互动并迭代地接收那些奖励信号来学习这个任务。

**策略梯度训练细节 (损失函数与梯度):**

现在，我认为这里的细微差别当然在于算法的这一部分。尤其是最后这两个步骤。这就是细微差别真正开始体现的地方。我们是如何增加好事情发生的概率，以及我们是如何减少坏事情发生的概率的？因为这就是我们训练模型的方式。步骤四和五是我们没有讨论过的训练和反向传播。其他一切实际上只是通过我们网络的前向传播和收集信息。

那么我们是如何做到这一点的呢？让我们实际地逐步了解这部分。任何训练都有两个部分：**损失函数 (loss function)**，然后是之后你如何反向传播。所以我们必须理解，对于我们在这里看到的步骤四和五，我们的损失是什么。

损失包含两个项。
第一项是**对数似然 (log likelihood)**，即基于当前选择选择一个动作的对数似然。
然后第二项，我们将这个乘以我们的**折扣回报 R (discounted return R)**。

好的。让我们稍微停顿一下，逐步分析。首先，这是第一部分：看到那个动作，选择被选中的那个动作的对数似然。然后第二部分是奖励，回报。

让我们假设我们为一个具有**高似然度 (high likelihood)** 的动作获得了**大量回报 (lot of reward)**。这个损失会非常大（绝对值），因为你将两个大数相乘。然后你会对它取负（注意开头的负号）。这意味着你最终会得到一个非常负的损失。这是一个非常理想的状态，对吧？你为一个回报丰厚的动作执行了一个高似然度的动作。所以你想继续这样做，那非常好。

让我们来看相反的情况。假设你为一个**概率非常高 (very high probability)** 的动作得到了一个**非常低的回报 (very low reward)**。所以你采取了一个看起来对你来说非常有信心的动作，但它导致了一个非常糟糕的回报。你把这两个数字相乘，你会得到一个更小的数字（绝对值可能仍较大但正负重要）。你对它取负，它会变成一个（代数上）更大的数字（即惩罚更大）。

当你把所有这些结合在一起时，你就得到了这个损失函数，现在你可以通过它进行反向传播了。你代入这两个数字，计算那个损失，然后你可以反向传播来更新你的网络，以便找到…并更新似然度。因为最终，比如说，在那种你有一个高似然度动作却导致了坏回报的情况下，下一次你运行那个网络时，你不希望似然度再高了。下一次你运行它时，你希望反向传播信号说：“好的，我想降低这个，因为现在损失告诉我我应该受到惩罚。”所以当我反向传播时，我将更新我所有的权重，这样下一次的似然度就会小得多。

这正是我们所做的。如果你实际代入，你会得到这个方程，看起来像这样。这是一个梯度下降方程，希望大家都很熟悉。但关键点是这个**策略梯度 (policy gradient)**。所以这个算法之所以被称为策略梯度，是因为你正在计算你的策略的梯度，也就是这个似然函数。这只是名字的来源。

**真实世界应用挑战与仿真:**

现在我想回到这个算法，这个算法的伪代码，并稍微讨论一下。不是在我们看到的那个二维驾驶例子的玩具世界、合成世界里，而是如果我们想在**现实世界 (real world)** 中进行强化学习呢？这个故事会如何改变？我们需要考虑什么？我们必须如何处理这里的一些缺点？所以我想把这个作为一个问题提出来。观众中的每个人都认为什么是局限性？哪一步？如果我们想做与我们在二维驾驶世界中看到的完全相同的过程，如果我们想在真实驾驶中这样做，这里的五个步骤中，哪一步会是真正不太好的那一步？

**学生 H:** 会有比你能看到的更多的状态，你不会看到所有的状态。
**主持人:** 是的，没错。但取决于你的模拟器，这也可能是真的，对吧？你可能有一个非常大的模拟器，里面也有一堆状态。它们也可能有一个宽泛的动作空间。还有什么？是的。
**学生 I:** （指出第二步的问题）
**主持人:** 完全正确，完全正确。是的，第二步。是的，这就是问题所在。因为**终止 (termination)**，顾名思义，它不是一个好兆头。你不想运行一个策略直到终止，因为在现实世界中，这……你知道的，基本上你不想在现实世界中达到终止状态。你可以在模拟中这样做，这不是问题。但在现实世界中，基本上，为了做我们到目前为止讨论的所有事情，我们需要运行直到终止。所以这不会那么顺利。

当然，人们已经尝试通过将**照片级真实感 (photorealism)** 和**仿真 (simulation)** 推进到一定程度来解决这个问题，使得终止是安全的，你知道的，你不需要运行到终止并在现实世界中承担后果。

所以实际上，我们在麻省理工学院这里创造了一个非常酷的成果。你知道的，这个实验室，实际上我所在的实验室就在楼上，我们做了很多关于自主性和机器人技术的工作。我们创造了这个极其照片级真实的模拟器。这是几年前的事了，但你实际上可以看到来自这个仿真引擎的图像。这些是…呃…一个自动驾驶汽车运行着你刚刚在上一张幻灯片中看到的相同强化学习策略的视频，它通过具有极其照片级真实场景的模拟器运行，并且完全在仿真中进行端到端训练，这样我们就可以之后将这个模型直接部署到现实中。

我们实际上也这样做了。这就是那个模型，它就在你今天坐的位置下面一层，在这栋楼的车库里。我们有一辆车，我们把它带到我们的测试跑道上，用你今天课堂上学到的算法以完全相同的方式训练它，然后在真实的道路上部署它，直接从仿真中来，使用了相同的算法，但是以一种非常安全的方式完成的，因为它都是通过照片级真实感实现的，并且能够直接转换到现实中。

### VII. 高级应用案例: AlphaGo

现在，我认为我们已经为 Q-learning 和 Policy learning 都打下了非常坚实的基础，这样你就能理解，给定，你知道的，动作和环境，或者智能体和环境，我们如何能优化我们的智能体在那些环境中执行动作。

但我认为，你知道的，一件非常令人兴奋的事情，当我在学习所有这些算法时，对我来说非常有趣的是，尝试理解我们如何能转换，类似于自主性和机器人技术的情况，我们如何能将很多这些从…不仅仅是强化学习文献几十年来涵盖的玩具案例，转换到一些现实世界的用例中。

所以强化学习的一大进步和突破发生在几年前，是关于**围棋 (Go)** 游戏。强化学习智能体在这款游戏上受到了考验，因为围棋是一款具有极其巨大的动作空间和非常复杂行为的游戏。当时，你知道的，这些强化学习智能体被用来对抗人类顶尖棋手，你知道的，这带来了很多非常令人兴奋的结果。

围棋游戏非常有趣，也许我只给出一个非常快速的游戏背景介绍。这是一个 19x19 的棋盘，一个二维游戏，你可以执白子或执黑子。围棋的特别之处在于…好吧，你想做什么？你只想比你的对手占据更多的棋盘。所以你想比你的对手占据更多的棋盘位置。虽然棋盘和游戏规则实际上相对简单——这是一个二维世界，两个玩家，黑或白——听起来其实很简单。但问题在于，因为是 19x19，你可以采取的可能动作有一个**爆炸性的增长 (explosion of possible actions)**。甚至不止于此，如果你向未来推演几步，可能的动作步骤更是爆炸性增长。事实上，在标准尺寸的围棋游戏中，合法的棋盘位置数量比整个宇宙中的原子数量还要多。

所以这里的目标是训练一个人工智能，它不仅能解释那些棋盘位置，而且给定一个棋盘位置，能找到一个**最优策略 (optimal policy)**——即每个动作是它能采取的最优动作的概率是多少——以便击败那些当前或过去的世界围棋冠军。

那么，谷歌 DeepMind 是如何解决这个问题的呢？他们实际上开发了一个与你今天学到的非常相似的流程。它仍然是…你知道的，并没有那么不同，实际上是…基本上思想是很难击败这些算法的。

从左手边开始，首先是，训练这个神经网络，它**观察人类围棋选手 (watches human go players)** 并学习，给定一个棋盘状态，预测该棋盘状态的**最优动作 (optimal action)**，仅仅是预测下一步。这不是强化学习，这是**监督学习 (supervised learning)**。所以给定状态一和人类会采取的下一个状态的例子，预测下一个状态。仅仅是监督学习。

但是，使用那个网络作为**初始网络 (beginning network)** 来训练你的强化学习网络。所以现在你应该有了一个，你知道的，某种早期版本的初级围棋选手，仅仅通过观看大量已经发生的围棋比赛，你可以学习到一些动态，一些正在发生的行为，并且你可以以某种方式预测下一个状态。

然后使用强化学习来**与自己对弈 (play against itself)**，并根据它何时获胜、何时未获胜来获得奖励。这样它实际上可以持续改进，不仅仅局限于被展示的人类对局，而是实际上通过其自身的强化和自身的奖励信号来提取更有意义的反馈和更有意义的信号。

现在，真正将这个想法提升到新水平的技巧之一是这第三层：即能够在每个阶段评估棋盘上那个点的**价值 (value)** 或得分是多少。因为你可以想象围棋，就像国际象棋一样，是一种信号直到最后才出现的博弈。如果你在游戏的最末端只有一个信号，那么就很难说：“好的，我很久以前走的这一步，是导致我输掉这盘棋还是赢得这盘棋的那一步吗？”所以信号…这是另一种说法，即信号非常稀疏。你只有一个信号，它必须稀疏地一直回溯时间。这里的挑战在于，你不仅仅想要这个稀疏信号。这里采用的技巧是说：“好的，我能否为可能存在的每个棋盘状态创建一个价值？”这样当我看到棋盘状态时，我能否评估在此时这个状态有多好？就在此时。并且有一个好的衡量标准——实际上人类棋手在国际象棋中就是这样做的，他们可以看着一个棋盘，不看任何过去或未来的走法，然后他们可以说：“这是一个好状态”，“这是一个坏状态”。就像，如果我只看这个棋盘，我可以拿起这个棋盘，我更愿意执黑还是执白？这基本上就是这里的问题。

你可以将所有这些部分组合在一起，基本上说：“好的，我将端到端地训练这个，并自我优化这个奖励模型。”

现在，一个局限性，实际上人们真正不喜欢的一点是，第一阶段你必须先给它看人类的对局。因为这实际上看起来非常没有必要，你不应该需要这样做。事实上，你也不必这样做。所以你可以从头开始，这就是后来被证明的：你可以从零开始进行自我对弈，同时使用强化学习（通过 AI 与 AI 对弈）和这种对游戏中每个棋盘状态的价值理解，甚至不需要基于人类反馈来预训练这些系统。

### VIII. 总结与展望

好的，我将非常简要地总结所有这些。我们在今天的讲座中涵盖了很多内容。我们涵盖了强化学习的许多基础知识、许多术语和背景。我们在此基础上，基本上涵盖了强化学习的两个不同方面：**Q-learning** 方面和 **Policy learning** 方面。然后我们过渡到涵盖了一些这些系统如何被使用的应用，你知道的，在各种不同的自主性应用和游戏应用中。人们在这里涵盖了大量不同的领域。

这就是强化学习部分的内容。我们将过渡到 Ava今天的讲座。也许就在我们过渡的时候，我强调一下，明天我们会有 T 恤衫和嘉宾讲座。在我们过渡的时候，我会把这个留在屏幕上，提供一些信息。但是，是的，请明天务必回来，因为我认为明天的讲座将是我们今天展示的一些最新内容，并且将由我们一些领先的行业演讲者带来。所以，非常期待明天的讲座，并且也会给你们发一些课程的周边产品。

好的，谢谢大家。