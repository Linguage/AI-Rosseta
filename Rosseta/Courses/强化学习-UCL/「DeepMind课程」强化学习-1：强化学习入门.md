

#  「DeepMind课程」强化学习-1：强化学习入门

- 视频链接：[Reinforcement Learning 1: Introduction to Reinforcement Learning](https://www.youtube.com/watch?v=ISk80iLhdfU&list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)
- 官方频道：[Google DeepMind](https://www.youtube.com/@googledeepmind)

---

### 内容介绍

本篇内容整理自 DeepMind 研究科学家 Hado Van Hasselt 在 DeepMind x UCL 高级深度学习与强化学习系列讲座中关于强化学习入门的第一讲。这节课旨在为学习者构建对强化学习（Reinforcement Learning, RL）领域的基本认识和整体框架。

讲座从引入强化学习的动机出发，将其置于自动化和人工智能发展的历史背景中，阐述了让机器通过与环境交互来自主学习解决方案的必要性。随后，清晰地定义了强化学习的核心概念，包括智能体（Agent）、环境（Environment）之间的交互循环，以及动作、观测和至关重要的奖励信号。讲座详细阐述了回报（Return）、价值函数（Value Function）和策略（Policy）等基本要素，并解释了它们之间的关系，特别是引入了折扣因子和贝尔曼方程的概念。

接着，内容深入到构成一个强化学习智能体的内部组件，探讨了状态表示的重要性，区分了完全可观测（MDP）与部分可观测（POMDP）环境，并介绍了策略、价值函数和可选的环境模型在智能体内部的作用。基于这些组件，讲座对不同类型的强化学习智能体进行了分类。最后，讲座还概述了强化学习领域面临的关键挑战，如学习与规划、预测与控制、近似方法（特别是与深度学习的结合）以及核心的探索与利用困境，并通过具体示例加深理解。

这份讲稿为后续深入学习强化学习算法和应用奠定了坚实的基础，适合对该领域感兴趣的初学者或希望系统梳理基础知识的学习者阅读。

### 内容纲要

```
Reinforcement Learning 1: Introduction to Reinforcement Learning
├── 1. 课程管理与资源
│   ├── 讲座安排与Moodle使用
│   ├── 评分方式（作业）
│   └── 背景材料（Sutton & Barto 书籍，章节对应）
├── 2. 强化学习简介
│   ├── 动机（自动化浪潮：物理 -> 脑力 -> 学习解决方案）
│   ├── 什么是强化学习？（交互学习、主动性、序贯性、目标导向、试错）
│   ├── 基本交互框架（智能体、环境、动作、观测、奖励循环）
│   ├── 学习目标（发现未知解、快速适应新环境）
│   └── 关键挑战概念（时间与长期后果、主动收集经验、预测未来、处理不确定性）
├── 3. 与其他学科的关系
│   ├── 相关领域（计算机科学/机器学习、神经科学、心理学、工程/最优控制、数学/运筹学、经济学）
│   └── 与机器学习其他分支对比（vs 监督学习、vs 无监督学习、RL的独特性：奖励信号、延迟反馈、时间/序列性）
├── 4. 强化学习系统的核心要素
│   ├── 奖励 (Reward $R_t$)（标量反馈、定义目标、奖励假设、稠密/稀疏、正/负）
│   ├── 回报 (Return $G_t$)（累积奖励、折扣因子 $\gamma$、递归形式 $G_t = R_{t+1} + \gamma G_{t+1}$）
│   ├── 价值函数 (Value Function)（预期回报、状态价值 $V(s)$、动作价值 $Q(s, a)$）
│   └── 策略 (Policy $\pi$)（行为方式、状态到动作映射、确定性/随机性）
├── 5. 智能体 (Agent) 的内部组件
│   ├── 状态 (State $S_t$)
│   │   ├── 环境状态 vs. 智能体状态
│   │   ├── 历史 (History $H_t$)
│   │   ├── 马尔可夫属性与马尔可夫决策过程 (MDP) ($P(S_{t+1}, R_{t+1} | S_t, A_t)$)
│   │   ├── 部分可观测性与POMDP
│   │   └── 状态构建（作为历史的函数 $S_t = f(H_t)$、状态更新函数 $S_{t+1} = f(S_t, A_t, R_{t+1}, O_{t+1})$、与RNN的联系）
│   ├── 策略 (Policy $\pi$)（(重申) 智能体行为定义）
│   ├── 价值函数 (Value Function)
│   │   ├── 定义回顾 ($V_{\pi}(s)$, $Q_{\pi}(s, a)$)
│   │   ├── 贝尔曼方程 (Bellman Equation)（期望方程 $V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t=s]$）
│   │   ├── 贝尔曼最优方程 (Bellman Optimality Equation) ($V^*(s) = \max_a E[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$)
│   │   └── 近似的需求 (Approximation)
│   └── (可选) 模型 (Model)
│       ├── 定义（环境动态的内部表示）
│       ├── 组成（状态转移预测 $P(s'|s,a)$、奖励预测 $R(s,a)$ 或 $R(s,a,s')$）
│       └── 作用（用于规划 Planning）
├── 6. 强化学习智能体的分类
│   ├── 基于价值 (Value-Based)
│   ├── 基于策略 (Policy-Based)
│   ├── Actor-Critic
│   └── 基于模型 (Model-Based) vs. 无模型 (Model-Free)
├── 7. 强化学习的主要挑战
│   ├── 学习 vs. 规划 (Learning vs. Planning)
│   ├── 预测 vs. 控制 (Prediction vs. Control)
│   ├── 近似方法与深度强化学习 (Approximation & Deep RL)（函数逼近器、神经网络、数据非iid、非平稳性）
│   └── 探索 vs. 利用 (Exploration vs. Exploitation)（基本困境、定义、重要性）
├── 8. 示例
│   ├── 迷宫示例（策略、价值、模型概念）
│   ├── 网格世界示例（对比随机策略价值与最优策略价值）
│   ├── Atari游戏示例（模型自由学习、像素输入、处理部分可观测性）
│   └── 仿真行走示例（奖励驱动的复杂行为涌现）
└── 9. 课程后续内容概要
    ├── Bandit 问题与探索
    ├── MDP 与规划 (动态规划)
    ├── 无模型预测与控制
    ├── 价值函数近似 (Value Function Approximation) - (隐含提及)
    ├── 策略梯度方法 (Policy Gradient Methods)
    ├── 深度强化学习挑战与整合
```


---

# 强化学习讲座 1：强化学习入门

## 1. 课程管理与资源

大家好，欢迎来到本课程强化学习系列的第一讲。

正如之前介绍过的，本课程大致分为两个独立的轨道，深度学习方向和强化学习方向，两者之间会有一些重叠，特别是在强化学习中，我们有时会用到深度学习的方法和技术。但很大程度上，这两个方向可以分开来看，强化学习本身作为一个领域已经被研究了很多很多年了。

具体到本讲座，我会从一个较高的层面概述强化学习的许多概念。在后续的讲座中，我们会深入探讨其中的一些主题。所以，如果你觉得有些信息缺失，是的，这是必然的。但是，如果你感到非常困惑，请随时打断我提问。没有愚蠢的问题。如果你没理解，很可能是因为我没解释清楚，而且教室里很可能还有很多其他人也没理解。所以，请随时提问。中间我也会安排一次短暂的休息，让大家放松一下。

好的，我们开始吧。先说一些常规的管理事宜，作为热身。

大部分强化学习的讲座都安排在现在这个时间。但也有一些例外，具体请查看 Moodle 上的课程表。当然，目前的安排是我们认为会保持不变的，但还是请随时查看更新，以防有变动。或者，干脆来参加所有讲座，这样就不会错过任何内容了。

请在 Moodle 上查看更新，也请使用 Moodle 提问，我们会尽量及时回复。

大家都知道，评分是通过作业完成的。

关于背景材料，本课程的强化学习部分将主要依据 Sutton 和 Barto 合著的书籍的新版（第二版）。完整的草稿可以在网上找到，我相信它目前正在印刷或者很快就会付印，如果你更喜欢纸质版的话。不过对于这门课来说，可能时间上来不及，但你可以直接获取完整的 PDF 文件。

具体到本讲座，背景材料主要是书中的第一章和第三章。下一讲的内容实际上会出自第二章。我特别鼓励大家阅读第一章，它提供了一个高层次的概览，介绍了 Rich Sutton 对这些问题的思考方式，讨论了许多核心概念，并且还提供了一个广阔的历史视角，说明这些想法从何而来，以及这些想法随着时间的推移是如何演变的。因为如果你只从这门课获取信息，你会形成某种特定的观点，但可能意识不到过去人们的看法可能截然不同，甚至现在有些人可能仍然持有不同的看法。所以，（阅读第一章）很有帮助。当然，我会尽量给出我的观点，但我会努力使其尽可能贴近书本内容，而且我认为我们的观点大体上是一致的，所以这应该没问题。

## 2. 强化学习简介

这是今天的提纲。我会先谈谈什么是强化学习。你们中许多人可能已经对此有了粗略或详细的了解，但统一认识总是好的。然后我会讨论一个典型的强化学习系统的核心概念，其中一个概念是“智能体”(Agent)。接着我会讲解构成这样一个智能体的组件有哪些。最后，我会谈谈强化学习面临的一些挑战，也就是强化学习研究领域内的一些研究课题或需要思考的问题。

当然，最好是从定义它是什么开始。但在那之前，我先讲一点动机。

### 动机

这可能是一个非常高层次、抽象的看法。但我们可以这样想：很久很久以前，我们开始用机器自动化物理解决方案，这就是工业革命。想想用火车取代马匹。我们大致知道如何沿着轨道把东西拉向前，然后我们把这种能力固化到机器中，用机器取代人力，或者在马的例子中是动物劳力。这自然极大地促进了生产力。

那之后，第二波自动化浪潮，基本上仍在进行中，但已经持续了很长时间，可以称为数字革命，或者有时被称为数字革命。在这次浪潮中，我们做了类似的事情，但这次我们自动化的是脑力解决方案，而非物理解决方案。一个典型的例子可能是计算器。我们知道如何做除法，所以我们可以把这个过程编程进计算器，让它在机器上完成过去纯粹是脑力劳动的任务。所以，我们自动化了脑力解决方案。

但在上述两种情况中，解决方案都是我们自己想出来的，对吧？我们想出了要做什么以及如何做，然后将其在机器上实现。

那么，下一步就是：定义一个问题，然后让机器自己解决它。这就需要学习，需要一些额外的东西。因为如果你不给系统输入任何东西，它怎么能知道该怎么做呢？一种可以输入到系统中的东西是你自己的知识，就像我们在那些用于脑力或体力解决方案的机器中所做的那样。但另一种可以输入的东西是关于如何学习的知识，然后让机器自己从数据中学习。

### 什么是强化学习？

那么，什么是强化学习呢？（顺便说一句，教室里还有一些零散的座位，想找座位的可以试试看，因为人越来越多了。）

好，强化学习的特殊之处在哪里？我认为，我们以及许多其他我们称之为智能的生物，是通过与环境的*交互*来学习的。这与其他类型的学习有所不同。例如，它是*主动的*(active)，而非*被动的*(passive)。你进行交互，环境会对你的交互做出响应。

这也意味着你的行动通常是*序贯的*(sequential)，对吧？环境可能因为你做了某件事而改变，或者你可能处于环境中的不同情境，这意味着未来的交互可能依赖于之前的交互。这些特点与比如说监督学习有点不同。在监督学习中，你通常得到一个数据集，它就摆在那里，然后你基本上就是处理这些数字来得到解决方案。这仍然是学习，对吧？这仍然是从数据中获得新的解决方案，但这是一种不同类型的学习。

此外，许多人认为我们是*目标导向的*(goal-directed)。我们似乎在朝着某些目标前进，也许事先并不确切知道如何达成目标。而且我们可以*在没有最优行为示例的情况下学习*。显然，我们也可以从示例中学习，比如在教育中，但我们也必须能够通过*试错*(trial and error)来学习，这一点非常重要。

### 基本交互框架

这是一个经典的强化学习图示，有很多版本。有一个*智能体*(Agent)，也就是我们的学习系统。它发出特定的*动作*(Actions)或决策。这些决策被*环境*(Environment)接收，环境基本上是智能体周围的一切。（尽管在这些图中我把它们分开了画，但你可以认为环境就是智能体之外的所有东西）。环境以某种方式响应，发回一个*观测*(Observation)。如果你愿意，也可以将其更多地视为智能体的一种“拉取”动作，即智能体观察环境。无论如何，这个循环持续进行。智能体可以采取更多行动，环境可能会也可能不会因为这些行动而改变，观测结果也可能会或可能不会改变。而你就在这个交互循环中进行学习。

### 学习目标

为了理解我们为什么需要学习，认识到学习存在不同类型的目标是很有帮助的。我已经区分了主动学习和被动学习，但学习的目标也可能不同。

你可以区分的两个目标是：一是找到先前未知的解决方案。也许你并不确切关心解决方案是如何得到的，但你可能发现很难手动编写或自己发明它们，所以你想从数据中获得。但要认识到，这与“能够在一个新环境中快速学习”是不同的目标。这两个都是学习的有效目标。

对于第一种学习类型，一个例子可能是，你想要找到一个能比任何人类都下得更好的围棋程序。这是一个找到特定解决方案的目标。

对于第二种学习类型，你可以想象这样一个例子：一个机器人在地形中导航，但突然发现自己处于一个它从未见过、并且在构建或训练机器人时也不存在的地形中。这时，你希望机器人能够在线学习，并且可能需要快速适应。

强化学习作为一个领域，旨在提供能够处理这两种情况的算法。有时人们并不明确区分这两种情况，有时人们也不明确说明他们追求的是哪个目标，但记住这一点是有好处的。

还要注意，第二点不仅仅是关于泛化(generalization)。它不仅仅是关于你学习了很多地形，然后遇到一个新的地形也能很好地处理。它也包含这一点，但更重要的是能够*在线学习*(online learning)，即使在执行任务的过程中也能适应。这是很自然的，我们也会这样做。当我们进入一个新情境时，我们确实会适应，我们不必仅仅依赖于过去的经验。

### 关键挑战概念

另一种表述强化学习的方式是：它是研究如何通过交互来学习制定决策的科学。

这要求我们思考许多概念，例如*时间*(time)，以及与之相关的*动作的长期后果*(long-term consequences)。它要求我们思考*主动收集经验*(actively gathering experience)，因为存在交互，你不能假设所有相关的经验都会被直接提供给你，有时你必须主动去寻找。它可能要求我们思考*预测未来*(predicting the future)，以便处理这些长期后果。通常，它还要求我们处理*不确定性*(uncertainty)。这种不确定性可能是问题固有的，比如你可能正在处理一个本身就充满噪音的情境；或者可能是问题的某些部分对你来说是隐藏的，例如，你正在与一个对手博弈，你不知道他们脑子里在想什么；或者也可能是你自己造成的不确定性，因为你不知道，也许你遵循的行为有时有点随机，所以你无法仅凭自己的交互就完全确定地预测未来。

（我再说一遍，还有几个座位，如果有人想坐的话。后面有一个，上面这儿也有几个。）

这个领域有巨大的潜在应用范围，因为决策在很多很多地方都会出现。所以，我只想让你们思考一下：这是否足以谈论什么是人工智能？当然，我可以在这里表明立场，但这只是为了激发你们思考。你能想到我们没有涵盖的东西吗？那些你认为人工智能可能需要，但这个框架没有包含的东西？这基本上就是我想让你们思考的问题。如果确实有，那我们或许应该把它们加进来。

## 3. 与其他学科的关系

### 相关领域

强化学习与许多相关学科有关，并以一种或另一种形式被研究了很多次，形式也多种多样。这是我从 Dave Silver 那里借用的一张幻灯片，他指出了其中一些学科。可能还有其他的，而且这些甚至可能不是唯一的例子，尽管其中一些相当有说服力。

他指出的学科，在顶部是计算机科学，你们中很多人可能正在学习它的某个分支。在计算机科学中，我们可能会做一种叫做机器学习的事情，你可以认为强化学习是该学科的一部分，我稍后会回到这一点。还有神经科学，人们对大脑进行了广泛研究，发现大脑中的某些机制看起来非常像我们将在本课程后面学习的强化学习算法。所以，那里可能也存在某种联系，或者你可以用我们将要讨论的这些概念来理解我们是如何学习的。同样，在心理学中，这可能更像是神经科学论证的一个更高层次的版本，那里有行为，显然有决策，也许你可以用非常相似的方式，甚至相同的方式来建模，就像你建模强化学习问题一样，然后思考学习意味着什么，学习过程如何使用这个框架进展。

另一方面，是工程学。有时你只是想解决一个问题，有很多实际问题人们出于各种原因想要解决，但通常是为了优化某些东西。在这个领域内，我们有一个叫做*最优控制*(optimal control)的领域，它与强化学习密切相关，许多方法是重叠的，尽管有时侧重点略有不同，符号表示也可能有点不同。

非常类似地，在数学中，有一个子类别，或者说也许称之为数学的一部分不太完全公平，可能更像一个维恩图本身，叫做*运筹学*(operations research)。运筹学这个领域，你基本上是使用数学工具为许多问题寻找解决方案，包括我们将在本课程后面接触到的*马尔可夫决策过程*(Markov decision processes)，以及动态规划(dynamic programming)等，这些也用于强化学习。

最后，在底部写着经济学，但你也可以考虑其他相关领域。关于经济学，非常有趣的一点是，它非常明显是一个*多智能体*(multi-agent)的设定。现在情境中有多个行动者，他们共同做决策，但也分开做决策，这些智能体之间存在各种有趣的交互。而且在经济学中，思考优化某些东西也是很自然的，很多人谈论优化比如回报或价值，这与我们也将要讨论的内容非常相似。

### 与机器学习其他分支对比

稍微聚焦于机器学习部分。有时人们会做这样的区分：机器学习基本上有几个子领域。也许其中最大的是*监督学习*(supervised learning)子领域，我们现在可以说已经相当擅长这个领域了，而且很多深度学习的工作，例如，都是在监督设定下完成的。那里的目标是找到一个映射关系。你有输入和输出的例子，你想学习这个映射，并且理想情况下，你想学习一个也能泛化到你从未见过的新输入的映射。简单来说就是这样。

*无监督学习*(unsupervised learning)则不同，它是当你没有带标签的例子时所做的事情。你可能有很多数据，但也许没有明确的关于映射应该是什么的例子，你所能做的或者想要做的，就是以某种方式结构化数据，以便你可以对其进行推理，或者能更好地理解数据本身。

现在是*强化学习*(reinforcement learning)。有些人有时认为它是上述两者之一的一部分，或者可能是两者的某种混合。但我认为它是不同的、独立的。在强化学习中，主要的区别之一是你得到一个我们称之为*奖励*(reward)的强化学习信号，而不是一个监督信号。这个信号，我稍后会更详细地讨论，它给你提供了关于某件事相对于其他事有多好的某种概念，但它并不确切地告诉你该做什么。它不给你一个标签或者你应该采取的行动。它只是告诉你，“我喜欢这个这么多”。我后面会更详细地说明。

那么，强化学习的特点，特别是它与其他机器学习范式有何不同，包括：
- 你的奖励信号没有严格的监督。
- 反馈可能是延迟的(delayed)。有时你采取一个行动，这个行动在很久以后才导致奖励。这也是你在监督学习设置中通常不会遇到的情况（当然，也有例外）。
- 时间和序列性(sequentiality)很重要。如果你现在做一个决定，以后可能无法撤销。而如果你只是做一个预测并在监督设置中更新你的损失函数，通常你以后还可以重做。这意味着早期的决策会影响后期的交互。记住这一点很重要。基本上，下一讲也会大量讨论这个问题。

决策问题的例子有很多，正如我所说的。一些具体的例子，也许能帮助你思考这些事情，包括：
- 驾驶直升机。
- 管理投资组合。
- 控制发电站。
- 让机器人行走。
- 玩视频游戏或棋盘游戏。

这些都是强化学习或其变种已被应用的实际例子。也许值得注意的是，这些之所以是强化学习问题，是因为它们是序贯决策问题，即使你未必使用人们可能称为强化学习方法来解决它们。做这个区分很重要，因为有些人想到的是当前的强化学习算法，他们基本上把这个领域等同于那些特定的算法。但强化学习既是一个关于如何思考这些问题的框架，也是一套人们称之为强化学习算法的算法集合。但你可能正在处理一个强化学习问题，而没有使用任何那些特定的算法。

## 4. 强化学习系统的核心要素

我已经提到了其中一些，但一个强化学习系统的核心概念包括：智能体所处的*环境*(environment)，指定智能体目标的*奖励信号*(reward signal)，当然还有*智能体*(agent)本身。智能体本身可能包含某些组件，在本讲座的剩余部分，我会逐一介绍所有这些。

但请注意，在我之前展示的交互图中（就是这张），我实际上没有把奖励放进去。我这样做是有原因的，因为你在文献中看到的大多数这类图实际上都把奖励画成从环境进入智能体。这没问题。在这种情况下，智能体本身基本上只是学习算法。这意味着，如果你有一个机器人，学习算法位于机器人内部的某个地方，但这个图中的“智能体”并不等同于整个机器人。学习算法可以将机器人的一部分视为其环境，某种意义上是这样。因为通常环境本身并不关心奖励，它没有奖励这个概念。通常是我们指定奖励，它存在于你的强化学习系统内部的某个地方。这就是为什么我没有把它放在图里，因为你可以认为它来自环境进入智能体，或者你可以认为它是智能体的一部分，但不是学习算法的一部分。因为如果学习算法可以修改自己的奖励，那么可能会发生奇怪的事情，它可能会找到优化奖励的方法，但这只是因为它在设置奖励，而不是因为它学到了任何有趣的东西。所以，将奖励视为学习算法外部的东西是有用的，即使它在整个系统内部。

这里发生了什么？这就是我一直在谈论的交互循环。如果我们引入一点符号：在每个时间步 $t$，智能体接收到某个观测 $O_t$（这是一个随机变量，所以我用大写 O）和一个来自某处的奖励 $R_t$（大写 R）。然后智能体执行一个动作 $A_t$（大写 A）。环境接收这个动作，你可以认为它发出一个新的观测和一个新的奖励，或者你也可以认为智能体从环境中接收（或拉取）这些。但现在，我们暂且认为环境就像一个函数，接收动作，然后返回下一个观测 $O_{t+1}$ 和下一个奖励 $R_{t+1}$。这是一个相当简单的设置，在某种意义上规模很小，但事实证明它也相当通用，我们可以用这种方式来建模许多问题。

### 奖励 (Reward $R_t$)

具体来说，奖励是一个*标量反馈信号*。它指示了智能体在时间步 $t$ 表现得有多好。因此，正如我所说，它定义了目标。

现在，智能体的任务是最大化*累积奖励*(cumulative reward)，而不是瞬时奖励，而是随时间累积的奖励。我们将这个累积奖励称为*回报*(Return)。

这个回报会延伸到未来，我没有具体说明它何时停止。最简单的想法是，未来总有某个时间点它会停止，这样回报就是明确定义且有限的。稍后我会谈到当它不停止时的情况，也就是连续性问题，那时你仍然可以定义一个明确定义的回报。

强化学习是基于*奖励假设*(reward hypothesis)的，即任何目标都可以被形式化为最大化累积奖励的结果。这基本上是关于这个框架通用性的一个陈述。

现在我鼓励你们思考一下，你是否同意这是真的？如果你认为不是真的，你能想出任何目标的例子，是你可能无法形式化为优化累积奖励的吗？

为了帮助你思考，我想指出，这些奖励信号可以是非常*稠密的*(dense)，可能每一步都有非零奖励；但它们也可能非常*稀疏*(sparse)。如果某个特定事件定义了你的目标，你也可以只在那个事件发生时获得一个正奖励，而在其他所有步骤都获得零奖励。这意味着存在一个奖励函数来模拟那个特定的目标。所以问题是，这是否足够通用？我自己还没能找到任何反例，但也许你能找到。

**问：** （提问者未录音，讲者复述问题大意）奖励是必须是正的吗？还是可以有负奖励，比如惩罚？

**答：** 不，这是一个非常好的问题。抱歉，我们用“奖励”(reward)这个词，但我们基本上指的是一个实值的强化信号。有时我们谈论负奖励时称之为惩罚(penalty)，这在心理学和神经科学中尤其常见。在更偏计算机科学的强化学习观点中，我们通常只使用“奖励”这个词，即使它是负的。那么，确实可以有让你远离某些你不希望重复的情况的东西。

我举个例子，稍后我会再次讨论这个例子，但现在给出也许有好处。你可以想象一个迷宫，你想走出迷宫，所以目标是离开迷宫。那么，有多种设置奖励函数来编码这个目标的方法。一种是，正如我刚才说的，每一步都给 0 奖励，但在你离开迷宫时给一个正奖励。但你也可以这样做：每一步都给一个负奖励，然后在你离开迷宫时停止这个回合(episode)。那么，最大化你的回报就意味着最小化负奖励的绝对值。所以它仍然编码了尽快离开迷宫的目标。你可以把一种看作是追逐胡萝卜，另一种是避开大棒。对于学习算法来说，这通常没有太大区别，至少对于形式化定义来说是这样。当然，在实践中，一切都很重要。

### 回报 (Return $G_t$)

好的，现在我们有了回报，我们就可以谈论预测这些回报了。为此，我们首先需要谈谈价值。

预期的累积奖励，基本上就是我们刚才定义的预期回报，就是我们所说的*价值*(Value)。在这种情况下，价值是状态的函数。这里的期望是条件在你将状态输入到函数中的状态上的期望，然后是对所有随机因素的期望。目标是通过选择合适的动作来最大化这个期望价值，而不是实际的瞬时价值或随机价值，因为通常你还不知道随机价值。

奖励和价值都定义了某件事的“可取性”(desirability)，但你可以认为奖励定义了某个*转移*(transition)（比如单步）的可取性，而价值则更普遍地定义了这个*状态*(state)的可取性，可能延伸到无限的未来。

### 价值函数 (Value Function)

（在幻灯片中显示公式：$V(s) = E[G_t | S_t = s]$）

预期的累积奖励，也就是我们刚才定义的预期回报，就是我们所说的*价值*(Value)。在这种情况下，价值是状态的函数 $s$。这里的期望是条件在你放入函数的状态 $s$ 上的，然后是对所有随机性的期望。目标是通过选择合适的动作来最大化这个*期望价值*，而不是实际的（可能是随机的）回报本身。

奖励和价值都定义了某物的可取性，但你可以认为奖励定义了某个特定*转移*（如一步）的可取性，而价值则更广泛地定义了这个*状态*的可取性，可能延伸到无限的未来。

另外请注意，因为我们将在本课程中大量使用这一点，回报和价值可以*递归地*定义。我在这里写下了回报的递归形式：时间步 $t$ 的回报 $G_t$ 基本上就是一步奖励 $R_{t+1}$ 加上从那之后的回报 $G_{t+1}$。这被证明是我们能有效利用的东西。

（在幻灯片中显示公式：$G_t = R_{t+1} + G_{t+1}$ （注：这里缺少折扣因子，后面会补充））

我之前说过，目标是选择动作，所以我们必须稍微谈谈这意味着什么。

同样，目标是选择动作以最大化你可能最终进入的每个状态的价值。这些动作可能有*长期后果*。就奖励信号而言，这意味着采取某个动作的即时奖励可能很低，甚至是负的，但如果它能带你到一个价值非常高的状态（这基本上意味着你稍后会获得高奖励），你可能仍然想采取这个动作。这意味着牺牲即时奖励以获得更多长期奖励可能更好。

这方面的例子包括：进行金融投资，你首先付钱投资某物，但希望稍后能收回更多钱。给直升机加油，这样做可能不会直接让你获得与目标相关的任何东西，但如果你不这样做，也许你的直升机在某个时候就无法工作了。在玩游戏时，你可能会阻挡对手的行动，而不是直接争取胜利，你首先阻止失败，这可能会让你稍后有更高的获胜概率。

在所有这些情况下，从状态到动作的映射，我们称之为*策略*(Policy)。

### 策略 (Policy $\pi$)

你可以把它想象成一个将每个状态映射到一个动作的函数。

也可以将价值条件在动作上。所以，不仅仅是条件在状态上，你可以条件在状态和动作对 $(s, a)$ 上。定义与状态价值非常相似。由于历史原因，符号表示略有不同，这被称为 *Q 函数*(Q-function)。所以对于状态，我们用 $V$，对于状态-动作对，我们用 $Q$。除了历史原因，真的没有其他理由。我们稍后会深入讨论这些。

（在幻灯片中显示公式：$Q(s, a) = E[G_t | S_t = s, A_t = a]$）

这里唯一的区别是，它现在也条件在动作 $a$ 上，否则定义与之前完全相同。

好的，如果大家都跟上了，我现在将讨论智能体的组件。

## 5. 智能体 (Agent) 的内部组件

我将从智能体状态开始。（房间里还有一点空间，如果还有人想找椅子的话。大家别太不舒服了。）

### 状态 (State $S_t$)

谢谢。首先，我已经稍微谈到了状态，但我实际上并没有说状态是什么。我相信你们对此有一些直观的概念。所以我会谈谈什么是智能体状态。

正如我所说，策略是从状态到动作的映射，或者换句话说，动作依赖于智能体的某个状态。智能体和环境都可能有一个内部状态，或者通常实际上都有。在最简单的情况下，可能只有一个状态，环境和智能体总是处于同一个状态。我们将在下一讲中相当广泛地涵盖这一点，因为事实证明，即使只考虑单个状态，你也可以有意义地讨论一些概念，比如如何做决策，它只是抽离了所有关于序列性、状态等问题。但下一整讲都将专门讨论这个。

但通常更普遍的情况是，存在许多不同的状态，甚至可能是无限多个状态。当我说无限多个时，我的意思是什么？可以就把它想成状态是一个连续的向量，也许它可以在某个无限空间内，仅仅因为你不知道它确切会在哪里，它基本上可以任意地处于那个状态中的任何位置。然后你就基本上处在一个典型的领域，深度学习也在这里大放异彩，在那里你或许可以在你从未见过的事物之间进行泛化，因为事物在某种意义上足够平滑。

智能体的状态通常不同于环境的状态，但一开始我们会将它们统一起来，我稍后会解释。但要记住，一般情况下，智能体可能不知道环境的完整状态。环境的状态基本上是环境返回其观测和（如果奖励是环境的一部分的话）奖励所必需的一切。就像我说的，它通常对智能体是不可见的。但即使它是可见的，它也可能包含大量不相关的信息。即使你考虑，比如说，现实世界，我们或者一个机器人在现实世界中操作，即使你能知道所有原子的位置以及所有其他可能在某种程度上与你的问题相关的事情，你可能也不想，甚至不能处理所有这些信息。所以在这种情况下，拥有一个比完整环境状态更小的智能体状态仍然是有意义的。

#### 历史 (History $H_t$)

取而代之的是，智能体可以访问一个*历史*(History)。它得到一个初始观测 $O_1$，然后这个循环开始：你采取一个动作 $A_1$，得到一个奖励 $R_2$ 和一个新的观测 $O_2$；你采取另一个动作 $A_2$，等等。原则上，智能体可以跟踪这整个历史。它可能会变得很大，但我们可以想象这样做。这样一个历史的例子可能是机器人的感觉运动流(sensorimotor stream)，即发生在机器人身上的所有事情。

然后，这个历史可以用来构建一个*智能体状态*(Agent State)。动作随后依赖于那个状态。

在*完全可观测*(fully observable)的情况下，我们假设智能体可以看到完整的环境状态。所以观测 $O_t$ 等于环境状态 $S_t^e$。这在环境特别简单的小问题中尤其有用，但有时在实际实践中也会出现。例如，如果你考虑玩一个单人棋盘游戏，你可以看到整个棋盘，这可能就是这种情况。或者即使你玩一个多人棋盘游戏，但你有一个固定的对手，这可能再次是这种情况。如果你在和一个未知的对手玩，情况就不再是这样了，因为你看不到对手的脑袋里面。

#### 马尔可夫决策过程 (Markov Decision Processes - MDPs)

如果这种情况成立，那么智能体就处于一个*马尔可夫决策过程*(Markov Decision Process, MDP)中。

我会定义它。你们中许多人可能知道这是什么，但马尔可夫决策过程本质上是一个有用的数学框架，我们将用它来推理和讨论强化学习中的许多概念。用它来推理比处理完整的、非马尔可夫的问题（我稍后会谈到）要容易得多。但由于马尔可夫假设，它也有点受限。

那么，什么是马尔可夫？一个决策过程是马尔可夫的，或者说它是一个 MDP，如果下一个状态和奖励的概率（我在这里按照新的 Sutton & Barto 版本写成奖励和状态的联合概率 $P(S_{t+1}, R_{t+1} | S_t, A_t)$），只依赖于当前状态 $S_t$ 和动作 $A_t$，并且如果你条件在完整的历史上，这个信息是完全充分的。

（在幻灯片中显示公式：$P(S_{t+1}, R_{t+1} | S_t, A_t) = P(S_{t+1}, R_{t+1} | H_t, A_t)$）

这意味着当前状态 $S_t$ 提供了你需要的所有信息来预测下一个奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。如果这个概率是固定的（即使你不知道这个概率，我不是说智能体知道它，但如果它存在并且是固定的），那么它就是一个马尔可夫决策过程。

直观地，这意味着给定现在（现在就是你的状态），未来独立于过去。

在实践中，这非常方便和有用，因为它意味着当你拥有这个状态时，你可以扔掉历史。历史可以无限增长，所以这是你不想一直做的事情。相反，你更喜欢可以扔掉一切，只保留那个状态的情况。另一种说法是，状态是历史的*充分统计量*(sufficient statistic)。

环境状态在大多数情况下通常是马尔可夫的。也有例外，例如，如果你考虑一个非平稳(non-stationary)的环境。但通常你可以认为环境状态是马尔可夫的，只是你无法感知它。所以即使事物本身是平稳的，它们也可能看起来是非平稳的。

历史本身也是马尔可夫的，因为显然，如果你条件在历史上，或者你条件在历史上，那是同一回事。但它会变得很大。

#### 部分可观测马尔可夫决策过程 (Partially Observable MDPs - POMDPs)

更常见的情况是，我们处于*部分可观测*(partially observable)的情况下。这意味着智能体只获得关于真实状态的部分信息。

这方面的例子包括：一个带有摄像头视觉但没有被告知其绝对位置或墙后面是什么的机器人；或者一个扑克牌游戏智能体，它只观察公共牌。第二种情况是部分可观测的，原因有多个：它看不到对手的牌，也看不到对手大脑内部的情况。

形式上，这些问题被称为*部分可观测马尔可夫决策过程*(Partially Observable Markov Decision Processes, POMDPs)。关于这些问题，特别是关于精确求解这些问题，有大量的文献。我们不会涵盖其中的大部分内容，但要记住，这实际上是常见情况：你只得到一些观测，但它们并不能告诉你完整的状态。这并不意味着你必须使用 POMDP 文献中精确求解这些问题的方法，特别是那些精确求解的方法，因为那往往是一个非常困难和有趣的问题，但它也往往计算成本很高。

同样，即使你只得到对环境状态的部分观测，环境状态本身仍然可以是马尔可夫的，但智能体根本无从得知。这清楚吗？好的。

#### 智能体状态 (Agent State)

那么我们现在可以谈谈智能体状态是什么了。正如我之前所说，智能体状态是历史的函数。智能体的动作依赖于状态，所以拥有这个智能体状态很重要。在一个简单的例子中，如果你只是让你的观测作为智能体状态。更一般地，我们可以将智能体状态视为随时间更新的东西：你有一个先前的智能体状态 $S_t$，你有一个动作 $A_t$，一个奖励 $R_{t+1}$ 和一个观测 $O_{t+1}$，然后你构建一个新的智能体状态 $S_{t+1}$。

（在幻灯片中显示公式：$S_{t+1} = f(S_t, A_t, R_{t+1}, O_{t+1})$）

请注意，例如，构建完整的历史就属于这种形式，你只是不断追加信息。但你也可以做其他事情，例如，你可以保持状态的大小固定，而不是让它像历史那样随时间增长。我在这里用 $f$ 表示的有时被称为*状态更新函数*(state update function)，这是一个重要的概念，我们稍后会回到这里。如何创建这样一个对你的智能体有用的状态更新函数，是一个非常活跃的研究领域，特别是当你不能仅仅依赖于你的观测时。

智能体状态通常比环境状态小得多，而且通常也比完整历史小得多，这仅仅是出于计算原因。

#### 示例 (Example: Maze with Aliasing)

这是一个例子。假设一个非常简单的问题，这是环境的完整状态。（也许不是完整的状态，也许迷宫里还有一个我没画出来的智能体）。但假设这是你的迷宫的完整状态。再假设有一个智能体，它感知到某一部分。所以现在的观测是部分的。智能体不知道它的坐标，它只看到这些像素，比如说。

现在可能发生的是，智能体在这个迷宫里走来走去，某个时刻之后，发现自己处于这种情况。这是一个部分可观测问题的例子，因为这两个观测彼此无法区分。所以仅仅基于观测，智能体无法知道它在哪里。

那么，一个让你们稍微思考一下的问题：对于任何奖励信号（因为我没有指定奖励信号是什么，如果你想，可以自己想一个，比如某个地方有目标），你如何在这个迷宫中构建一个*马尔可夫的智能体状态*？

**问：** （提问者声音不清，讲者复述并回应）把最后几个观测或者动作包含进来？

**答：** 对。确实，在这种情况下，你需要仔细检查具体的迷宫，看这样做是否足够。它可能足够，也可能取决于你的策略。如果你有一个原地不动的动作，那可能就不够了，因为你可能会两次看到相同的观测。如果这个迷宫里没有那个动作，那可能就足够了，我没有仔细检查。但更普遍的想法，我认为是好的想法，就是你使用历史的一部分来构建一个智能体状态，以某种方式区分这两种情况。如果你确实有一个特定的策略，可能在左边的状态你总是从上面来，而在右边的状态你总是从下面来，仅仅拥有这个额外的信息——上一个观测是什么——可能就足以完全区分这些情况。这确实就是状态更新函数的想法。这将是一个简单的状态更新函数，它只是连接前两个（观测），每次你看到一个新的观测，就丢掉最旧的那个。这实际上是相当常用的做法，例如，在你之前看到的 Atari 游戏中，就只是连接了几帧，这就是完整的智能体状态。所以它基本上只是一个增强的观测。

**问：** （提问者声音不清，讲者复述问题大意）关于 $O_{t+1}$ 和 $R_{t+1}$ 的时间下标问题？为什么是 $t+1$ 而不是 $t$？

**答：** 这里的顺序是这样的：你在某个状态 $S_t$。基于这个状态，你采取一个动作 $A_t$。然后我们认为时间是在你采取动作之后，基本上当你把动作发送给环境时，向前推进了一步。这只是一个约定，实际上有些人写 $R_t$ 而不是 $R_{t+1}$，所以要注意。但我们将采用这个约定，基本上时间步是在你将动作 $A_t$ 发送给环境时发生的。然后奖励 $R_{t+1}$ 和新的观测 $O_{t+1}$ 返回。我们可以认为下一个智能体状态 $S_{t+1}$ 是这个新观测的函数，这样当你采取下一个动作 $A_{t+1}$ 时，你已经可以把观测 $O_{t+1}$ 考虑进去了。如果这里是 $O_t$ 而不是 $O_{t+1}$，那么你在采取下一个动作时就无法考虑最新的观测。这是个好问题。

#### 总结 (Partial Observability Summary)

我已经讲了很多这些内容，但总结一下：为了处理部分可观测性，智能体可以构建一个合适的*状态表示*。这些例子包括，正如我之前所说：
- 你可以只让你的观测作为智能体状态。但这在某些情况下可能不够。
- 你可以把完整的历史作为你的智能体状态。但这可能太大，用完整的历史计算可能很困难。
- 或者，作为我之前展示的部分版本，你可以有一些增量更新的状态，也许只看观测，忽略奖励和动作，在这个例子中是这样 $S_{t+1} = f(S_t, O_{t+1})$。但如果你这样写下来，你可能会注意到这看起来与*循环神经网络*(Recurrent Neural Network, RNN) 非常相似，我知道我们在深度学习那边还没讲到，但我们会讲。RNN 的更新看起来和这个完全一样。这已经暗示了我们可以使用深度学习技术，比如 RNN，来实现状态更新函数。事实上，这已经被做到了。

所以有时，出于这个原因，智能体状态也被称为智能体的*记忆*(memory)。我们使用更通用的术语“智能体状态”，它可能包括记忆，也可能包括其他东西，如果你想这样认为的话。但你可以认为记忆是智能体状态的重要组成部分，特别是在这些部分可观测的问题中。或者，你可以认为记忆是构建合适的智能体状态的一个有用工具。

这结束了状态部分。如果有任何问题，请随时提出。

### 策略 (Policies)

否则，我将继续讲策略，这部分相当短。

策略定义了智能体的行为。它是从智能体状态到动作的映射。主要有两种情况：
- 一是*确定性策略*(deterministic policy)，我们通常写成一个函数 $a = \pi(s)$，输入状态，输出动作。
- 另一种是重要的*随机性策略*(stochastic policy)用例，其中动作基本上是在每个状态下选择每个动作的概率分布 $\pi(a|s) = P[A_t=a | S_t=s]$。

通常我们不会太仔细地区分这两者，你可以认为随机性策略是更一般的情况，有时分布恰好总是选择同一个动作，那么你就已经涵盖了确定性的情况。

注意，我没有指定这个函数的结构是什么，甚至没有指定动作的结构是什么。在本课程的开始，并且实际上在整个课程中，我们将主要关注动作可以被认为是离散集合的一部分的情况。例如，如果你想到 Atari 游戏中使用的操纵杆，它基本上有上、下、左、右、开火这类动作，但它没有“将你的马达朝这个方向稍微移动一点”这种动作。我们称后者为*连续动作*(continuous action)。也有可以处理这些连续动作的算法。对于符号表示来说，这并不重要，它只是一个函数，输出结果可能是离散情况下的一个整数，或者是连续情况下的一个向量或实数值。

我还没有讲如何学习这些东西，这将在课程的后面部分进行。这意味着我们可以继续，因为关于学习策略有很多要说的，但关于策略本身是什么，内容不多。

### 价值函数 (Value Functions)

我们将继续讨论价值函数。

如前所述，实际的价值函数只是条件在状态上的期望回报。还有一些我之前没有谈到的东西，但它也条件在一个策略 $\pi$ 上。我基本上在上一张幻灯片中隐藏了这一点。

（在幻灯片中显示公式：$V_{\pi}(s) = E_{\pi}[G_t | S_t=s]$）

我还隐藏了另一件事，我在这里介绍一下，那就是*折扣因子*(discount factor) $\gamma$。现在的回报 $G_t$ 重新定义了，它与之前的回报略有不同。

（在幻灯片中显示公式：$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$）

这里有一个 $\gamma$。如果 $\gamma = 1$，它就和之前一样，只是你未来奖励的累加。在许多情况下，我们实际上选择一个略小于 1 的 $\gamma$。这样做是为了权衡即时奖励与长期奖励，给即时奖励更高的权重。基本上，你在降低或*折扣*(discounting)未来奖励的权重，以偏好即时奖励，这就是为什么它被称为折扣因子。

回想我之前说的迷宫例子，每一步奖励为 0，但在离开迷宫时获得 +1 的奖励。如果你没有折扣，智能体基本上没有动力快速离开迷宫，只要它能在未来的某个时间离开迷宫就会满意。但是当你有了折扣，权衡突然就不同了，它会倾向于尽可能快，因为这样 $\gamma$ 的指数会更小。如果到达出口的步数更少，它对这个未来回报的折扣就会更少。

所以，价值依赖于策略，正如我所说。它可以用来评估状态的可取性，一个状态相对于另一个状态。因此，它也可以用来在动作之间进行选择。你可以说，向前规划一步，你可以使用你的价值。虽然我没有在幻灯片上写出来，但在这种情况下使用*动作价值*(action values) $Q_{\pi}(s, a)$ 可能更方便，因为它们直接让你访问每个动作的价值。

（在幻灯片中显示公式：$Q_{\pi}(s, a) = E_{\pi}[G_t | S_t=s, A_t=a] = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t=s, A_t=a]$）

这只是价值的定义，当然，我们稍后会在我们的智能体中近似这些东西，因为我们通常无法访问真实的价值。

哦，抱歉，顶部的回报定义那里少了一个加号。应该是 $G_t = R_{t+1} + \gamma G_{t+1}$。我会在幻灯片上传到 Moodle 之前修正它。

我之前说过未折扣情况下的递归形式，现在我再说一遍折扣情况下的：回报有一个递归形式，它是一步奖励加上剩余的回报，但现在被折扣了一次。

（在幻灯片中显示公式：$G_t = R_{t+1} + \gamma G_{t+1}$）

这意味着价值也有一个递归形式，因为我们可以直接写下价值是这个回报的期望。但事实证明，因为期望也可以放到内部，对这个 $G_{t+1}$ 取期望，这等价于再次将价值放在那里。

（在幻灯片中显示贝尔曼期望方程：$V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t=s]$）

这是一个非常重要的递归关系，我们将在整个课程中大量利用它。

符号方面，请注意我写的是 $a$ 从策略 $\pi$ 中采样 $a \sim \pi(\cdot|s)$，所以这基本上是假设随机性策略，但正如我所说，确定性策略可以看作是它的特例。这个方程被称为*贝尔曼方程*(Bellman equation)，由 Richard Bellman 在 1957 年提出。

有趣的是，对于*最优价值*(optimal value) $V^*(s)$（即对于任何策略可能的最高价值）也有一个类似的方程。

（在幻灯片中显示贝尔曼最优方程：$V^*(s) = \max_a E[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$）

这个方程写在这里。它基本上是选择那个最大化一步期望（奖励+折扣后继状态最优价值）的动作。所以它也是递归定义的。

你基本上可以将其视为一个方程组。如果状态数量有限，动作数量也有限，这只是一个你可以求解的方程组，从而得到最优价值和最优策略。为了做到这一点，你需要知道如何计算这个期望，这也是我们稍后会涵盖的内容。你将使用动态规划(dynamic programming)技术来解决这个问题。

**问：** （提问者声音不清，讲者复述问题大意）关于价值函数的递归形式如何推导？特别是 $V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t=s]$ 这一步？

**答：** 是的，基本上是基于回报的递归性，也就是 $G_t = R_{t+1} + \gamma G_{t+1}$。我假设这一点比较清楚，你可以把回报分解成单个奖励和剩余的回报（这本身也是奖励的累加）。为了得到价值的递归形式，特别是对于这个 $V_{\pi}(s)$ 的情况，只需要知道顶部的期望 $E_{\pi}[G_t | S_t=s]$ 可以写成 $E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t=s]$。由于期望的线性性质，这等于 $E_{\pi}[R_{t+1} | S_t=s] + E_{\pi}[\gamma G_{t+1} | S_t=s]$。关键在于认识到 $E_{\pi}[G_{t+1} | S_t=s]$ 等于 $\sum_{s'} P(s'|s, \pi(s)) V_{\pi}(s')$ （这里我简化了写法，假设只有一个动作，且 $s'$ 是 $S_{t+1}$）。所以，内部的回报 $G_{t+1}$ 的期望就是下一个状态 $S_{t+1}$ 的价值 $V_{\pi}(S_{t+1})$。这是一个嵌套期望，但它们是等价的。我们稍后会回到这个问题，我会给出更明确的公式，包括所有状态的概率求和，来展示这个递归关系成立。所以不用担心，下一讲不会涉及，但在再下一讲会。

**问：** （提问者声音不清，讲者复述问题大意）如果我们向前看10步，我们是优化当前的回报还是优化所有10步的回报？

**答：** 在每个状态，你基本上都想遵循那个能优化*从该状态开始的期望回报*的策略。这本质上意味着，在最后一个状态，你想做最优的事情。但在倒数第二个状态，你想做最优的事情，条件是假设在最后一个状态你会做最优的事情。所以这在某种意义上也是递归的。这里还有一个不同的问题，也许只是为了澄清，还有一个问题是：你关心哪些状态？你是关心从当前这个状态开始表现最优，还是关心从所有可能状态开始都表现最优？如果你能精确地解决所有问题，你实际上可以两者兼得，你可以从每个你可能在的当前状态和未来状态都最优。当我们开始使用近似方法时，你就必须选择：我关心哪些状态，不关心哪些状态？然后你可能会更关心在某些状态下有好的解决方案，而不是在其他状态。

**问：** （提问者声音不清，讲者复述问题大意）那么是否可以通过从终点开始反向递归来解决这个问题？

**答：** 是的。那么可以通过从终点开始反向递归来解决这个问题吗？找到…是的，那是一个简单的问题，某种意义上，因为那时你只看瞬时奖励。如果你在终点，你选择那个优化瞬时奖励的动作。这将给出那个状态的最优价值。然后，确实，这是一个有效且常用的解决方法：迭代地向后推导。你也可以，我稍后会更深入地讨论，同时考虑所有状态，并使用这些递归定义来基本上逐步逼近解决方案。所以，你确实可以从某些状态（比如终点）开始，然后递归，这可能更有效率；或者你可以同时处理所有状态，你仍然会得到最优解。

**问：** （提问者声音不清，讲者复述问题大意）这里我们近似的是期望累积回报，但有时我们可能更关心回报的整个分布，比如风险？

**答：** 是的，这是一个非常好的问题。这里我们近似的是期望累积回报或期望回报，但有时你更关心回报的整个分布。这绝对是真的，而且实际上研究得还不是那么多。在像*安全强化学习*(safe reinforcement learning)这样的领域已经做了一些工作，人们例如想要优化期望回报，但条件是回报永远不会低于某个阈值。但最近，我指的是去年，发表了一篇关于*分布强化学习*(distributional reinforcement learning)的论文，其中明确地建模了回报的分布。之前在这方面有一些少量的工作，但实际上没有你想象的那么多。事实证明，在这种情况下，你可以用递归定义做非常类似的事情。然后，确实，在某些情况下，建模分布非常有帮助。它可以帮助你引导决策远离风险情况，或者有时你实际上想要寻求风险（这在经济学术语中称为risk-seeking），这也可能有用，取决于你想要什么。谢谢，是的，非常好的问题，这是非常前沿的研究。

**问：** （提问者声音不清，讲者复述问题大意）在最优贝尔曼方程 $V^*(s) = \max_a E[...]$ 中，我们是在对动作 $a$ 进行边缘化吗？

**答：** 我们不是在边缘化(marginalizing)，我们是在对其进行最大化(maximizing)，这有点不同。但在某种意义上是相似的，在于你摆脱了对 $a$ 的依赖，因此也摆脱了对策略的依赖。因此，这个递归定义的最优价值不再依赖于任何特定的策略，因为我们在每一步都做这个最大化。你可以类似地考虑在每一步进行边缘化，但这略有不同，因为边缘化需要考虑一个分布，也就是你的策略 $\pi(a|s)$，即动作的分布。但在这种情况下，我们不关心一个固定的动作分布或固定的策略，而是选择对其进行最大化。但是的，除此之外非常相似。

#### 近似 (Approximations)

**问：** （提问者声音不清，讲者复述问题大意）如何处理连续领域，比如连续时间？以及如何处理近似？因为即使时间不是连续的，状态空间也可能非常大，这也可能需要近似。

**答：** 是的。问题有两个部分，一是如何处理连续领域，例如连续时间；另一个是如何处理近似，因为即使你没有连续时间，状态空间例如也可能巨大，这也可能需要你进行近似。

近似将在本课程中非常核心，我们会一直遇到它们。即使一切都很小，你仍然会遇到近似，因为你不知道这些价值。如果你无法计算期望，因为你不知道环境的模型，那么你仍然必须近似这些价值。你可以通过采样来简单地做到这一点，但是有些采样方式比其他方式更有效率，有些学习算法比其他算法更有效率。

关于连续时间点，贝尔曼方程实际上是离散时间的。还有一个不同的方程叫做*哈密顿-雅可比-贝尔曼方程*(Hamilton-Jacobi-Bellman equation, HJB)，或者有时叫哈密顿-雅可比方程，它基本上是这个方程的连续时间变体。那个方程在控制理论和控制问题中研究得更多，在那些领域中，很多东西通常更连续。但那时，人们通常也会对问题做更多的假设，这使得他们能够解决这些问题。它基本上再次变成一个方程组，但现在输入和输出是无限的，但如果你对问题做出合适的假设，你仍然可以解决这些问题。我们在这门课中不会过多涉及这一点，但如果你想了解，我很乐意给你一些指引。

**问：** （提问者声音不清，讲者复述问题大意）回报 (Return) 和价值 (Value) 的区别？回报是随机的吗？

**答：** 是的。回报是你实际看到的东西，所以它是随机的，是采样得到的。而价值是它的期望。谢谢。

**问：** （提问者声音不清，讲者复述问题大意）MDP 的马尔可夫属性在什么情况下会被违反？

**答：** 是的。不，我实际上已经给过一个例子。有时人们设置的环境中，这些转移概率会随时间变化，这意味着它已经不是马尔可夫的了。我们称之为*非平稳环境*(non-stationary environment)。在这种情况下，你仍然可以，总有办法绕过这个问题，这在数学上有点奇特。某种意义上，你可以说它变化的方式本身可能是某个东西的函数，所以如果你把那个也考虑进去，也许整个系统又变成马尔可夫了。但这通常很复杂，所以你不想关心。所以通常更简单的说法是：它随时间变化，那么它就不是马尔可夫的。还有其他原因可能导致它不是马尔可夫的，但非平稳性是经常出现的一个。

**问：** （提问者声音不清，讲者复述问题大意）如何定义回报？比如在金融投资例子中，除了直接用金钱差异，还有其他方法吗？

**答：** 是的，这是一个非常好的问题。如何定义回报，这实际上可以某种程度上归结为如何定义奖励的问题。例如，想想金融投资的例子。一个自然的方式是将每个奖励 $R_t$ 定义为你拥有的金钱的变化量 $\Delta M_t$。然后这个的累积 $G_t$ 就是你最终拥有的钱和你开始时拥有的钱之间的差额 $\sum R = M_{final} - M_{initial}$，你想最大化这个差额。这是非常自然的做法。但你也可以定义*事件*(events)。你可能会说，“当我的钱超过这个水平时，我会得到奖励”，或者“当它低于这个水平时，我会得到惩罚”。也许你并不关心确切的数字，也许你不关心建模金钱的期望回报，而可能关心金钱的某个其他函数。通常你可以将那个函数折叠进奖励函数中。

与之前关于建模分布而非期望回报的问题相关，实际做分布RL的算法看起来有点像那样。你实际上可以认为它是通过建模回报的变体来建模分布，这些变体在某种意义上更基于事件。

但有时，设置这些事件非常棘手。这就是为什么，例如在安全强化学习中，人们更典型的做法是，仍然建模例如期望的金钱，但随后只是添加一个条件，即他们不希望它低于某个值。也许可以将问题用不同的方式表述，比如对某些负面结果进行不同的加权，例如某些负奖励被赋予更重的权重，这就是学习系统得到的奖励。但有时这样做比直接用某些约束来解决问题更难。非常好的问题。

我想在这里说一个高层次的事情：我现在展示的很多东西基本上只是定义。例如，回报和价值是以某种方式定义的。而它们被定义的方式可能依赖于无限的未来，基本上是这样。这意味着在实践中你无法访问这些东西。这只是一个定义。稍后我们将讨论如何学习。当你学习时，我们会回到那个交互循环，你一次只得到一个奖励。这意味着你通常还没有获得完整的回报，或者你可能永远也无法获得完整的回报，因为它可能是无限长的。但即使你没有完整的回报，你仍然可以学习。

现在，我们只是在定义这些概念，稍后我们会回到这些问题上。所以，如果你不太确定如何使用这些概念，也不用担心，因为我会在未来的讲座中详细解释。

关于价值函数，最后一点，我们接下来要讨论的很多内容都围绕着*近似*(approximating)这些价值函数。正如我所说，这只是价值函数的定义，两者都是：一个针对某个策略 $\pi$ ($V_\pi$)，另一个针对最优价值函数 $V^*$。我没有说如何得到它们，或者如何近似它们。

现在，你可能想要近似这些价值函数的原因有很多。我已经提到了一个原因：你的状态空间可能太大，无法精确地建模这些东西，甚至无法将它们存入内存。那么你可能想要像在深度学习中通常用神经网络做的那样，跨状态进行泛化。

另一个近似的原因是，你可能无法访问计算这些期望所需的模型 $P(s', r | s, a)$。所以你可能需要采样，这意味着你最终会得到近似值，这些近似值在你采样越来越多时会变得越来越好，但可能永远不会完全精确。

**问：** （提问者声音不清，讲者复述问题大意）Q函数 $Q(s,a)$ 的贝尔曼方程是什么样的？

**答：** 是的，我可能应该把 Q 值也放在这里。它们会在后面的讲座中回来，届时我会明确给出它们。但既然我有了 V，也许我应该也放上 Q。我可以告诉你们这两种情况下的 Q 函数是什么样的，这可能会有帮助。

对于第一种（贝尔曼期望方程 for Q），我们条件在一个随机选择的动作上，这个动作来自你的策略 $\pi(a|s)$。如果你有一个 Q 函数 $Q_{\pi}(s, a)$，左边会有一个动作 $a$。我们会条件在实际采取的动作就是那个动作 $a$ 上。然后在递归的内部部分，你会有一个期望，期望的是 $R_{t+1} + \gamma Q_{\pi}(S_{t+1}, A_{t+1})$，其中 $A_{t+1} \sim \pi(\cdot|S_{t+1})$。所以，Q 函数的贝尔曼期望方程是：
$Q_{\pi}(s, a) = E_{\pi}[R_{t+1} + \gamma Q_{\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a]$

对于第二种（贝尔曼最优方程 for Q），在最优价值定义中，本质上会发生的是：左边同样会有一个动作 $a$ 作为条件 $Q^*(s, a)$。那么期望外面的 $\max_a$ 就消失了，因为我们已经选择了一个动作作为函数的参数，而不是对其进行最大化。但它会重新出现在内部：会有一个折扣因子乘以*下一个状态的最大动作价值*。所以，Q 函数的贝尔曼最优方程是：
$Q^*(s, a) = E[R_{t+1} + \gamma \max_{a'} Q^*(S_{t+1}, a') | S_t=s, A_t=a]$

但就像我说的，你现在不必记住这些，我们稍后会广泛地回到这些问题上。谢谢，好问题。

我刚才在谈论近似这些东西，我们将讨论如何有效地学习它们的算法。在很多情况下，比如在一个小的 MDP，状态空间很小，你可以用某种方式近似这些东西，也许你可以访问模型，我们会讨论这个。但我们也会讨论状态空间巨大的情况，也许是像素输入，你有成千上万的像素，每个像素可以有许多不同的值，我们可能仍然想学习一个价值函数。我们会讨论在那些情况下，当你无法访问模型时，当你需要采样时，如何学习。

无论我们做什么，当我们确实得到一个准确的价值函数时（我这里说的准确，基本上是指精确的最优价值函数），我们可以用它来做出最优的行为。更一般地，通过合适的近似，即使在极其巨大的领域中，我们也可以表现得很好。在这种情况下，我们失去了最优性，因为我们在学习，我们在近似，没有办法得到真正的最优策略。但在实践中，你其实不太在乎这个，因为好的性能本身已经非常有用。而且如果问题本身是棘手的，那么好的性能就是你能得到的最好的结果了。

### (可选) 模型 (Model)

这结束了智能体的价值部分。我将稍微谈谈*模型*(model)，尽管我们在本课程中对此覆盖较少。一个原因是，学习和使用这些模型实际上相当棘手，当然也有时间限制。

但模型基本上只是对环境动态(dynamics)如何工作的预测。

为简单起见，考虑完全可观测的情况。所以这里的状态 $S$ 既是环境状态也是智能体状态，这只是简化了思考，尽管你可以将其推广。那么我们可能有一些函数 $P(s' | s, a)$，它预测对于任何下一个状态 $s'$，基于当前状态 $s$ 和动作 $a$，到达 $s'$ 的概率。你也可以预测期望的下一个状态，但我在这里选择写下概率分布，所以我们明确地建模了下一个状态的分布。在某些情况下，只预测期望的下一个状态是什么样子是有用的。有时这不太有用，因为也许在期望中，你可能部分在一个洞里，而不是完全在洞里或完全不在洞里；或者门是半开的，现在在期望状态中它既是开的又不是开的，这可能不是一个真实的状态。所以在某些情况下，期望没有太大意义。在其他情况下则有意义。也许更通用的做法是为所有可能的状态建模下一个状态的完整分布。

类似地，对于奖励，我们也可以有一个模型 $R(s, a)$，它可能只依赖于状态和动作，然后预测对于那个状态和动作，奖励会是什么。你也可以扩展这个模型，也许让它也成为下一个状态 $s'$ 的函数 $R(s, a, s')$，并预测如果你有某个状态 $s$，动作 $a$ 和下一个状态 $s'$，那么奖励会是什么。在某些情况下，这很简单，也许如果你有这三样东西，奖励是确定性的，你可以很快学会它。在其他情况下，它可能是随机性的。也许在最坏的情况下，它甚至可能是非平稳的，所以你可能想要跟踪它，而不是假设它是一个平稳量来近似。

所以模型是有用的，我们稍后会讨论如何学习模型或者说用模型进行规划。但它并不能直接给你一个好的策略或最优策略，因为你仍然需要*规划*(plan)。

如下一讲所示，我们将讨论当你确实拥有精确模型时如何使用动态规划进行学习。我们将学习如何构建价值函数。在许多问题中，这确实是情况。例如，想想围棋游戏。如果你处于某个状态（基本上是完全可观测的），你采取某个动作，你确切地知道会发生什么。你确切地知道，如果你把你的棋子放在那里，棋子就会落在那里。你的下一个状态是完全已知的。在这种情况下，模型就在那里，你可以使用它。

在其他情况下，比如一个机器人在走廊里走动，这就棘手得多，你可能无法访问真实模型，而且学习真实模型可能非常困难。所以是否使用模型很依赖于具体领域。这就是为什么我基本上把模型列为智能体的一个*可选*部分。许多强化学习智能体没有模型组件，有些则有。

也有介于两者之间的版本，我们可能有一些看起来很像模型的东西，但它并不试图捕捉完整的环境动态，也许只捕捉其中的一部分，然后也许你仍然可以利用它。

关于模型，我还想说最后一件事。我给出的版本是提供完整分布的。有时让它是隐式的，而拥有一个你可以从中*采样*(sample)的模型会更有用。我们可以称之为*样本模型*(sample model)或*随机模型*(stochastic model)，或者在深度学习中常说的*生成模型*(generative model)。它基本上对于给定的状态和动作，给你一个采样的下一个状态。然后你仍然可以构建完整的轨迹，你可以从中采样，等等。这是你用期望状态模型做不到的事情，因为如果你从模型中得到一个期望状态，你不能必然地再把它输入到模型中，这可能没有意义，因为正如我所说，那个期望状态可能不是真实问题中实际出现的状态。

## 6. 强化学习智能体的分类

如果我们进行分类，这也是为了让你熟悉文献中用于描述这些事物的语言。构建智能体的方式有很多种。它可以包含这些组件中的任何一个，或者包含其中的许多组件。还有一个区别是，智能体是否拥有这些组件，以及它是否是*显式地*(explicitly)拥有。当我说显式地，我的意思是有一些近似，内部有一个实际的函数，你可以用它来计算某些东西。

所以当我们说我们有一个*基于价值*(value-based)的智能体时，我的意思是智能体内部有一个近似的价值函数，它用这个函数来判断哪些动作比其他动作更好。在这种情况下，可能没有显式的策略。事实上，当我说基于价值时，我的意思是*没有*显式的策略，而是我们在需要时从价值中*构建*策略。

或者，也许这是最简单的例子，可能有一个*基于策略*(policy-based)的智能体，它只有一个你的策略的表示，某种从状态到动作的映射，并且从不拥有显式的价值概念。

术语 *Actor-Critic* 用于指一个智能体同时拥有显式的策略和价值函数的情况。这一点有点……取决于你问谁，以及你读哪里的文献，因为有时人们说 Actor-Critic 系统也暗示了某种学习这些东西的方式。但我将简单地使用它，只要你有一个显式的策略和价值表示，并且你同时学习两者，我就会称之为一个 Actor-Critic 系统，为了简单起见。其中策略是 Actor（行动者），价值函数是 Critic（评论家）。

另外还有一个区别，是拥有环境模型还是无模型。*基于模型*(model-based)的智能体，基本上是前面幻灯片中的任何一种（基于价值、基于策略、Actor-Critic）都可以拥有一个模型。这就是这里的区别。当它们确实有模型时，我们可以说它是一个基于模型的智能体。所以你可以有一个基于模型的 Actor-Critic 智能体，或者一个基于模型的价值智能体。

对于基于价值的智能体，这些事情当然比我指出的要更灰色一些，因为你也可以有这些部分模型，或者可以被解释为模型的东西。事实上，有些人会说，“嘿，价值函数实际上也是某种类型的模型。”当然。但是当我在这种情况下说模型时，我指的是试图显式地建模环境的某些东西，而这个东西不是价值，也不是最优策略。

这看起来有点像这样（展示维恩图）：有这三个组件：价值函数、策略和模型。当你拥有例如价值函数和策略的重叠部分时，我们称之为 Actor-Critic。Actor-Critic 也可以是下面那个模型圈的一部分，所以你可以有一个带模型的 Actor-Critic，但它也可以是无模型的，也就是模型圈之外的所有东西。所以你可以有一个无模型的 Actor-Critic，或者一个基于模型的 Actor-Critic。你可以有一个基于模型的基于价值的智能体，和一个基于模型的基于策略的智能体。你也可以只有一个模型，正如我所说，那你仍然需要规划来得到你的策略，但在某些情况下，这是解决问题的合适方式。

我们将主要涵盖这里的顶部（指无模型部分），通常那里没有模型。但即使有模型，通常也会有一个策略和/或一个价值函数。

## 7. 强化学习的主要挑战

这是高层次的观点。我将谈谈强化学习中的一些挑战，我已经谈到了其中的一些，但明确说明是有好处的。

### 学习 vs. 规划 (Learning vs. Planning)

我们可能做两件根本不同的事情来解决一个决策问题。
一是*学习*(Learning)。环境最初是未知的。智能体与环境交互，并由此以某种方式得出一个更好的策略。你不一定需要学习模型，正如我所说，我将在本课程中给出不学习模型但仍然学会如何最优行动的算法示例。

另外一个是叫做*规划*(Planning)。当我们说规划时——规划是一个被极度重载的术语，对很多人来说意味着很多事情——但当我在本课程的概念范围内说规划时，我的意思是环境的模型是给定的或已学习的，并且智能体在这个模型中进行规划，也就是*没有外部交互*。这里的区别在于采样部分，对吧？在规划阶段，你不采样，你只是在思考。有时人们用诸如推理(reasoning)、思考(pondering)、思想(thought)、搜索(search)或规划(planning)等词来基本上指代同一个过程。

这里模型可能是未知的、近似的，这一点很重要，因为在你关心的、我们最终会考虑的问题中，你通常无法访问环境的完整模型。在某些情况下你可以，那么规划技术就有大量的文献，有非常高效和非常好的算法可以解决这些你有真实模型的问题。然而，需要注意的一件事是，这些算法通常假设你的模型是真实的。这意味着，如果你用一个近似模型进行规划，你可能会陷入一种情况，你的规划算法找到了这个非常奇特的策略，它恰好能穿过某处的墙壁，因为它错误地建模了那里有墙的事实。你也许可以让这些规划算法对模型错误更鲁棒，这是一个活跃的研究领域，但我们在这门课中没有时间深入探讨。

### 预测 vs. 控制 (Prediction vs. Control)

另一个经常做出并且有用的区分——术语很有用——是*预测*(Prediction)和*控制*(Control)之间的区别。这实际上不是二分法，这两者都是重要的，并且可以同时重要。但这些术语很重要，因为我们会经常使用它们，文献中的人也经常使用它们。

其中*预测*基本上意味着评估未来。我们讨论的所有这些价值函数都是对某物的预测，在这种情况下是回报。一个模型也是预测，它是对动态的预测。

*控制*意味着优化未来。当我们讨论那些价值函数的定义时，这个区别也很明显：其中一个价值函数是为*给定*策略定义的（$V_\pi$），所以这将是一个预测问题，我们有一个策略，我们只想预测那个策略有多好；而另一个价值函数被定义为*最优*价值函数（$V^*$），即对于任何策略，最优的做法是什么。那将是控制问题，找到最优策略。

我们主要关心的是控制问题，我们想优化事物。但为了做到这一点，有时预测那些不一定是最优的东西是有意义的。所以要记住，有时我们在优化，有时我们不是在优化，我们只是在预测。这也意味着，有时严格的监督学习技术在强化学习的背景下非常有用。有时你只想预测某些事情，也许你可以只使用监督学习，然后利用所有你可以利用的技巧，所有新的技巧，来有效地做到这一点，这可能非常有用。

它们也是密切相关的。如果你对回报有非常好的预测，通常提取一个好的策略是相当容易的。如果你设法预测了所有策略的价值，你也许可以直接选择最好的策略，你可以一步到位。当然，在实践中这不太可行。我们稍后会讨论一个叫做*策略迭代*(policy iteration)的算法，它迭代这个过程：你基本上有一个策略，然后你预测该策略的价值，然后你使用那些价值来选择一个新策略，然后你预测新策略的价值，然后你一遍又一遍地重复这些事情。这是通过使用预测来随时间改进你的策略的一种有效方式。

这里有另一个思考点，类似于我们之前的那些。这是给你们思考的一个问题，我不声称我有答案：如果我们能够预测一切，我们还需要其他任何东西吗？一个能够预测一切的系统，为了拥有比如说完整的人工智能，是否还缺少什么？

### 近似方法与深度强化学习 (Approximation & Deep RL)

现在，本讲座的大部分内容不是关于如何学习这些东西，但本课程的大部分内容将是。为此，现在就注意以下这点可能很重要：我所展示的所有这些组件基本上都是*函数*。
- 策略是从状态到动作的函数。
- 价值函数（名字里就有）将状态映射到价值。
- 模型将状态映射到状态，或状态的分布，或奖励，或这些的任何子集或超集。
- 状态更新函数（我们没怎么谈，或者说没怎么谈如何构建它们）也是函数，它们从你之前的状态创建一个新状态。我们谈到了一个给定的版本，有一个例子是你用一些先前或当前的观测来增强你的观测。但也许你也可以学习如何有效地构建你的状态。

在实践中，这意味着我们可以例如将这些东西表示为*神经网络*。然后我们也许可以使用所有的深度学习技巧来有效地优化这些函数，如果我们有一个好的损失函数和一个强大的函数类别，比如深度神经网络。也许这是一个有用的组合。

事实上，我们经常在现在所谓的*深度强化学习*(Deep Reinforcement Learning)中使用来自深度学习的工具，来找到对许多这些函数的良好且高效的近似。

需要注意的一件事是，在强化学习中，我们通常会违反典型监督学习中所做的假设。例如，数据通常*不是独立同分布*(i.i.d.)的。这有不同的原因。

那么，为什么它不是 i.i.d. 的呢？一个原因是你的策略会改变。仅仅是你改变策略这个事实就意味着数据会改变，这已经使你的问题非平稳(non-stationary)且非 i.i.d. 了。所以这对典型的监督学习技术来说是一个挑战。你可能需要进行跟踪(tracking)，而不是仅仅试图拟合某些数据。

非平稳性本身也可能以其他方式出现。例如，也许不仅你的策略改变，也许你的更新也改变，或者问题本身甚至是非平稳的。例如，在一个单一环境中可能有多个学习智能体，这使得一切都非常非平稳，非常困难，但也很有趣。

所以这里的要点是，深度强化学习是一个丰富而活跃的研究领域。尽管本课程的开始，特别是开始部分，将主要关注强化学习本身，而不太谈论与深度学习的联系，但我会在适当的时候偶尔建立这些联系。并且要记住，我们可能会使用许多深度学习的技术，但在应用它们时必须小心，因为你可能违反了这些技术被创造时所做出的某些假设。

另外，也许要记住的一件事是，目前神经网络并不总是最好的工具，但它们通常效果很好。过去在强化学习方面的大量工作是在*表格*(tabular)和*线性函数*(linear functions)上完成的，这更容易重新分析，而且它也已经是一个相当丰富的设置，你可以做很多事情。如今，很多人更喜欢使用深度网络，因为它们更灵活，而且它们往往更容易拟合奇怪的函数。但要记住，这不是唯一的选择，有时你用线性函数可能会更好，它在某种意义上可能更稳定，可能更容易学习。但那样的话，也许你的函数类别就受限了，也许它就不那么灵活了，也许这在某种程度上会损害你。除非你的特征足够丰富，但那样的话你可能需要以某种方式创建你的特征，也许这是你不想考虑的事情，或者也许你无法考虑，因为你对问题了解不够。所以这只是需要记住的事情。

### 探索 vs. 利用 (Exploration vs. Exploitation)

（这部分内容在原脚本中较后出现，但根据框架调整到此处）

我想提到的另一件事，这将是下一讲的重点，所以我将更深入地讨论它，这也是强化学习中相当核心的东西。正如我所说，我们通过交互学习，并且我们主动搜索信息。这有时被称为*探索*(exploration)和*利用*(exploitation)之间的困境。

因为在你学习的过程中，你会越来越多地了解你试图解决的问题，你会得到越来越好的策略，也许你会越来越倾向于只做你当前认为最好的事情。但如果你这样做，你基本上就停止了获取关于可能仍然存在的事物的新信息。所以你想要做的是，有时选择你以前从未做过的动作。这是因为你不会自动获得所有数据，你必须主动搜索数据。可能，例如，拐角处就有一个宝箱，如果你从不去那里，你永远不会知道。所以也许最终你想要确保你最终有时会去那些你从未去过的地方。这叫做探索。

但你也不想一直只是抖动，你不想一直只是做随机的事情，因为那会损害你的性能，损害你的奖励。所以，当你做一些你当前认为好的事情时，那叫做利用。

平衡这两者在一般情况下实际上相当棘手。所以下一讲将讨论许多可以做到这一点的方法。

目标是从新的经验中发现好的策略，但同时在此过程中不要牺牲太多奖励。“新的经验”部分是探索，“不牺牲奖励”是利用。

再想一个例子：一个智能体需要走钢丝才能穿过峡谷。在这种情况下，你可能想要利用一个已经能走过钢丝的策略，然后在到达另一边之后才开始探索。这表明，在某些情况下，先利用一下是非常好的，甚至是为了到达你可以有效探索的情境。所以这些事情是紧密交织在一起的，但我会更多地讨论这一点。

总结一下我刚才说的：探索找到更多信息，利用则利用你现在拥有的信息来尽可能最大化当前的奖励。两者都做很重要。这是一个基本问题，在监督学习中自然不会出现。事实上，我们甚至可以在不考虑序列性和状态的情况下就已经看到这个问题，这就是我们将在下一讲中做的。

简单的例子包括：如果你想找一家好餐馆，你可以去你现在最喜欢的餐馆，你会相当可靠地得到非常好的东西；或者你可以探索，尝试一些全新的东西，也许它比你以前见过的任何东西都好得多，或者也许不好。所以探索可能有点危险。另一个例子：石油钻探。你可以在你知道有石油的地方钻探，但也许那里的石油越来越少，也许开采成本越来越高；或者也许有时你想尝试一些全新的地方。在游戏玩法中，你只想时不时地尝试新的走法。但基本上，在你所能想到的任何决策问题中，都有这样的例子。

## 8. 示例

#### 迷宫示例 (Maze Example)

我会用一个例子让这些事情更具体一些。这是一个简单的例子，一个简单的迷宫。有一个起点和一个终点。只有四个动作：基本上可以向上、向左、向下和向右移动，或者北、西、南、东，如果你更喜欢的话。状态基本上就是智能体的位置，在这种情况下，它提供了你需要的所有信息，因为环境是固定的。（这有点奇怪，如果你仔细想想，状态不包括墙在哪里的观测，但因为一切都是固定的，位置仍然提供了你需要知道的一切。）

我们定义奖励是每时间步 -1。没有折扣因子，但因为每步都是 -1，你仍然被鼓励尽快离开迷宫。

那么，一个策略可能是什么样子？这实际上是这个迷宫的一个*最优策略*。在这种情况下，它在每个状态都给你一个确定性的动作。在某些问题中，最优策略可能是随机性策略，但在这个例子中，显然存在一个确定性策略，它能让你尽快离开迷宫。这可能是你需要解决这个问题所需的最简单的东西：策略映射。我没有具体说明我们可能如何学习它，我们稍后会涉及，但认识到这是你可能需要的最小的东西是好的。

或者，作为补充，你可能学习*价值*。这是我刚才展示的那个策略的*真实价值*。因为那个策略恰好是最优策略，这也是*最优价值函数*。如果我选择了不同的策略，数字会不同，那将是条件在该策略下的价值。这里的价值当然特别简单，因为它就是你到达目标之前的负步数，正如你所期望的那样。顺便注意，我们认为目标是你实际离开迷宫的时候，所以那最后一步仍然有 -1 的奖励，因为你仍然需要采取离开迷宫的动作才能停止，然后问题基本上就终止了。所以我们之前看到的回报，我让它延伸到潜在的无限未来，在这种情况下，它们实际上是有限的，最多是 24 步。

#### 网格世界示例 (Gridworld Example)

最后，在结束之前，我想再过一遍一个例子，这个例子比我之前给的迷宫例子稍微复杂一点，以确保这些事情更清楚一些。

这是一个非常简单的网格世界。智能体在里面走动，当它撞到墙时，得到 -1 的奖励。

我们可以问一个预测性问题：如果你只是随机地做事情，如果你在这个网格中随机移动，那么*价值函数*是什么？即条件在该随机策略下的期望回报是什么？

现在，这里有两个特殊的转移：每当你在状态 A 时，你实际上会转移到状态 A'，并获得 +10 的奖励。这是这个问题中你能得到的最高奖励。如果你在状态 B，你会得到 +5 的奖励，并转移到状态 B'。

现在可能不立即明显哪个更受偏好，因为一个奖励较低，但它也把你放得不那么远，所以可能更容易经常重复它；而另一个给你更高的奖励，但它是在你从 A 跳到 A' 之后发生的一个更长的跳跃。所以你需要更长的时间才能回来。然后就不清楚这两者哪个更受偏好了。

为了能够讨论哪个更受偏好，我们需要谈谈*折扣因子* $\gamma$，它权衡了高额即时奖励与未来的高额奖励。在这个例子中，折扣因子被设置为 0.9。这是一个有点随意的选择，但它意味着现在的价值函数既条件在均匀随机策略上，也条件在我们选择的折扣因子上，折扣因子与奖励一起基本上定义了目标是什么。所以这里的目标不仅仅是找到高奖励，还要相当快地做到，因为未来的奖励在这里被折扣了。

在图 B 下面给出了价值函数，它是均匀随机策略的状态价值函数。我们看到，实际上最受偏好的状态是你可能在的状态 A，因为你总是可靠地得到 +10 的奖励，然后你会转移到 A'，A' 的价值是负的，但负得不是那么厉害。A' 是负奖励的原因是你的策略是随机的，它会偶尔撞到墙，所以你会得到一些负奖励。而且因为那个状态离边缘相当近，你会比离边缘更远时更频繁地撞到墙。顺便注意，状态 B 的价值高于 5。你在从 B 到 B' 时得到 5 的奖励，但因为 B' 的价值是正的，所以处于 B 的价值高于仅仅是即时奖励的价值。而 A 的价值低于 10，因为它转移到的状态 A' 的价值是负的。

现在我们也可以问这个问题：如果我们能以任何我们想要的方式选择策略，那么*最优价值函数*是什么？最优策略是什么？最优策略的价值是什么？

现在，如果你先看右边的图（最优策略），你会发现在状态 A 和 B，所有的动作都是最优的。这是因为我们定义了它们都是相等的，对吧？你在状态 A 采取任何动作，你都会跳到 A'，你选择哪个动作都无关紧要。所以我们不关心你选哪个。我们还可以看到策略中也有很多结构。所以很可能，如果你要做一些函数近似，你可能会泛化得很好，因为在很多相似的、邻近的状态中，策略实际上非常相似。

这是一个非常简单的问题，你可能不需要做很多函数近似。但在一个更大的问题中，假设你是一个在走廊里的机器人，你当前的最优动作是沿着走廊向前移动，很可能下一步你的观测非常相似，你会因为泛化而继续向前走。

最优价值函数现在严格为正。原因很简单，这里的策略可以选择永远不撞墙。所以最优策略没有负奖励，它完全避免了那样做。因此，它可以去收集那些正奖励。现在也注意到，状态 A 的价值远高于 10，因为它可以获得即时奖励 10，然后在几步之后，它可以再次获得 10 的奖励，等等。这些是被折扣的，所以它不会无限增长到无穷大，但它确实会重复访问这些地方。再次顺便说一句，状态 A 比状态 B 更受偏好，这是沿途奖励和折扣因子共同作用的结果。你可以用不同的方式权衡这些。

#### Atari游戏示例 (Atari Example)

这是一个 Atari 游戏的例子。正如我所说，有一个系统基本上学会了这些 Atari 游戏。那个系统假设游戏的规则是未知的，所以没有已知的模型加载环境。然后系统会通过玩，仅仅是玩游戏来学习，然后直接从交互中学习。

这意味着什么？操纵杆现在基本上定义了动作，正如我所说。智能体不是你在屏幕上看到的那个化身，它实际上是那个按下操纵杆按钮的东西。然后这个输入到模拟器中，在这种情况下是这些 Atari 游戏的模拟器。模拟器输出奖励，在这种情况下，奖励是作为你也可以在屏幕上看到的得分的变化量提取出来的。你的观测就只是像素。在这种情况下，它会是一些像素的快速解释（注：原文指 quick concatenation of a few pixels，意为几帧像素的连接），因为实际上在这个 Atari 游戏中，有时比如说屏幕会闪烁，所以你可能在中间有完全黑色的单个观测。仅仅为了避免这成为一个问题，我们保留了一个非常短的几帧历史。这在某些游戏中也有帮助，你可能知道 Pong 这个游戏，你基本上有两个拍子，一个球在两者之间来回。如果你有不止一帧，你可以用它来判断球正在朝哪个方向移动，而如果你只有一帧，你实际上无法区分球正在朝哪个方向移动。那将是部分可观测的。

在 Atari 中，你也可以假设游戏规则已知来进行规划。在这种情况下，你可以查询模型。在每个状态，你可以尝试所有不同的动作，看看所有可能的下一个状态是什么，然后看看沿途的奖励是什么。然后你可以构建这棵巨大的树，并在这棵树内进行规划搜索。

在我们用来做很多实验的原始 Atari 模拟器中，实际的模拟器是确定性的。游戏是确定性的。所以如果你在某个状态，采取某个动作，总是会发生同样的事情。在模拟器的一个后续版本中，他们实际上通过使动作随机化来增加了一点噪音（持续时间稍长或稍短），只是为了破坏某些严重利用环境确定性的算法。因为最终你想要的是能够处理非确定性情况的算法。而且关于这些 Atari 游戏的大部分工作实际上使用了在环境非确定性时同样有效的算法。但是，当环境是确定性的时候，你可以做某些算法，而当它是随机性的时候你就做不了。

#### 仿真行走示例 (Simulated Locomotion)

（播放视频）我会解释你正在看什么，因为它挺酷的。这是一个学习系统，对吧？这里有东西在学习。那么这里在学习什么呢？有东西在学习如何控制这个，如果你想称之为虚拟机器人的，模拟的，这些关节。

现在，关于这个有趣的是，基本上除此之外，给系统的信息非常少。这里的奖励函数本质上是“前进”。基于智能体的身体和环境，智能体学会了前进，而且是以有趣的方式。特别要注意，我们……没有人给它输入任何关于如何移动或如何行走的信息。没有预先编码任何关于如何移动关节的东西。这意味着你也可以将其应用于不同的身体。同样的学习算法，不同的身体，仍然学会移动。你可以把它放在不同的环境中，你也可以让它在平面上行走，而不是在一条线上。基本上，它可以选择要么爬过障碍物，要么有时绕过它们。

同样，所有这一切，都只是那一个简单的目标，也就是“前进”的奖励。这里有一个普遍的原则：当你确实编写一个强化学习系统，并且你必须定义奖励函数时，通常最好精确地定义你想要什么。

（视频中出现奇怪的动作）正如你可以看到的，有时你可能会得到稍微出乎意料的解决方案，而且不完全是最优的。那么，为什么会这样呢？有谁知道为什么这个智能体做出这些奇怪的动作吗？

**可能的回答：** 为了平衡？为了记忆？

**答：** 是的，为了平衡，这是一个很好的答案。所以你的智能体状态的一部分可能是你之前的动作会被编码在你的观测中。所以你可以用你的动作在某些情况下为你提供某种记忆，对吧？这是一个非常有趣的观点。

另一件事是，我在这里提到了奖励是“前进”。通常对我们来说，情况并非如此。通常我们想去某个地方，但我们也倾向于想要最小化能量消耗，我们不想太累。但如果你没有那个约束，你也可能得到这些奇怪的东西，它们可能有助于平衡，它们可能有助于记忆，但它们也可能仅仅是因为它们无害而存在，对吧？这在强化学习中是相当普遍发生的。如果你建模问题，确保在你的奖励函数中放入你真正关心的东西，否则系统会优化你给它的、你要求它的东西，而这可能不是你想要的。在这种情况下，没关系，对吧？因为我们实际上并不关心这个，而且在这种情况下它实际上可能是有帮助的，我实际上不知道，对吧？它可能有助于平衡。但在其他情况下，在你的（奖励）中加入一些它会……哦，如果你想让它做某事，也许你应该先这样做，但这有点危险，因为在某些情况下，它会优化那个你只希望作为中间子目标的东西，而不是你真正关心的最终目标。

**问：** （提问者声音不清，讲者复述问题大意）为什么它在跑而不是爬？

**答：** 是的，这是一个非常好的问题。为什么它在跑而不是爬？这有两个原因。一个原因是奖励本质上是尽可能快地前进。另一个原因是身体。如果你有一个不同的身体，爬行实际上可能是更有效的，或者滚动可能是更有效的。网上有一些很酷的关于类似系统的视频，人们做了类似的事情，也有一些早期的工作，人们使用进化方法来处理各种奇怪的身体，看看它会找到什么样的移动方式。你会发现非常可爱和奇怪的移动方式。

## 9. 课程后续内容概要

好的，那么我想用这个结束。

（展示课程大纲幻灯片）

我只想给你们一个关于本课程将包含内容的高层次概述。我们将学习…讨论如何通过交互学习，这是主要内容。重点是理解核心原理和学习算法。

在课程的某些时候，我会给出一些实践或经验上的见解，只要我有的话。并且在课程结束时，我们会有来自 Vlad Mnih 和 Dave Silver 的客座讲座，他们会谈论他们的工作，其中也包括一些这样的见解。但总的来说，我们将主要在一个相当概念化的层面上讨论这些，但这离实践并不遥远，我会尽可能指出如何将这些东西变成现实，以及如何让它们实际工作。

另外，还会有作业，如你所知，这将允许你去做那些并尝试一下。

所以主题包括：
- 下一讲，我们将讨论在所谓的 *Bandit 问题*中的探索。Bandit 基本上来自“独臂老虎机”(one-armed bandit)，也就是一种老虎机，你只有一个动作（拉杆），每次尝试都会得到一个随机回报。这在文献中被推广到了数学框架，称为*多臂老虎机*(multi-armed bandit)问题，你基本上可以认为有多个动作可以做，多个老虎机，每个都给出随机奖励，你的工作是决定哪个是最好的。这里没有状态，总是同样的老虎机，没有任何变化，问题中没有序列性。所以这里唯一的问题是探索和利用的问题：如何权衡这两者，以及如何学习——你如何学习这些东西的价值？但在那种情况下，这相当简单。
- 然后，稍后，我们将更多地讨论*马尔可夫决策过程*(Markov Decision Processes)。我已经稍微接触过这些，但我将讨论如何在其中进行规划，使用*动态规划*(dynamic programming)等方法。
- 我们将转向*无模型预测和控制*(model-free prediction and control)，我们将不再假设我们拥有模型，然后我们将不得不进行采样。
- 会有叫做*策略梯度方法*(policy gradient methods)的东西，这是一类算法，允许你直接学习策略，我们会讨论这个。
- 我们将讨论*深度强化学习*中的挑战，如何建立一个完整的智能体，如何组合这些东西，以及如何*整合学习和规划*。

在结束之前，还有什么问题吗？

**问：** （提问者声音不清，讲者复述问题大意）作业什么时候发布？

**答：** 我不知道，Moodle 上某个地方有写。我以前知道，但我现在不想承诺一个日期然后说错了。

**问：** （提问者声音不清，讲者复述问题大意）作业会在本周发布吗？

**答：** 哦，是的。我以为问题是何时截止，而不是何时发布。好的。如果 Moodle 说本周开始，那可能应该发布了。我得查查它在哪里。但谢谢提醒，因为这很重要。我们需要，如果那个时间表是正确的，我们需要确保尽快发布。

**问：** （提问者声音不清，讲者复述问题大意）如果作业本周初就该发布了，截止日期会调整吗？

**答：** 是的。如果它本应在本周初发布，那么我们还必须检查截止日期是否仍然正确。也许需要推迟。但我需要检查时间表，并与应该发布作业的人核对。谢谢，这非常非常重要。

**问：** （提问者声音不清，讲者复述问题大意）Sutton & Barto 书籍的链接打不开？

**答：** 啊，是的，有时会发生这种情况。我想我可能给的链接有点问题，这是一种可能。另外，根据我的经验，他的网站并不总是能正常工作。但是，好吧，如果你直接谷歌搜索 "Sutton Barto 2018"，你应该能找到这本书。或者加上 "reinforcement learning"，如果你想非常确定的话。但那样你应该能找到它。

**问：** （提问者声音不清，讲者复述问题大意）Moodle 上的幻灯片会更新吗？

**答：** 是的，是的。我会确保这些幻灯片总是更新的。我会努力……现在 Moodle 上的基本上是去年的幻灯片，我们会尽快更新，只要可能。所以当幻灯片确实改变时——有些会保持不变——但当幻灯片改变时，我们会尽量提前更新它们。这次没做到，但我会尽力尽快把它们放上去。但是要注意，如果你现在就看未来讲座的幻灯片，材料可能会略有变化，但不会太大，主要是略有变化。所以我在这方面会尽力。

好的，我想我们时间就到这里了。谢谢大家的到来。

---

# 要点回顾

**1. 课程管理与资源**
- 讲座安排：大部分在固定时间，具体查看Moodle上的时间表，留意更新。
- 提问：使用Moodle进行提问。
- 评分：通过作业进行。
- 背景材料：
    - 主要参考Sutton & Barto的书籍《Reinforcement Learning: An Introduction》第二版（在线有草稿，即将出版）。
    - 本讲座主要对应书中的第1章和第3章。
    - 强烈建议阅读第1章，了解高层次概念、历史发展和不同观点。

**2. 强化学习简介**
- **动机**
    - 第一波自动化：工业革命，用机器自动化体力解决方案（如火车替代马）。
    - 第二波自动化：数字革命，用机器自动化脑力解决方案（如计算器）。
    - 下一阶段：定义问题，让机器自己学习解决方案。这需要学习能力，特别是从数据/交互中学习。
- **什么是强化学习？**
    - 核心思想：智能体 (Agent) 通过与环境 (Environment) 的*交互*来学习。
    - 关键特征：
        - 主动学习 (Active)：智能体需要主动行动以获取信息。
        - 序贯决策 (Sequential)：当前动作会影响未来的状态和接收到的信息。
        - 目标导向 (Goal-directed)：旨在达成特定目标，但不一定预先知道最优路径。
        - 试错学习 (Trial-and-Error)：可以从非最优的经验中学习。
- **基本交互框架**
    - 智能体 (Agent)：学习和决策系统。
    - 环境 (Environment)：智能体外部的一切。
    - 交互循环：智能体执行动作 (Action) $A_t$ -> 环境接收动作，发生状态转移，返回新的观测 (Observation) $O_{t+1}$ 和奖励 (Reward) $R_{t+1}$ -> 智能体接收信息，继续循环。
- **学习目标**
    - 发现未知解决方案：找到人类难以设计或发现的最优策略（例如，AlphaGo）。
    - 在新环境中快速学习/适应：智能体在遇到未曾经历的情况时能够在线学习和调整（例如，机器人在未知地形中导航）。
- **关键挑战概念**
    - 时间与长期后果：当前的决策会影响未来的收益。
    - 主动收集经验：智能体需要主动探索以获取全面的信息。
    - 预测未来：为了评估长期后果，需要对未来状态和奖励进行预测。
    - 处理不确定性：环境可能本身是随机的，或者智能体对环境状态的了解不完全（部分可观测），或者智能体自身的策略是随机的。

**3. 与其他学科的关系**
- **相关领域**
    - 强化学习与多个学科交叉：计算机科学（机器学习）、神经科学、心理学、工程学（最优控制）、数学（运筹学、马尔可夫决策过程、动态规划）、经济学（多智能体、优化）。
- **与机器学习其他分支对比**
    - 监督学习 (Supervised Learning)：从带有标签的样本 (输入, 输出) 中学习映射关系。
    - 无监督学习 (Unsupervised Learning)：从未标记数据中发现结构或模式。
    - 强化学习 (Reinforcement Learning)：
        - 通过奖励信号 (Reward Signal) 学习，奖励是评价性的（好坏程度），而非指导性的（应该做什么）。
        - 反馈可能有延迟 (Delayed Feedback)。
        - 时间和序列性 (Time/Sequentiality) 很重要，早期决策影响后期数据。

**4. 强化学习系统的核心要素**
- **奖励 (Reward $R_t$)**
    - 一个标量反馈信号，表示智能体在时间步 $t$ 表现如何。
    - 定义了智能体的目标。
    - 奖励信号可以稠密（每步都有），也可以稀疏（仅在达成目标等特定事件时出现）。
    - 可以是正奖励（鼓励），也可以是负奖励（惩罚）。
    - *奖励假设 (Reward Hypothesis)*：任何目标都可以被形式化为最大化累积奖励的期望。
- **回报 (Return $G_t$)**
    - 从时间步 $t$ 开始的累积（通常是带折扣的）未来奖励。
    - 定义： $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...$
    - $\gamma$ 是折扣因子 ($0 \le \gamma \le 1$)，权衡即时奖励与未来奖励的重要性。$\gamma < 1$ 使得无限序列的回报有限，并鼓励更快获得奖励。
    - 递归形式： $G_t = R_{t+1} + \gamma G_{t+1}$
- **价值函数 (Value Function)**
    - 对未来回报的期望，衡量“状态”或“状态-动作对”的好坏程度。
    - 状态价值函数 (State Value Function)： $V_{\pi}(s) = E_{\pi}[G_t | S_t=s]$，在状态 $s$ 开始，遵循策略 $\pi$ 的期望回报。
    - 动作价值函数 (Action Value Function / Q-Function)： $Q_{\pi}(s, a) = E_{\pi}[G_t | S_t=s, A_t=a]$，在状态 $s$ 执行动作 $a$，然后遵循策略 $\pi$ 的期望回报。
    - 用于评估和选择动作。可能需要牺牲短期奖励以获得更高的长期价值。
    - *贝尔曼方程 (Bellman Equation)*：价值函数的递归关系式，例如 $V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t=s]$。
    - *贝尔曼最优方程 (Bellman Optimality Equation)*：最优价值函数的递归关系式，例如 $V^*(s) = \max_a E[R_{t+1} + \gamma V^*(S_{t+1}) | S_t=s, A_t=a]$。
- **策略 (Policy $\pi$)**
    - 智能体的行为方式，是从状态到动作的映射。
    - 确定性策略 (Deterministic Policy)： $\pi(s) = a$。
    - 随机性策略 (Stochastic Policy)： $\pi(a|s) = P[A_t=a | S_t=s]$。
    - 强化学习的目标是找到最大化期望回报的策略。

**5. 智能体 (Agent) 的内部组件**
- **状态 (State $S_t$)**
    - 智能体用来做决策所依据的信息。
    - 环境状态 vs. 智能体状态：环境状态包含环境的所有信息，通常智能体无法完全观测。智能体状态是智能体内部维护的信息表示。
    - 历史 (History $H_t$)：观测、奖励、动作的序列 $O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t$。
    - 马尔可夫属性 (Markov Property)：未来仅依赖于当前状态，而与历史无关。即 $P[S_{t+1}, R_{t+1} | S_t, A_t] = P[S_{t+1}, R_{t+1} | H_t, A_t]$。如果智能体使用的状态满足马尔可夫属性，则问题构成马尔可夫决策过程 (MDP)。
    - 部分可观测性 (Partial Observability)：智能体的观测 $O_t$ 不足以完全确定环境状态。这种情况构成部分可观测马尔可夫决策过程 (POMDP)。
    - 状态构建：智能体状态 $S_t$ 是历史 $H_t$ 的函数， $S_t = f(H_t)$。在POMDP中，可能需要利用历史信息（如过去的观测序列）或使用循环神经网络（RNN）等方法构建包含记忆的状态表示 $S_{t+1} = f(S_t, A_t, R_{t+1}, O_{t+1})$。
- **策略 (Policy)**：(见核心要素部分) 定义智能体行为。
- **价值函数 (Value Function)**：(见核心要素部分) 评估状态或状态-动作对的价值。
- **(可选) 模型 (Model)**
    - 智能体对环境工作方式的内部表示/预测。
    - 预测状态转移： $P(s' | s, a)$ 预测在状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率。
    - 预测奖励： $R(s, a)$ 或 $R(s, a, s')$ 预测在状态 $s$ 执行动作 $a$ (可能到达 $s'$) 得到的奖励。
    - 作用：允许智能体进行规划 (Planning)，即在与环境实际交互之前，通过模型进行“思考”或模拟。
    - 无模型 (Model-Free) RL：不学习环境模型，直接从经验中学习策略或价值函数。
    - 基于模型 (Model-Based) RL：学习环境模型，并利用模型进行规划或辅助学习。

**6. 强化学习智能体的分类**
- 基于价值 (Value-Based)：显式学习价值函数，策略通常是隐式的（例如，根据价值贪婪地选择动作）。
- 基于策略 (Policy-Based)：显式学习策略函数。
- Actor-Critic：同时显式学习策略（Actor）和价值函数（Critic）。
- 基于模型 (Model-Based) vs. 无模型 (Model-Free)：根据是否学习和使用环境模型进行区分。这些类别可以组合。

**7. 强化学习的主要挑战**
- **学习 vs. 规划 (Learning vs. Planning)**
    - 学习：环境未知，通过与环境交互改进策略。
    - 规划：环境模型已知（或已学习），通过模型进行计算和模拟来改进策略，无需与真实环境交互。
- **预测 vs. 控制 (Prediction vs. Control)**
    - 预测：评估未来（给定策略下），估计价值函数或模型。
    - 控制：优化未来，寻找最优策略。通常控制问题包含预测子问题（例如策略迭代）。
- **近似方法与深度强化学习 (Approximation & Deep RL)**
    - 当状态空间巨大或连续时，或模型未知需要采样时，需要使用近似方法。
    - 可以使用函数逼近器（如神经网络）来表示策略、价值函数、模型或状态更新函数。
    - 深度强化学习 (Deep RL)：使用深度神经网络作为函数逼近器。
    - 挑战：RL中的数据通常非独立同分布 (non-iid)，且可能非平稳 (non-stationary)（由于策略或环境变化），这给基于标准监督学习的技术带来挑战。
- **探索 vs. 利用 (Exploration vs. Exploitation)**
    - 基本困境：是利用当前已知最佳策略获取奖励，还是尝试新动作以发现可能更好的策略？
    - 探索：尝试未知的动作或状态，以获取更多关于环境的信息。
    - 利用：执行当前认为最优的动作，以最大化已知期望回报。
    - 需要在两者之间取得平衡，这对有效学习至关重要。（下一讲重点）

**8. 示例**
- 迷宫示例 (Maze Example)：展示了策略、价值函数（负数表示到达目标的步数）和（可能不完整的）模型的概念。
- 网格世界示例 (Gridworld Example)：通过特殊转移点A->A'和B->B'，对比了随机策略下的价值（预测）和最优策略下的价值（控制），以及折扣因子的影响。
- Atari游戏示例：输入为像素，动作为游戏手柄操作。展示了模型自由学习，以及处理部分可观测性（如Pong游戏中球的方向）需要利用历史信息（帧叠加）。
- 仿真行走示例 (Simulated Locomotion)：仅给予“前进”的奖励信号，智能体能自发学习出复杂的行走或移动步态。说明了RL发现复杂行为的能力，以及奖励函数设计的重要性（如是否包含能量消耗等）。

**9. 课程后续内容概要**
- Bandit 问题（无状态下的探索与利用）。
- 马尔可夫决策过程 (MDP) 与规划（动态规划）。
- 无模型预测与控制（基于采样的方法）。
- 价值函数近似。
- 策略梯度方法。
- 整合学习与规划。
- 深度强化学习的挑战与实践。