# 最优传输 (Optimal Transport), Part 2 - Marco Cuturi

[Optimal Transport, part 2 - Marco Cuturi - MLSS 2020, Tübingen](https://www.youtube.com/watch?v=Bl8ZDN3Dbwk&list=PLUnQ4mNHOUWU0IvAiPhIq6hNrMF1DnviQ&index=5)

### 导言

本次讲座实录是 Marco Cuturi 关于最优传输（Optimal Transport, OT）系列讲座的第二部分，重点聚焦于OT的**计算**方面。在第一部分对OT理论及其吸引力进行铺垫后，本部分深入探讨了如何实际计算OT距离和传输方案。

讲座首先讨论了**精确计算OT**的几种情况。虽然理论上OT的定义广泛适用，但精确求解通常具有挑战性。内容涵盖了少数存在闭式解的“简单”场景，如一维测度（引出切片Wasserstein距离）、高斯测度以及椭圆分布。随后，重点转向了在数据科学中更为常见的离散测度（经验测度）上的OT计算，阐述了其如何形式化为一个线性规划（LP）问题，并介绍了对偶问题及其解释。然而，精确LP方法面临计算复杂度高、解对输入不光滑（不可微）以及难以并行化等显著局限，使其在处理大规模数据集时效率低下，且在统计性质上存在样本复杂度随维度指数增长的问题（维度灾难）。

为了克服这些挑战，讲座的后半部分转向了**为数据科学计算OT**，核心是**正则化**。讨论了正则化的必要性以及多种正则化策略。其中，**熵正则化**被详细介绍，它通过在原始OT问题中加入熵惩罚项，牺牲部分最优性以换取计算效率、解的唯一性和光滑性。求解熵正则化OT的关键算法——**Sinkhorn算法**——得到了深入阐述，包括其迭代原理、基于高效矩阵运算的实现、并行性优势以及其优越的可微性，后者使得正则化OT能够方便地集成到现代机器学习的梯度优化框架中。最后，讲座还包含了问答环节，进一步探讨了OT与聚类等概念的联系。

### 内容纲要

```
Optimal Transport, Part 2 - Marco Cuturi
├── 引言与回顾
│   ├── 讲座重点：计算方面（精确与近似）
│   └── 目标：理解OT在实践中的应用与实现
├── 精确计算OT (Computing OT exactly)
│   ├── 何时可以计算OT？
│   │   ├── 离散-离散 (网络流，成本高)
│   │   ├── 离散-连续 (低维可行，高维需随机优化)
│   │   └── 连续-连续 (数值方法如Benamou-Brenier)
│   ├── 易于计算的情况 (闭式解)
│   │   ├── 一维测度 (Univariate Measures)
│   │   │   ├── 基于分位数函数的闭式解
│   │   │   └── 应用：切片Wasserstein距离 (Sliced Wasserstein Distance)
│   │   ├── 高斯测度 (Gaussian Measures)
│   │   │   ├── 基于均值和协方差的闭式解 (平方欧氏距离成本)
│   │   │   └── 最优映射是仿射变换
│   │   └── 椭圆分布 (Elliptical Distributions)
│   │       └── 最优映射仍是线性/仿射
│   ├── 离散测度上的OT (经验测度)
│   │   ├── 两个狄拉克测度之间 (成本即距离)
│   │   ├── 线性分配 (等价于最优匹配)
│   │   ├── 两个经验测度上的OT (LP formulation)
│   │   │   ├── 定义：成本矩阵M, 耦合矩阵P
│   │   │   └── 线性规划形式
│   │   └── 对偶Kantorovich问题
│   │       ├── 对偶LP形式
│   │       └── 对偶变量的意义 (成本贡献)
│   └── 求解精确OT的问题与挑战
│       ├── 计算复杂度高 (如 O(N³ log N))
│       ├── 非光滑性/不可微性 (对输入敏感，不利于梯度优化)
│       └── 难以并行化 (传统LP求解器)
├── 为数据科学计算OT (Computing OT for data sciences)
│   ├── 实践者关心的问题
│   │   ├── 计算效率
│   │   └── 统计性质 (样本复杂度)
│   ├── 样本复杂度问题
│   │   └── 维度灾难 (误差随维度指数级增长 O(N⁻¹/ᵈ))
│   ├── 从理论到实践的挑战 & 正则化的必要性
│   ├── 正则化OT的多种方式
│   │   ├── 对偶问题正则化 (限制势函数，如RKHS, Lipschitz, 梯度惩罚)
│   │   └── 原始问题正则化 (修改成本M, 修改测度表示, 添加耦合P的先验)
│   ├── 重点：熵正则化 (Entropic Regularization)
│   │   ├── 动机 (牺牲最优性换取效率、稳定性、光滑性)
│   │   ├── 公式 (添加 -γ H(P) 项)
│   │   ├── 性质 (强凸, 唯一解, 解稠密, 连接OT与MMD)
│   │   └── Sinkhorn算法 (Fast & Scalable Algorithm)
│   │       ├── 算法原理 (基于 P = diag(u)K diag(v))
│   │       ├── 迭代步骤 (交替更新 u, v)
│   │       ├── 计算优势 (矩阵-向量乘法, 易并行, GPU友好)
│   │       ├── 数值稳定性 (对数域计算)
│   │       └── Sinkhorn优势总结 (快速, 可扩展, 可微, 连接W与MMD, 统计性质改善)
│   └── Sinkhorn 的可微性
│       └── 可通过自动微分框架反向传播
└── 问答环节 (Q&A)
    ├── 问题1：Sliced Wasserstein的近似保证
    ├── 问题2：高斯混合模型的OT
    ├── 问题3：OT与聚类(K-means, 模糊聚类, 稀疏性)的关系
```



# 讲座实录

## 引言与回顾

早上好，或者下午好，无论大家现在身处何时区。昨天我们介绍了最优传输，其中包含一些数学内容，但也有一些直观解释，同时也包含了很多承诺。我基本上是在努力推广最优传输这个理论，说明它的用处，并试图让大家初步了解其原因。

今天的讲座将更侧重于计算。我希望大家能看到其中涵盖了各个方面：精确计算、近似计算，希望通过这次综述，大家能更好地理解这如何在实践中使用，以及应该从哪里开始编写代码。

## 精确计算OT (Computing OT exactly)

### 何时可以计算OT？

第一部分将关于精确计算最优传输，这基本上包括存在闭式解的情况。然后我会描述更通用的求解器，用于处理离散测度（即当你拥有的是数据，而不仅仅是密度时）上的最优传输问题。我会展示这里存在一些问题，这将引出讲座的第三部分，关于正则化。

在处理最优传输时，首先应该注意到的是，与其他经典散度（如KL散度甚至L2距离）不同，定义最优传输无论测度的结构如何都非常有意义，或者说仍然有意义。我这里特别指的是Kantorovich问题，即具有线性规划形式的问题。因为你们记得，无论我们处理什么样的测度，我们总能构造出耦合。这些耦合可能是离散耦合、半离散耦合（即一个测度离散，另一个连续）或连续耦合。对于这三种设置中的每一种，Kantorovich公式至少在理论上是可行的。所以，问题更多在于如何计算它，但它确实是被明确定义的。比如，即使两个测度的支撑集不重叠，它也不会像KL散度那样发散。

对于这三种情况（离散-离散，离散-连续/连续-离散，连续-连续），都有研究进展和工作。我提供了一些参考文献的指针，请注意在PDF版本中，我提供的每个引用都应该是指向链接的。

-   在**离散到离散**的世界中，我们比较两个离散测度。这非常像我之前展示的士兵和兵营的例子。我们知道（我稍后会展示）有一类求解器可以精确计算最优传输。但这种计算成本很高，这些求解器被称为网络流求解器。
-   在**离散到连续**（或反之）的情况下，问题要复杂得多。正如你可以预料的，它要么只在非常低的维度下有效，此时你可以对连续测度进行良好的网格近似。在这方面有很多工作，特别是来自Guillaume Carlier和Quentin Mérigot等人的工作，他们提供了在2D或3D下计算球体或球与点集之间的半离散传输的求解器。这在传输的某些应用中很重要，比如一个叫做反射器计算的应用。但如果你处于更高维度，你可能可以预见到，我们将需要某种形式的随机优化，需要从连续分布中采样，而无法将其作为一个整体来使用。
-   从**连续到连续**分布的传输，这更多是PDE（偏微分方程）领域人士喜欢考虑的设定，也是你在大多数最优传输书籍中会看到的设定。这是需要涵盖的地方。处理这种情况的一种数值方法，也是在低维中非常著名的方案，叫做Benamou-Brenier方案，我不会详细介绍，但它本质上将传输视为一个动态问题，试图一步步地找到质量如何从一个密度最优地移动到另一个密度。

因此，在本次讲座中，我将主要涵盖离散到离散和涉及离散测度的情况，因为除了Benamou-Brenier形式外，大多数数值方法最终都会归结到这里。

### 易于计算的情况 (闭式解)

#### 一维测度 (Univariate Measures)

重要的是，存在一些最优传输问题是已解决的，我们知道它有闭式解，而且是一个简单的解。这实际上激发了很多后续工作。我会稍微解释一下是哪种后续工作，但结论如下：

如果你比较的是实直线上的分布，并且你的成本函数表现良好，即它是实直线上两点之差 $x-y$ 的凸函数 $C(x,y) = h(x-y)$ (其中 h 是凸函数)，例如 $C(x,y)=|x-y|^p, p\ge 1$。那么，如果你用 $F^{-1}_\mu$ 和 $F^{-1}_\nu$ 表示两个测度 $\mu$ 和 $\nu$ 的分位数函数（Quantile Function），则使用此成本的Wasserstein距离（或者说最优传输成本）等于将这个成本函数 $C$ 应用于两个测度的分位数函数之差，并在 $[0, 1]$ 上积分：

$W_C(\mu, \nu) = \int_0^1 C(F^{-1}_\mu(t), F^{-1}_\nu(t)) dt = \int_0^1 h(F^{-1}_\mu(t) - F^{-1}_\nu(t)) dt$

让我稍微说明一下。假设我这里有一个密度，这里有另一个密度，它们都支撑在实线上，比如说都在0到10之间取值。什么是累积分布函数（CDF）？在得到分位数函数之前，通常先从CDF开始。CDF基本上是这个密度的积分。所以对于这里的每个值x，它表示密度左侧的面积。这就是它的样子。显然，$\mu$ 的密度开始时很快增长，然后在这里开始趋于平缓，所以CDF迅速上升，然后比如说从5开始趋于平缓，缓慢达到其值1。而对于 $\nu$，情况相反，这里有三个峰值，但在前五个单位（0到5之间）几乎没有质量，所以它只累积了一点点质量。然后有三个峰值，使得CDF逐渐攀升到1。这里x轴在0到10之间，y轴在0到1之间。

如你所知，对应于这些测度的分位数函数仅仅是CDF的反函数。所谓反函数，我指的是关于直线 $y=x$ 对称的函数。所以，如果你之前的y轴在0到1之间，x轴在0到10之间，你自然会得到一个y轴在0到10之间，x轴在0到1之间的函数。

这里的主要见解是，当你想比较两个密度时，与其做一些看起来像KL散度或L2距离（直接比较密度函数本身）的事情，你首先进行转换：将密度转换为其分位数函数。这种分位数函数表示法的优势何在？它非常方便，因为它允许你以一种聪明的方式比较两个分布支撑集上的值。也就是说，我们将比较中位数与中位数，第一四分位数与第一四分位数，第三四分位数与第三四分位数，等等。一旦我们在所有可能的值（即[0, 1]区间）上对这两个分位数函数的差异进行积分，我们就精确地得到了与该成本函数C相关的Wasserstein或最优传输值。

所以，我会简单地查看 $\mu$ 和 $\nu$ 的分位数在每个 $t \in [0,1]$ 的差异 $F^{-1}_\mu(t) - F^{-1}_\nu(t)$，应用函数 $h$ 到这个差值上，然后积分。

你可以注意到的第一个简单的事情是，如果你取 $h$ 为恒等函数，即成本就是差值 $x-y$，那么你得到的就是这两个曲线（分位数函数或原始CDF）之间差异的面积总和。你是在分位数函数层面还是原始CDF层面计算这个总和是一样的，因为取恒等函数 $h$ 时，计算方式没有改变。因此，分位数之差的积分与CDF之差的积分相同，这也就是Kolmogorov-Smirnov距离。然而，如果成本函数更一般化，这种等价性就不再成立了。

另一个需要注意的重要事情是，Monge映射，就是那个我们苦苦追寻的、输入实数值并输出实数值的映射，实际上在这个公式中是以闭式形式给出的。它很简单：如果你想把 $\mu$ 映射到 $\nu$（或者反过来），映射 $T: \mathbb{R} \to \mathbb{R}$ 由 $T(x) = F^{-1}_\nu(F_\mu(x))$ 给出。它基本上做了以下事情：取测度支撑集中的一个值 $x$，计算该值对应的某种“键”（key），这个键在0到1之间，表示它在分位数中的位置。然后使用这个键来输出第二个分布对应位置的分位数。记得我最初玩的那个一维土壤移动问题吗？实际上它有闭式解。你只需要计算第一个测度的CDF $F_\mu$ 和第二个测度的分位数函数 $F^{-1}_\nu$，然后进行这种查找过程。

更一般地说，对于满足某种次模性（submodular）条件的函数 $C(x, y)$，即它们的交叉导数 $\partial^2 C / \partial x \partial y \le 0$，这个结论也成立（此时积分的是 $C(F^{-1}_\mu(t), F^{-1}_\nu(t))$）。所以这个结果相当通用。

#### 在机器学习中简单且有用 (Easy and useful in ML)

这之所以有用，是因为近年来人们考虑了一种叫做**切片Wasserstein距离 (Sliced Wasserstein Distance)** 的东西。这最初是由 G. Peyré, M. Cuturi 等人在一篇论文中提出的。有趣的是，我认为“sliced”这个关键词并没有真正出现在标题中，这更多是他们关于廉价计算重心（barycenter）工作的副产品。

他们提出的方法如下：如果你给我两个现在处于高维度的测度 $\mu$ 和 $\nu$，我将把它们随机投影。这里的 $\theta$ 是单位球面上的一个向量。我只是将测度随机投影到一条直线上，然后观察这些投影得到的值的分布。然后我只需计算这两个一维投影测度之间的标准Wasserstein距离（或最优传输成本）。最后，我将在多个可能的投影方向上对这个距离进行平均。这些方向实际上可以是随机选择的，或者更有趣的是，尝试优化这些方向。也有工作考虑的不是单条线，而是更高维的子空间，但那样计算会变得更复杂。

但总而言之，这种切片方法在机器学习中相当流行，因为它有很多好处：计算上稳定且容易实现，并且在整个应用中表现出良好的性能。所以，这是一维情况，第一个简单案例。

#### 高斯测度 (Gaussian Measures)

第二个简单案例是**高斯测度**。这是另一个令人兴奋的好案例，因为它有不少应用。如果你听说过生成对抗网络（GANs）中的 Fréchet Inception Distance (FID)，它基本上就使用了这个公式。

现在我将切换设定。现在空间 $\Omega$ 是 $\mathbb{R}^d$，可以是任何维度d。实际上，它也可以是无限维的希尔伯特空间，我接下来要说的所有内容对于希尔伯特空间中的高斯算子也适用。

为了简化，假设 $\Omega$ 是 $\mathbb{R}^d$，成本是**平方欧氏距离** $C(x,y) = \|x-y\|^2_2$。这是个限制条件。你有两个高斯测度 $\mu = N(m_\mu, \Sigma_\mu)$ 和 $\nu = N(m_\nu, \Sigma_\nu)$，每个都由其均值和协方差矩阵指定。我暂时不假设它们是可逆的，所以这些可能是退化的高斯分布。那么，这两个测度之间的Wasserstein-2距离 ($W_2$) 的平方非常简单：

$W_2^2(\mu, \nu) = \|m_\mu - m_\nu\|^2_2 + B^2(\Sigma_\mu, \Sigma_\nu)$

它首先只是两个均值之差的平方范数。你可以相信，它将位置上的差异简化为仅考虑均值的位置。再加上一个**仅依赖于协方差矩阵**的部分。这部分被称为**Bures度量 (Bures metric)** 的平方。

Bures度量的平方是这个对象：
$B^2(\Sigma_\mu, \Sigma_\nu) = \text{trace}(\Sigma_\mu) + \text{trace}(\Sigma_\nu) - 2 \cdot \text{trace}\left( (\Sigma_\mu^{1/2} \Sigma_\nu \Sigma_\mu^{1/2})^{1/2} \right)$

这里有协方差矩阵的迹（trace）以及矩阵平方根 $\Sigma^{1/2}$。有其他等价的方式来写这个量，如果你查阅论文，会发现可以用一些恒等式来变换它，但这是最常见的定义。

你可以注意到一点，这在你思考KL散度时可能不明显：当 $\Sigma_\mu$ 和 $\Sigma_\nu$ 趋于零时（即你有退化的、坍缩的高斯分布），首先这个Bures度量部分显然是零，它会是 $0 + 0 - 2 \times 0$。所以这部分会消失为零。你会得到，当你有两个无限薄的高斯分布（即狄拉克delta函数）时，它们的Wasserstein-2距离就是它们均值之间的距离。而如果你看KL散度，它在这种情况下会发散。

关于这一切还有一个非常好的结果是，这个量（指$W_2^2$公式）在经过一番繁琐的计算后，实际上来自于一个简单的观察和使用Brenier定理。首先，我将声称（并在幻灯片中尝试证明）以下映射是最优的。让我解析一下这个映射：它将 $\mathbb{R}^d$ 中的向量 $x$ 映射到目标测度的均值 $m_\nu$ 加上一个线性算子 $A$ 乘以 $(x - m_\mu)$。

$T(x) = m_\nu + A (x - m_\mu)$

这个线性算子 $A$ 是什么？它现在需要 $\mu$ 的可逆性，但还不需要 $\nu$ 的。我们需要输入测度（你开始的那个测度）具有完整的体积，即协方差矩阵是非退化的。算子 $A$ 是：

$A = \Sigma_\mu^{-1/2} (\Sigma_\mu^{1/2} \Sigma_\nu \Sigma_\mu^{1/2})^{1/2} \Sigma_\mu^{-1/2}$

为什么我能声称这个映射是最优的？首先你应该注意到，如果你有一个随机变量 $X$ 服从均值为 $m$、协方差为 $\Sigma$ 的正态高斯分布 $N(m, \Sigma)$，那么如果你对这个随机变量进行线性或仿射变换 $CX+B$，你得到的也是一个高斯分布。这是众所周知的。基本上，新的高斯分布的均值是 $Cm+B$，协方差是 $C \Sigma C^T$。

如果我有一个大白板，我会向你展示，这个映射 $T(x)$ 的构造方式就是这样：如果我取 $X \sim \mu = N(m_\mu, \Sigma_\mu)$，那么 $X$ 的这个仿射变换 $T(X)$ 将精确地服从第二个高斯分布 $\nu = N(m_\nu, \Sigma_\nu)$。你可以开始推导：如果 $X \sim N(m_\mu, \Sigma_\mu)$，那么 $T(X)$ 的均值将是 $m_\nu + A(E[X] - m_\mu) = m_\nu + A(m_\mu - m_\mu) = m_\nu$。现在重要的是看协方差会发生什么。根据 $Cov(CX+B) = C Cov(X) C^T$， $T(X)$ 的协方差是 $A \Sigma_\mu A^T$。如果你把我们定义的 $A$ 代入 $A \Sigma_\mu A^T$ 并进行化简（这需要一点矩阵代数），你会发现它恰好等于 $\Sigma_\nu$。我们正是这样精心设计了这个线性算子 $A$，使得最终得到的输出协方差恰好是 $\Sigma_\nu$。

现在，为什么我能声称这是一个最优映射？我能声称它是最优映射的原因如下：感谢Brenier定理。你记得我告诉过你们，Brenier定理告诉我们，如果我们有两个密度，并且你可以用一个**凸函数 $\phi$ 的梯度 $\nabla \phi$** 从一个密度到达另一个密度，那么这个传输（移动）必然是最优的，并且该映射 $T(x) = \nabla \phi(x)$ 是最优映射。

所以，既然我已经找到了一个传输方式 $T(x)$（一个从第一个测度到第二个测度的映射），我们唯一需要检查的是，这个东西 $T(x)$ 是否是某个凸函数的梯度。回忆一下它的定义，它只是一个仿射函数 $T(x) = m_\nu - A m_\mu + Ax$。这个仿射函数的线性部分是矩阵 $A$。这个矩阵 $A$ 通过其定义（涉及正定矩阵的乘积和平方根）是一个**对称正定 (Symmetric Positive Definite, SPD)** 矩阵。一个仿射映射 $T(x) = b + Ax$ 如果其线性部分 $A$ 是对称正定的，那么它就是凸函数 $\phi(x) = b^T x + \frac{1}{2} x^T A x$ 的梯度。

因此，我们可以断定这个映射 $T(x)$ 是最优的。

让我在这里用视觉辅助一下。这是真实数据，或者说是真实的测度。这是一个高斯分布，这些是它的等高线。这是另一个高斯分布及其等高线。这个映射 $T(x)$ 所做的就是，每当我取这里的一个点，我就把它移动到这里。这个点被移动了，这些是一些箭头，描述了Monge过程，即工人拿起这里的质量并把它放到那里。

为什么我声称这是最优映射？我们声称它是最优的，多亏了Brenier定理。我们发现了一个传输 $T(x)$，将第一个测度 $\mu$ 变换为第二个测度 $\nu$。我们检查了这个 $T(x)$ 是一个仿射函数 $T(x) = b + Ax$。这个矩阵 $A$ 被构造成对称正定的。因此，$T(x)$ 是凸函数 $\phi(x) = b^T x + \frac{1}{2} x^T A x$ 的梯度。根据Brenier定理，这个映射 $T(x)$ 是最优的。

所以这使得在高斯分布之间进行传输非常方便，因为Monge映射总是线性的（仿射的），并且相对于高斯参数是以闭式形式给出的。

#### 椭圆分布 (Elliptical Distributions)

让我说明一下，这当然不适用于高斯混合模型，但它确实扩展到一个有趣的分布族，称为**椭圆分布 (Elliptical Distributions)**。这基本上是任何具有椭圆等高线的分布族。首先，这包括椭球上的均匀分布。在这种情况下，同样的结果适用：存在一个线性的（仿射的）最优Monge映射。

这是一个小小的例证。这里我们有一个椭球，它具有一定的协方差结构。这里我们有另一个椭球，结构有点不同，位置也不同。这是一个退化的椭球，基本上只是一个点，周围是一个零体积的球。这是一个退化的椭球，因为它只在平面上取值。这是一个超级退化的椭球，因为它只在直线上取值。对于这里的所有情况，从这个（中心的）椭球到所有其他椭球的（Wasserstein-2）距离都是相同的。所以这里你看到的是一个3D渲染，展示了与这个原始对象等距的对象的集合。

我们可以将退化高斯分布与此设定进行比较。一个有趣的方向是，Monge映射本身可能是退化的。如果你想深入了解细节，这里的 $W_2^2$ 公式总是有效的，无论协方差矩阵是否可逆。但这里的最优映射 $T(x)$ 的闭式公式需要 $\Sigma_\mu$ 可逆。这触及了Monge问题和Kantorovich问题之间的界限：在某些情况下，你可以计算最优传输成本（Kantorovich值），但你无法找到一个最优的Monge映射。可能不存在Monge映射。更准确地说，会有一个Monge映射从这个椭球（源）到一个线段（目标），但没有Monge映射能从线段（源）到椭球（目标）。你不能通过Monge映射增加维度，你只能保持在当前维度或降低维度。

### 离散测度上的OT (经验测度)

#### 两个狄拉克测度之间 (Wasserstein Between Two Diracs)

现在，对于数据科学领域的人来说，另一个更相关的设定是当你开始处理**狄拉克质量 (Dirac masses)** 或**离散测度**时。这是你能想到的最简单的离散测度，就是两个狄拉克函数 $\delta_x$ 和 $\delta_y$。关于最优传输的Kantorovich公式的好处在于，这里你可以精确计算它，而且非常简单。如果你有两个狄拉克测度，只有一种耦合方式能将它们耦合起来，那就是位于 $(x, y)$ 处的狄拉克质量 $\delta_{(x,y)}$。你可以想象一下那个图景，你有两个测度，试图找到一个联合测度，其在一个方向上的积分得到第一个测度，在另一个方向上的积分得到第二个测度。对于 $\delta_x$ 和 $\delta_y$，这个联合测度就是 $\delta_{(x,y)}$。如果你要用这个耦合（即 $\delta_{(x,y)}$）来对成本函数 $C$ 进行积分，结果就是 $C(x, y)$。

总结这张幻灯片：如果你在流形或其他空间上有一个成本函数 $C$，当你使用基于此成本的Wasserstein距离时，你基本上是在将距离的概念从点提升到分布。好处在于，某种程度上Wasserstein距离是一致的：如果你不再将点视为流形上的观测值，而是将它们视为退化的概率分布（狄拉克测度），那么这两个概率分布之间的Wasserstein距离仍然与原始空间中点之间的距离（成本）相同。

#### 线性分配 (Linear Assignment)

当然，当你开始添加更多的狄拉克质量时，事情会变得更复杂一些。但这里有一个与经典计算机科学非常容易理解的联系：在这种特殊情况下，最优传输等价于一个**最优匹配 (Optimal Matching)** 问题。什么是最优匹配问题？它只是说，如果我有两个**大小相同**且**权重均匀**的点云，那么最优传输就是要找到一种配对方式。这就像兵营和前线的问题，假设每个兵营有一万名士兵，每个前线点需要一万名士兵。那么你只需将一个兵营分配给一个前线点。这可以看作是在所有可能的排列（permutations）空间中寻找最优解的问题，但通过使用线性规划并在所谓的Birkhoff多面体（置换矩阵的凸包）上求解，问题得以简化。

但这只是一个特例。

#### 两个经验测度上的OT (OT on Two Empirical Measures)

让我更关注一般情况。在大多数应用中，当你处理最优传输时，你会遇到这种情况：左边有一个点云，可能有权重。这里我有 $n$ 个点 $x_i$，权重为 $a_i$。右边有另一个点云 $y_j$，也有权重 $b_j$，但通常点数可能更多或更少。你有点云大小不同、权重不同是很常见的。然后你想解决这个最优传输问题。我们某种程度上回到了最初用Kantorovich解释的那个小例子。

我将在这里用形式化的方式更严谨地阐述。真正需要注意的是，正如Kantorovich问题所强调的，我们将关注两件事：
1.  问题的**几何结构 (Geometry)**：红点相对于蓝点是如何分布的？这基本上是在衡量在任何一对红点和蓝点之间移动质量的成本有多高。
2.  **质量/权重 (Masses/Weights)**：我们将把质量分开记录。

为了形式化这个问题，我们一方面会有一个**成本矩阵 (Cost Matrix)** $M$，大小为 $n \times m$。这只考虑点的位置，并计算距离或成本。这里我写了距离的p次方 $d(x_i, y_j)^p$ 以与Wasserstein记号保持一致，但实际上它可以是任何成本 $C(x_i, y_j)$。这是一个成对距离（或成本）矩阵。例如，做核方法的人对此很熟悉。

另一方面，我们将考虑**耦合空间 (Space of Couplings)**。这是一个离散的耦合集合。它是由大小为 $n \times m$ 的非负矩阵 $P$ 组成的集合，满足行和等于 $a$（即 $P \mathbf{1}_m = a$）且列和等于 $b$（即 $P^T \mathbf{1}_n = b$）。这是主要的数学对象。记得Kantorovich形式化有相同的设定。

现在，最优传输问题仅仅是在所有满足这些边际约束条件的矩阵 $P$ 中，寻找那个与几何成本矩阵 $M$ 的Frobenius内积 $\langle P, M \rangle = \sum_{i,j} P_{ij} M_{ij}$ 最小的矩阵。

如果你思考一下，这是一种非常非常经典的**线性规划 (Linear Program, LP)**。它是一个以所谓的**标准形式**给出的LP，因为你正在最小化一组变量的线性乘积，这些变量被约束为非负，并且只有等式约束。这是人们所说的标准型线性规划，当然只是众多LP中的一种。

#### 对偶Kantorovich问题 (Dual Kantorovich Problem)

与其他LP一样，它有一个**对偶形式 (Dual Formulation)**。你还记得我在连续世界中介绍了Kantorovich问题的对偶形式，它在离散设定下有一个非常简单的类比。在离散设定下，我们将不再积分 $\int \phi d\mu + \int \psi d\nu$，而是将向量与权重 $a$ 和 $b$ 相乘。对偶问题是：

最大化 $\alpha^T a + \beta^T b = \sum_i \alpha_i a_i + \sum_j \beta_j b_j$
约束条件：$\alpha_i + \beta_j \le M_{ij}$ （对于所有 $i, j$）

记得几张幻灯片前，我们有 $\phi(x) + \psi(y) \le C(x, y)$。这就是它的离散形式。

正如我之前提到的，对偶形式实际上可能非常有用。想象一下，我要在这个问题上运行一个最优传输求解器。这里有一些源（比如矿山或资源产地），这些是需要资源的目的地。我需要找出哪个源（这里的圆点）的资源应该送到哪个目的地（这里的方块）。这是最优传输方案 $P$ （用线表示）。我们知道这个点有一些质量，它将质量分散到这里。这个点会去那里。这个点主要去那里。这个点去这里。这是你在原始问题 (primal problem) 中会得到的解。

那么在对偶问题 (dual problem) 中你会得到什么解？对偶问题有点不同。它试图最大化那些 $\alpha$ 和 $\beta$（我称之为价格或势函数 potentials），同时满足约束条件。重要的是看到，我们将为输入测度的每个点寻找一个值 $\alpha_i$，并为输出测度的每个点寻找另一个值 $\beta_j$。

这里我为输入中的每个点（圆点）和输出中的每个方块绘制了这些值（用颜色表示）。你观察到的是，在输入源的这些点上，值似乎相当高。然后你看到目标点的值也根据某种规则分布，其中一些值似乎很低，而那里的值相当高。

这实质上告诉你什么？因为对偶问题的最大化值与原始问题的最小化成本相同，这里我们采用的是最小化成本的视角，而这里是最大化价格的视角（有一些约束）。你在这里看到的是，这些值（高 $\alpha_i$ 或 $\beta_j$）之所以更高，本质上是因为它们对最优问题、最优成本的贡献更大。总成本意味着，从规划者的角度来看，这是我想如何发送物品的方式。从预算（比如会计）的角度来看，这基本上反映了每个地点对移动质量的总成本的贡献有多大。

这告诉我，实际上那个点（高 $\beta_j$）有点搞砸了事情，这个点（高 $\beta_j$）也增加了成本，因为它相对于输入点来说运输成本非常高。而这些点（低 $\beta_j$）看起来做得还不错。同样的情况也适用于输入点：这些点（低 $\alpha_i$）的价格较低，这意味着运输到它们或从它们运输出去的成本不太高。而这个点（高 $\alpha_i$）实际上在搞砸事情。

所以，对偶问题基本上告诉你，在输入和目标测度的支撑集中，哪些点对Wasserstein距离的贡献最大。很明显，如果你想找到一种方法来减少这两个点集之间的距离，比如如果你能够移动一个工厂或一个目的地来降低距离，你应该关注对偶变量。你会说：“这家伙（成本贡献大的点）太贵了，也许我想让它离其他所有点更近一些。”

### 求解OT问题 (Solving the OT Problem)

现在让我说明一下，如果你看问题的几何结构，这是一个经典的LP。满足边际约束的矩阵 $P$ 的集合是一个**多面体 (polytope)**。它是一个有界的凸多面体。成本矩阵 $M$ 只是一个向量（如果我们将矩阵向量化）。所以你必须想象我们处于大小为 $n \times m$ 的矩阵空间中。在这种情况下，线性规划的目标只是在多面体的所有点中找到那个与成本向量 $M$ 内积最小的点。那当然会是这个顶点。

为什么我一直说计算最优传输具有挑战性？原因是这个问题看起来很简单，但它吸引了20世纪下半叶一些最杰出的头脑。因为当人们开始提出线性规划和网络单纯形法求解器时，这引发了大量研究，试图找到更好的算法。目前的理解是，存在一些算法，其复杂度大概是 $O(N^3 \log N)$ 或类似级别（如果 $n$ 和 $m$ 不同，可能是 $O(nm(n+m)\log(n+m))$ 或相关复杂度），可以解决这个问题。我们能够证明这一点，当然这只是复杂度上界，通常算法可能稍快一些。

但事实上，你可以看到，如果你必须实例化一个网络单纯形法求解器来解决我刚才描述的最优传输问题，那么如果你的点数比如说超过20,000个，你就需要等待相当长的时间。这在机器学习中当然是个问题，因为我们通常要比较的测度可能比20,000个点大得多。

但还有另一个问题，我认为有点更隐蔽，那就是下面这个。想象一下这个场景：你给定了一个测度 $\mu$ 和一个测度 $\nu$，它们有点 $x_i$ 和权重 $a_i$，以及点 $y_j$ 和权重 $b_j$。你运行你的网络单纯形法，得到最优传输方案 $P^*$ 和Wasserstein距离（记得这只是 $P^*$ 和 $M$ 的内积 $\langle P^*, M \rangle$）。如果你只做一次性的操作，那没问题，它会花些时间，但你可以做到。

然而，在机器学习中经常发生的情况是，我们感兴趣的是尝试**微分 (differentiate)** 那个量。也许我们有这个Wasserstein距离，我们想让它变小。所以我们会寻找输入的微小变化，看它们是否能转化为Wasserstein距离的减少。LP的一个问题是，它们本质上是**不可微 (non-differentiable)** 的。因为最优传输解 $P^*$ 不一定总是唯一的。

让我举例说明。想象我开始稍微改变我的 $x$（点的位置），给位置添加一些 $\delta$。这在实践中很少发生，但你很可能处于这样一种情况：你的成本矩阵 $M$（其方向）实际上与这个多面体的一个面（facet）正交。于是，突然之间，你从多面体的一个顶点（唯一最优解）变成了一个面上的所有点都是最优解。然后发生的是，如果你继续稍微移动一点点（用你的 $\delta$），你将切换到多面体的另一个顶点。

需要说明的是，成本向量 $M$ 恰好与一个面正交的情况在实践中非常罕见，这并不是你真正需要担心的。真正令人头疼的是，有时我们会从这个最优传输矩阵 $P^*$（通常有很多零的稀疏矩阵）突然切换到一个不同的最优矩阵 $P^{* \prime}$，它位于不同的顶点，仅仅因为我们稍微改变了 $x$ 的值。

换句话说，如果你在离散数据上实例化它，两个离散测度之间的Wasserstein距离是**不可微的**。它会有某种次梯度 (subgradient)，你可以在局部对其进行线性化，但会存在与之相关的问题。

### 离散OT问题 (Discrete OT Problem)

所以，第一点是计算量大。第二点是缺乏可微性。第三点，这确实是密切相关的，并且或多或少是在反复描述同一件事：输出最优传输方案的程序实际上将**非常难以并行化**。对于那些熟悉线性规划求解器的人来说，它本质上是一长串离散操作，通常是围绕某些变量进行旋转（pivoting），将变量放入活动集，从活动集中移除变量，并遵循一些旋转规则。这是最早实现之一，即Earth Mover's Distance (EMD) 的实现，它对计算机视觉产生了影响。这里我只是想表明，这实际上不是一个简单的算法，它有几百行代码，而且这可以说是轻量级的实现。如果你看看现在实际使用的实现，它们通常有几千行代码，以便利用一些加速技巧。

总结一下关于求解离散测度上的最优传输的三个主要问题：
1.  它**不具有可扩展性 (doesn't scale)**，至少在复杂度上是这样，实践中也能看到瓶颈。
2.  它关于输入（点位置、权重）是**不可微的 (not differentiable)**。
3.  它通常是你无法在GPU或TPU或其他并行硬件上运行的东西。你可能只能为每一对要比较的测度使用一个CPU核心进行朴素的并行化。

这就是为什么如果你坚持使用那种方式来解决最优传输问题，我认为我们不会取得太大进展。这也是为什么人们持续地对这个优化问题进行正则化或改变其条款。

（问答环节暂停）

## 为数据科学计算OT (Computing OT for data sciences)

我想从我称之为“现实检验 (reality check)”的部分开始这一节。因为到目前为止，你们可能已经听了大约两个小时关于Wasserstein的内容了。第一部分我提到它理论很棒，人们喜欢它，很多领域（菲尔兹奖得主等）都在研究。然后我开始深入探讨如何计算它，我认为你可以说那些结果有些令人失望。因为容易计算的情况基本上只有一维（好吧，你可以用切片，这是个好技巧）和高斯分布（但正如我们刚讨论的，这不适用于混合模型，所以有点像玩KL散度时遇到的同样限制）。然后我在计算方面展示了非常糟糕的结果，对吧？因为我告诉你们，基本上这是立方的，不能很好地并行化，而且不稳定、不光滑，所以你不能用它来微分。

所以，我在开头描绘的美好图景与我现在所说的所有这些之间有点脱节。那么，在我真正展示正则化如何有帮助之前，让我再对此持一种悲观的看法。

### 实践者关心的问题 (What matters for practitioners?)

如果你想让机器学习实践者相信Wasserstein距离是有用的，你可以提出各种各样的论点，但本质上它们会归结为两类论点：一是**计算方面 (computational)**，主要是性能（速度、可靠性）；二是所谓的**统计性质 (statistical properties)**。实践中的性能有点概括了这两者，即速度有多快，基本上有多可靠，以及（比如分类）性能是否好。你可以将这些分解为计算和统计两方面。

第一个（计算）方面本质上是：我计算两个离散测度（它们是从某个底层分布中采样得到的）之间的Wasserstein距离或其某种代理需要多长时间？

第二个（统计）方面是：好吧，我可能投入了一些精力来计算它，但问题是，它与我试图处理的实际真实量（比如两个真实密度之间的Wasserstein距离）有多接近？如果你试图评估 $\mu$ 和 $\nu$ 之间的距离，你唯一能接触到的是样本。你可以计算样本之间的Wasserstein距离，但完全没有保证样本间的这个距离与密度间的真实距离有任何关联。那么，也许这就不值得付出努力。你通常希望看到的结果是：关于样本，样本间距离与真实距离之差的期望值，是否可以作为样本大小 $N$, $M$ 的函数而减小？你希望看到这个差异随着 $N$ 和 $M$ 的增加而减少。

### 样本复杂度 (Sample Complexity)

在第一个（计算）方面，我已经告诉了你一些相当糟糕的事情，那就是基本上如果我想用线性规划的方式，使用样本来近似这个距离，成本非常高。第二个（统计）方面实际上我可以说更糟。因为实际上，这是已知的——这里有一长串结果，所以你可以想象结果通常会依赖于你对 $\mu$ 和 $\nu$ 所做的假设、对空间所做的假设、对成本所做的假设。所以有很多相关的细节，但可以说，主要的一个结论，即高维欧氏空间、平方欧氏成本的情况，是非常非常悲观的。

它表明，两个连续测度 $\mu$ 和 $\nu$ 的Wasserstein距离与它们的样本估计值（即基于 $N$ 个样本计算的 $W(\mu_N, \nu_N)$）之间的差异，在期望意义下，基本上是 $O(N^{-1/d})$ 的量级，其中 $d$ 是维度。这意味着在高维情况下，这个差异会极其缓慢地减小。如果你在10维空间，你需要将样本量提高10次方才能在这个估计中获得精度上的改进。

正如我说的，有一系列关于非参数估计的定理，人们试图获得更精确的结果。这基本上为什么会这样，也与量化误差 (quantization error) 问题有关。想象一下，你想用少量原子点来量化一个高维（比如10维）超立方体上的均匀测度。这就像是能够实际查看这些样本的位置。你会发现，即使在这种非常简单的设置下，当你实际上可以自由选择最好的离散近似（通过量化）来逼近你的测度时，量化误差在Wasserstein距离意义下仍然是这个量级 ($O(N^{-1/d})$)。所以这里有一个我们无法真正摆脱的障碍。

### 从理论到实践？(From theory to practice?)

所以，你是一个机器学习从业者，你喜欢Wasserstein，喜欢最优传输，这些想法很巧妙。你去找隔壁的同事，同事问：“计算它需要多长时间？统计性质如何？” 你告诉他们：“计算是立方的，统计性质在高维下很差。” 那么，争论就结束了，没有必要再谈论最优传输了，因为它没用。

当然，故事的结局要乐观一些。故事的结局是，这两个负面结果（高计算复杂度和差的样本复杂度）本质上都源于我们坚持使用了Kantorovich大约80年前定义的线性规划形式。这个线性规划形式对于连续密度效果很好，但在你实际使用样本进行计算时，它效果不佳。计算上效果不好，因为它太昂贵了。统计上效果不好，因为它基本上过度拟合 (overfitting) 了样本，它过于努力地寻找多面体上那个非常狭窄的、给出最优解的角落。

所以，最优传输之所以在机器学习中有效，每当你看到人们声称它有效时，很少是因为它坚持了原始的公式。几乎总是因为它以某种方式**正则化 (regularizes)** 了最优传输问题，它改变了问题定义的方式。

### 正则化OT的多种方式 (Many ways to regularize OT)

#### 对偶问题正则化 (Regularize dual OT)

如果你想一想，你可以改变很多东西。我想我昨天提到过，这终究只是一个优化问题，所以我们可以自由地玩转它的所有元素。

例如，如果你开始说，我想把对偶问题中的势函数 $\alpha, \beta$ 约束在一个再生核希尔伯特空间 (RKHS) 中。你可以看到这将提供一个更平滑的方法，一个更平滑的问题。然后你可以证明存在更好的统计性质，这与我稍后将描述的熵正则化有关。

你也可以说，好吧，实际上这个带有平方欧氏距离或边际成本的设定太复杂了。我将使用Kantorovich对偶性，它告诉我，在成本是距离（比如 $L_1$）的特定情况下，问题等同于在1-Lipschitz函数上进行优化。这实际上更简单，因为我只需要处理一个势函数 $\phi$ (因为 $\psi = \phi^c$ 可以由 $\phi$ 导出)。现在，想象我引入另一个技巧，我说，也许优化1-Lipschitz函数太复杂了，我将通过某种方式在“某种程度上”Lipschitz的函数上进行优化，例如，使用带有ReLU激活的深度网络。

你也可以想到改进版Wasserstein GAN (WGAN-GP) 是另一种正则化，我在其中添加了一个关于势函数 $\phi$ 的梯度范数偏离1的程度的惩罚项。所以你可以开始添加你能想到的各种正则化方式。

同样的事情也适用于这篇论文 [Cuturi & Doucet, 2014]，我们提出了另一种称为 Mover's Distance 的变体，它使用了小波分解。基本上我们说，1-Lipschitz函数 $\phi$ 具有某种小波分解，可以以某种方式进行约束。

当然，我想强调的是，每次我们改变一些东西，我们都在偏离Wasserstein距离的正统定义。我认为这没关系，我认为这是我们应该做的。因为在统计意义上使用原始的最优传输公式可能是错误的，它简直是一场灾难。即使我们因为拥有大量资源而能够应对立方的计算量，但样本复杂度呈指数级增长的事实使得它在某种程度上与应用无关。所以我们需要做点什么。

#### 原始问题正则化 (Regularize primal OT)

我们也可以正则化**原始问题 (primal problem)**。在原始问题中，一个重要的研究方向，甚至在理解这些问题之前，就是某种程度上限制成本矩阵 $M$ 的范围。昨天我们与参与者对此进行了一些讨论。也许一个明显的方法是对成本矩阵进行阈值化，这是多年前在计算机视觉论文中提出的。或者你可以开始强制要求成本具有非常特定的结构，例如图上的最短路径距离等。这简化了LP，使其更快，并且在统计上也表现更好。

你可以做的另一件事是改变测度输入到求解器的方式。当然，这是通用的 $\mu$ 和 $\nu$。但如果你给我的是样本，也许我可以先对它们进行**量化 (quantize)**。想象一下，你给我10000个点和另外10000个点，我在两组点上都运行k-means，设置k=100。这大大减小了问题的规模，并且其统计稳定性也已被研究。所以，这本身并非愚蠢之举。如果你唯一想做的就是计算Wasserstein距离，那么先运行k-means，然后计算k-means中心点之间的距离。

你也可以将测度视为高斯分布，我们已经提到了，这种情况下计算很容易。或者你可以将测度投影到随机直线上，这就是我介绍过的**切片 (sliced)** 的想法。或者你也可以投影到k维子空间。还有第三类方法是研究哪些是好的子空间，你应该先将测度投影到这些子空间上，然后再计算Wasserstein距离。

另一件事是，你可以在**耦合矩阵 P (coupling)** 上添加**先验 (priors)**。这就是我接下来几张幻灯片主要要讲的内容。你可以说，也许只取任何满足边际约束的耦合 P 太不受约束了。我需要对这个耦合多面体进行某种处理，我需要选择一个更平滑、更小的球来从中选择我的耦合 P。这可以通过添加任何形式的正则化来实现，比如添加平方欧氏范数正则化 $||P||^2_F$，或者对 P 的条目进行阈值化或设置上限等等。有很多方法可以做到。

### 熵正则化 (Entropic Regularization)

这让我想到了这个**熵正则化 (entropic regularization)** 方案，它已成为近期你能看到的许多应用的引擎。

其背后的方程式其实很简单。如果你为了到达多面体的那个角落而付出了如此多的汗水，那是因为我们基本上完全相信了所有数据 X, Y, a, b。之前我想你记得，我稍微改变了成本向量 $M$ 的方向，并展示了存在不稳定性。但实际上，同样的事情完全适用于权重 a, b。一旦你稍微改变权重，那个多面体就会开始摆动，然后突然之间它会撞到一个不同的最优解。

所以，你想要做的是，用那种精确性、最优性来换取一些更快、更稳定（关于输入）的东西。这只是教科书式的正则化。

我想获得的另一个好处（稍后我会谈到）是，通过平滑问题，得到某种**可微的 (differentiable)** 东西。你想要得到一个**唯一解 (unique solution)**，而不是那种可能是退化的、有无限多个解的情况。还需要一些在算法上更容易计算的东西，这样你就可以把它放到你的PyTorch、TensorFlow、JAX或任何其他自动微分库中。这是一个宏大的愿望清单。

事实证明，有一个简单的方法可以满足所有这些要求。还有其他的正则化方法，但出于某种原因，它们不一定能满足整个愿望清单。熵正则化在这方面相当出色。

#### Wilson '69 与动机

让我说一下，这个想法相当古老。从历史上看，它出现的原因很有趣。它出现基本上是因为当人们运行最优传输时——昨天在问题中提到了这一点，它最早是用于什么的？在最初几年，它首先被用于经济学，人们试图根据边际信息（比如人们住在哪里，他们在哪里工作）来预测交通模式。他们会观察：“这是人们住的地方，这是他们工作的地方。如果人们遵循最优传输原则，我应该预期这些点之间的交通流量会是怎样？” 但人们观察到的是，人们并**不**遵循最优传输，他们有点更混乱。所以，他们可能在某种程度上进行优化，但他们绝不可能精确地处于那个角落。你会看到交通流量有点模糊。

于是有人开始思考，好吧，如果我们无法仅用最优传输来预测发生的事情，让我们实际上在LP中嵌入一些**模糊性 (fuzziness)**，把它放在那里，然后希望我们模糊LP的结果会更像我们看到的模糊现实。

所有这些都总结在这一行里。这是我之前给出的线性规划：
最小化 $\langle P, M \rangle$
约束条件： $P \ge 0, P\mathbf{1} = a, P^T\mathbf{1} = b$

我们只是添加了这个： **$-\gamma H(P)$**。其中 $H(P)$ 是 $P$ 的熵。熵 $H(P) = -\sum_{i,j} P_{ij} \log P_{ij}$。记得 $P$ 是一个矩阵，行和为 $a$，列和为 $b$，这意味着它的总和为1（如果 $a, b$ 是概率向量）。所以它是一个概率分布（一个耦合）。你可以定义它的香农熵。熵在单纯形上是**强凹 (strongly concave)** 的。所以如果你加上 $-\gamma$ 乘以 $P$ 的熵（其中 $\gamma > 0$），你将得到一个 **$\gamma$-强凸 (gamma-strongly convex)** 的正则化项。

因此，只要 $\gamma = 0$，这就是一个线性规划。当 $\gamma$ 严格为正时，这变成了一个**强凸问题**。因为它是强凸问题，并且可行域（耦合多面体）是有界的，所以如果 $\gamma > 0$，我们就有**唯一的解**。

这就是它的样子。这是分布 $\mu$（比如一些点），这是分布 $\nu$（另一些点）。显然，为了计算目的，我已经在一个网格（比如 100x100）上将它们离散化了。这是**最优传输方案 P**（对应 $\gamma=0$）。你可以看到它非常非常**稀疏**，几乎不可能看清它。我可以在上面画一条线来表示。基本上你得到的是，当你运行LP时，你运行的是一个有 $100 \times 100 = 10000$ 个变量的LP。这是一个耦合矩阵。结果是，解的稀疏度非常大。这是线性规划的一个简单结果：如果你有10000个变量，但只有 $100 + 100 = 200$ 个约束（行和+列和），那么最优解最多将有200个非零变量。所以这里，或者更准确地说，你会找到一个最多有200个非零变量的解。

你在这里看到的是，我们基本上处于Monge问题和Kantorovich问题的边界。它仍然是一个耦合，但它几乎处处为零，非常细，看起来像一条脊线。它基本上是说，你在这里有的所有质量，你应该首先把它定位到这里；你在这里有的所有质量，如果在这里定位。几乎就像这是一个一一对应的关系，而不是一个真正的耦合。

当你引入**熵正则化**时，这种情况是不可能的，或者说是不好的。因为一个有很多零、只有少数正值的矩阵，其熵非常低。记得我们想要**高熵**，因为在最小化目标 $-\gamma H(P)$ 中有一个负号。所以我们想要高熵。当然，如果 $\gamma$ 非常小，关于这个矩阵是应该有高熵还是低成本的权衡将主要倾向于低成本。因此，你将开始看到原始曲线上出现一点**扩散 (diffusion)** 或**模糊 (blur)**。然而，这种模糊不是各向同性的，并非像你逐点模糊整条曲线那样。原因在于，我们仍然需要满足**边际约束**。如果我把这个点的所有东西都模糊掉，典型地，例如，我会开始产生不存在的质量，或者需要不存在的质量，或者产生不存在的质量。同样的事情也适用于这里：如果我用一个小的高斯模糊围绕它进行模糊，那会在这里产生太多质量。所以我们仍然需要模糊，但要以一种聪明的方式进行。这就是为什么你看到模糊只在可以出现的地方出现，而在不能模糊的地方（比如需要精确匹配边际的地方）就没有模糊。

随着 $\gamma$ 变得越来越大，你最终会看到模糊变得越来越明显，但仍然不模糊那些你不能模糊的区域。直到最后，你基本上是在寻找具有边际 $\mu$ 和 $\nu$ 的**最大熵分布**，而这恰好就是边际的**乘积 $P = ab^T$**。

所以我们有这样一种正则化路径，从Kantorovich（它本身如果你愿意的话，可以说已经有点偏离了Monge，因为Monge可能不存在或退化）开始，这里可能是Monge问题，可能有一些质量分裂，一些线可能有不同的值。然后逐渐地，我们走向基本上是边际的乘积。这对应于**能量距离 (Energy Distance)** 或**最大均值差异 (Maximum Mean Discrepancy, MMD)**，它们使用的耦合就是边际的乘积。这就像那个天真的将军的想法，他说：“每当我派兵时，我只需按比例分配所有东西。无论我在哪里有质量，我都按比例将其分配到第二个边际中我拥有的任何地方。”

### Sinkhorn算法 (Fast & Scalable Algorithm)

这听起来像是一个不错的解释，我们已经连接了许多点：Monge、Kantorovich、能量距离/MMD。但这在计算上是如何工作的呢？事实证明，这是令人惊喜的地方：这在计算上**极其有效**。

原因在于，如果你只写下这个问题的**一阶最优性条件 (first-order conditions)**，我把它们写在这里了。一阶条件只是目标函数（内积项加上按 $\gamma$ 缩放的负熵项）加上关于约束（行和等于 $a$，列和等于 $b$）的拉格朗日乘子项的梯度等于零。注意，我不需要添加 $P \ge 0$ 的约束，因为熵项中的 $\log P_{ij}$ 确保了这一点。$P_{ij}$ 不能小于零，它可以是零（因为 $x \log x \to 0$ 当 $x \to 0$ 时），但不能是负数。

我只取这个拉格朗日函数关于 $P_{ij}$ 的导数，并令其为零。你会发现，你需要一个解 $P_\gamma$（这个问题的最优解），它被定义为：一方面是一个取决于索引 $i$ 的因子 $u_i$，另一个取决于索引 $j$ 的因子 $v_j$，然后中间是某个取决于 $i, j$ 的、由问题给定的项 $K_{ij}$。这个 $K_{ij}$ 不是优化的。

$P_{\gamma, ij} = u_i K_{ij} v_j$

这里我用 $K$ 这个字母，并用彩色表示，是为了强调这个矩阵 $K$ 是通过对**成本矩阵 $M$ 逐元素取负数、除以 $\gamma$、再取指数**得到的：

$K_{ij} = \exp(-M_{ij} / \gamma)$

$K$ 被称为**吉布斯核 (Gibbs Kernel)**。这并非巧合它被称为K，它确实是一个核矩阵。如果成本函数 $M_{ij}$ 是负定的（比如 $M_{ij} = -k(x_i, y_j)$），或者如果成本是平方欧氏距离 $M_{ij} = \|x_i - y_j\|^2$，那么 $K$ 就是一个（高斯）核矩阵。

所以我们有这个核矩阵 $K$。一阶条件告诉你，必须存在两个**非负向量 $u$ 和 $v$**，使得 $P_\gamma$ 可以写成**对角矩阵形式**：

$P_\gamma = \text{diag}(u) K \text{diag}(v)$

这里 $\text{diag}(u)$ 是对角线上为 $u$ 中元素的对角矩阵。所以 $P_{\gamma, ij} = u_i K_{ij} v_j$。

我们唯一需要解决的事情是找到这样两个向量 $u$ 和 $v$，使得这个由一阶条件定义的矩阵 $P_\gamma$ 满足约束条件：即该矩阵具有正确的行和与列和。所谓行和正确，是指如果我用 $P_\gamma$ 乘以全1向量 $\mathbf{1}_m$，我得到 $a$。所谓列和正确，是指如果我用 $P_\gamma^T$ 乘以全1向量 $\mathbf{1}_n$，我得到 $b$。

$P_\gamma \mathbf{1}_m = a \implies (\text{diag}(u) K \text{diag}(v)) \mathbf{1}_m = a$
$P_\gamma^T \mathbf{1}_n = b \implies (\text{diag}(v) K^T \text{diag}(u)) \mathbf{1}_n = b$

让我稍微简化一下这两个恒等式。我们正在寻找满足这些条件的 $u$ 和 $v$。
$\text{diag}(v) \mathbf{1}_m$ 就是向量 $v$ 本身。所以第一个等式变成 $\text{diag}(u) (Kv) = a$。
$\text{diag}(u) \mathbf{1}_n$ 就是向量 $u$ 本身。所以第二个等式变成 $\text{diag}(v) (K^T u) = b$。

对角矩阵乘以一个向量就是这两个向量的**逐元素乘积 (element-wise product, Hadamard product $\odot$)**。所以我们归结为需要 $u$ 和 $v$ 满足这两个方程：

$u \odot (Kv) = a$
$v \odot (K^T u) = b$

也许用以下方式写更简单一些：
$u = a / (Kv)$  (除法是逐元素的)
$v = b / (K^T u)$ (除法是逐元素的)

这两者，我想强调的是，这里唯一的变量是 $u$ 和 $v$。 $a, b$ 和 $K$ 是由数据给定的。所以唯一需要找到的是满足这两个非线性方程组的 $u$ 和 $v$。

有一个非常简单的解决方法，并且被证明是收敛的，它被称为**Sinkhorn算法 (Sinkhorn's algorithm)** 或 Sinkhorn-Knopp 算法。它基本上就是重复你设定的迭代过程。比如，随机初始化 $v$（或设为全1向量），然后交替执行：
1.  $u \leftarrow a / (Kv)$
2.  $v \leftarrow b / (K^T u)$

如果你迭代这个过程，最终你会收敛到一个解 $(u^*, v^*)$，它精确地满足这两个方程。

不仅如此，收敛速度实际上也得到了很好的理解。Sinkhorn 首先证明了它以线性速率收敛。让我说明一下，这个算法实际上可以追溯到更早，在经济学中被称为 RAS 算法，在统计学中被称为 IPF（迭代比例拟合）过程。它有几个名字，但通常与 Sinkhorn 联系在一起，因为他首先证明了它是一个收敛的不动点迭代。

最近，由于 Sinkhorn 算法作为解决（正则化）最优传输问题的引擎引起了兴趣，文献中出现了一些更精细的关于该算法收敛性量化的结果。

### Sinkhorn算法的优势

有一点你应该注意到，而且非常重要，那就是我刚才提供给你的本质上是一种计算传输（或者至少是近似传输）的简单方法。请记住，我们想要解决的是给定 $\gamma$ 的熵正则化问题。我基本上是说解必须是 $\text{diag}(u) K \text{diag}(v)$ 的形式，并且我给了你一个算法来计算缺失的部分，即 $u$ 和 $v$。而你只需重复这个迭代过程就能得到 $u$ 和 $v$。

在每次迭代中，我们究竟在做什么？看一下迭代步骤：$Kv$ 是一个大小为 $n \times m$ 的矩阵 $K$ 乘以一个大小为 $m$ 的向量 $v$，得到一个大小为 $n$ 的向量。然后是逐元素除法。$K^T u$ 是一个大小为 $m \times n$ 的矩阵 $K^T$ 乘以一个大小为 $n$ 的向量 $u$，得到一个大小为 $m$ 的向量。然后是逐元素除法。

主要的计算开销在于**核矩阵乘法 (kernel multiplication)** $Kv$ 和 $K^T u$。这里的除法是线性时间复杂度的。所以每次迭代的复杂度主要是 $O(nm)$。每次你应用这个核（乘以 $K$ 或 $K^T$），你都需要付出 $O(nm)$ 的代价。

巨大的挑战是，我们能否将这个 $O(nm)$ 进一步降低？但也许值得注意的一点是，即使我们不能进一步降低它，这已经是一组现代硬件极其擅长执行的操作了。GPU 的全部目标就是快速进行**矩阵-矩阵乘法**（以及矩阵-向量乘法）。因此，通过这个迭代，我们现在可以用只需要调用矩阵乘法和逐元素除法的操作来近似最优传输。

从簿记的角度来看，这非常简单：只需从一个向量 $v$ 开始，然后运行这个迭代列表。

当然，无论何时你进行矩阵-向量乘法，你都应该问自己是否有更快的方法。在这方面已经有一些工作。在某些设定下，这个复杂度实际上可以降到 $O(n \log n)$ 或类似级别，特别是当点位于低维网格上，或者可以使用快速多极方法 (Fast Multipole Methods) 或分层矩阵 (Hierarchical matrices) 技术时。我会引用参考文献，几周前在ICML上发表了相关论文。

#### 并行性与矩阵运算 (Embarrassingly parallel & Matrix Operations)

我想给你一点这个算法如何工作的感觉。基本上，这是左边的数据 $a, b, K$（$K$ 由 $M$ 和 $\gamma$ 计算得到），这些我暂时不会动。算法所做的就是初始化一个 $v_0$（与 $b$ 大小相同），然后将核矩阵 $K$ 应用于它 ($K v_0$)，然后取 $a$ 与结果向量逐元素相除，得到 $u_1$。这是第一次迭代后我得到的向量 $u_1$。然后我可以继续这样：将 $K^T$ 应用于 $u_1$ ($K^T u_1$)，然后取 $b$ 除以这个向量，得到 $v_1$。我可以一遍又一遍地这样做。你可以看到，这部分 $(a, b, K)$ 不会改变，它没有移动，只是像这样应用这两个操作。

从编码的角度来看，这很简单，只需要两行代码。从实现的角度来看，更有趣的是（这实际上是真正激励我在2013年写那篇论文的原因），它不仅是**易于并行化 (embarrassingly parallel)**，而且实际上你可以使用**矩阵-矩阵运算**而不是矩阵-向量运算来加速它。

通常在机器学习中，我们很少只需要比较两个直方图。我们经常处理大量的直方图，并且我们想要同时计算许多对之间的距离。也许我想计算这个和这个、这个和那个之间的距离，等等。在这种情况下，你可以非常容易地将所有这些（比如计算 $k$ 对直方图之间的距离）转换为张量运算或矩阵-矩阵运算。你将不再初始化单个向量 $v$，而是初始化一个缩放矩阵 $V$（比如 $m \times k$），它将对应于你试图比较的所有 $k$ 对。然后你将以与我之前相同的方式进行操作（比如 $U \leftarrow A ./ (K V)$, $V \leftarrow B ./ (K^T U)$，其中 $A, B$ 现在是包含所有 $k$ 个 $a_i, b_i$ 的矩阵）。这使得它对于大规模的成对计算非常方便。

这是2013年原始论文中的一张图，显示了相对于LP方法显著的加速。这仍然是相关的。当然，你可以将曲线向两个方向移动，因为所有东西都会变得更快，但本质上，如果你引入正则化，你可以很容易地找到例子，其中使用Sinkhorn算法求解比使用LP求解器快数千倍。

#### MNIST 示例与数值稳定性

为了继续一些简单的例子，我会快速过一下这个。例如，如果你想在两个MNIST数字之间进行传输，会发生什么？在一个数字中，质量集中在这个区域。在另一个数字中，质量以不同的方式分布在同一个正方形区域。我将使用它们之间的平方欧氏距离作为成本 $M_{ij} = \|x_i - y_j\|^2$。

我们将在这里运行Sinkhorn。只需从设置一个缩放向量 $v_0$（与 $b$ 形状相同，比如带噪声的全1向量）开始。然后你做的是应用核 $K = \exp(-M/\gamma)$。这里的核，如果图像是 $20 \times 20$ 像素，这意味着有 400 个像素。核将是一个 $400 \times 400$ 的大矩阵。但事实证明，这也是为什么当你在网格上操作时，你可以将其转换为比二次方更快的运算的原因：这里的核 $K$（高斯核），如果你深入挖掘一下，它只是一个**高斯模糊 (Gaussian blur)**。所以在这种情况下，应用核 $K$ 就等同于对势函数进行模糊。

现在我要取 $a$（第一个数字的像素值），然后用它除以这个模糊后的 $v_0$ ($Kv_0$)。这是我的第一个势函数 $u_1$。我用 $a$ 中的值除以 $Kv_0$ 中的值。你应该开始注意到的一件事是，因为 $a$ 中的某些值是0，那么 $u_1$ 中也会有很多0。

接下来，对于第二个势函数 $v_1$，我要再次对 $u_1$ 应用高斯模糊 ($K^T u_1$，在网格上，高斯核是对称的，所以还是模糊)，然后取 $b$ 除以这个结果 ($b / (K^T u_1)$)。这开始变得有点棘手。因为你可以看到，在这个分母 $K^T u_1$ 中，在那些区域（$b$ 非零但 $K^T u_1$ 接近零的区域）你会看到一些非常接近零的值。而这里的分子是 $b$。所以你会看到一些值似乎开始发散。但这并非bug，实际上这就是Sinkhorn算法的工作方式。你看到 $10^8$ 这样的值开始出现是正常的。这里实际上是由这一点主导的：这是一个值为 0.015 的像素 $b_j$，它被一个非常接近零的值 $ (K^T u_1)_j$ 除，所以除以零的情况出现了。

如果你继续这样下去，Sinkhorn 似乎在这种设置下完全出错了。但这实际上是正常的。如果你在使用Sinkhorn时遇到问题，很可能就是源于此。

为什么这看起来像是bug但实际上不是？原因是，我们应该在**对数空间 (log space)** 中看待所有这一切。这就是为什么最近提出的许多用于稳定Sinkhorn的程序基本上都依赖于对数形式。如果你取这些缩放因子 $u, v$ 的对数（即 $\alpha = -\gamma \log u$, $\beta = -\gamma \log v$），它们本质上就是我之前介绍的对偶变量。你开始观察到的是，看起来像对偶变量的东西出现了。高值（在 $\log u, \log v$ 中绝对值大）出现在那些运输执行成本实际上很高的区域。很明显，如果你想执行这个数字和那个数字之间的传输，成本高昂的部分将是移动这里的质量，使其到达大约这里；也许还有这里的质量，使其到达那里。对于数字1来说，同样的事情也会发生。

所以你开始基本上看到（这些 $\log u, \log v$ 值）……只是给你一些提示，基本上Sinkhorn中出现的缩放因子 $u, v$，如果你取它们的对数（并乘以 $-\gamma$），你可以将其与对偶Kantorovich形式中的势函数联系起来。

这里我绘制了传输方案 $P_\gamma = \text{diag}(u) K \text{diag}(v)$，但它不太容易可视化。容易可视化的是，在每次迭代中，**边际违反 (marginal violation)**，即你计算的矩阵 $P_k = \text{diag}(u_k) K \text{diag}(v_{k-1})$ （或 $P_k = \text{diag}(u_k) K \text{diag}(v_k)$）在边际上越来越接近目标 $a, b$ 的事实。

### Sinkhorn 与 W 距离及 MMD 的关系

我快要结束讲座了，还剩几分钟。我想我可以跳过几张幻灯片，抱歉有点快，但我想涵盖一下这个材料。

我不会涵盖这个，我会涵盖这个，这个 MMD 和最优传输之间的关系，因为我之前稍微谈到过。

回顾一下：如果你给我两个带权重的测度 $\mu = \sum a_i \delta_{x_i}$ 和 $\nu = \sum b_j \delta_{y_j}$，经典的最优传输问题（Kantorovich形式）是在这个多面体（耦合空间）上，你有一个成本方向（由成本矩阵 $M$ 给出），然后你试图找到那个角落（最优解 $P^*$）。

然后我介绍了有一个新的解 $P_\gamma$，它是次优的（关于原始成本 $\langle P, M \rangle$），但关于熵 $H(P)$ 是最优的。它既有较低的成本，又有较高的熵。我提到的另一件事是，如果你只关心熵（即 $\gamma \to \infty$），你最终会收敛到边际的乘积 $P_\infty = ab^T$。

如果你沿着 $\gamma$ 移动，你就有这个正则化路径，它让你能够接触到一系列不同的最优传输方案。

现在让我们比较一下这三者能做什么。需要说明的是，我现在将熵包含在我所谓的平滑Wasserstein距离 $W_\gamma$ 的定义中：$W_\gamma(\mu, \nu) = \min_P \langle P, M \rangle - \gamma H(P)$。

一方面，你有这个**能量距离 (Energy Distance) / MMD**。对于核 $K = \exp(-M/\gamma)$，MMD平方可以写成 $\text{MMD}_K^2(\mu, \nu) = \langle ab^T, M \rangle - \frac{1}{2} \langle aa^T, M \rangle - \frac{1}{2} \langle bb^T, M \rangle$ (这里可能需要调整符号或使用 $K$ 而不是 $M$，取决于定义)。这对应于使用独立耦合 $P = ab^T$ 时的（正则化）成本。
另一方面，你有**精确的Wasserstein距离** $W_0(\mu, \nu) = \min_P \langle P, M \rangle$。
然后你有这个**熵正则化的Wasserstein距离** $W_\gamma(\mu, \nu)$。

事实证明，你可以使用这个中间的对象 $W_\gamma$（通过调整 $\gamma$）来解释能量距离/MMD ($\gamma \to \infty$) 和精确Wasserstein距离 ($\gamma \to 0$) 之间的关系。

有趣的是，如果你回顾几张幻灯片前我们的论点，在计算复杂度和统计复杂度方面：
-   **MMD** 很难被击败，因为它基本上只是实例化这个成本矩阵并求和（或使用核技巧），计算非常简单 $O(N^2)$ 或更快。从统计角度来看，它也非常有效（通常具有与维度无关或较好依赖性的样本复杂度）。它做的事情很简单，因此统计上表现良好，这不足为奇。
-   **精确OT ($W_0$)** 做的事情非常有用和复杂（捕捉几何结构），所以我们在计算上付出了代价（立方复杂度），并且从统计角度来看它过度拟合（指数级样本复杂度）。
-   **熵正则化OT ($W_\gamma$)**，Sinkhorn算法让你达到二次方计算复杂度 ($O(N^2 / \epsilon^k)$，其中 $\epsilon$ 是精度，k取决于 $\gamma$)。最近我们也能证明，使用Sinkhorn方法也可以获得良好的统计样本复杂度（通常介于MMD和精确OT之间，取决于 $\gamma$）。还证明了其他一些好的性质，比如它总是正的，在某些参数下是凸的，等等。

### Sinkhorn 的可微性 (Differentiability)

我想强调的最后一件事是**可微性 (differentiability)**。所谓可微性，我的意思是：好的，我们可以快速计算这个 $W_\gamma(\mu, \nu)$。我想问的问题是，如果你稍微改变测度的权重 $a, b$ 会发生什么？你能计算出某种线性近似（梯度）吗？如果我稍微改变位置 $x_i, y_j$ 会发生什么？

对此，我的简短回答是：有论文从凸分析的角度研究这个问题，并且有很多好的结果。但在当前的世界中，也许看待这个问题最简单的方式是作为一个程序员：你只需实例化一个程序，按以下方式运行Sinkhorn迭代：从朴素的初始化开始，使用你的数据 $(a, b, K)$ 运行Sinkhorn迭代得到 $(u, v)$，然后计算出 $W_\gamma$（或 $P_\gamma$）。然后，每当你想改变任何参数（可能是 $X, Y, a, b$，或者是成本函数 $M$ 或核 $K$ 中的参数，甚至是 $\gamma$）时，你只需通过所有这些步骤进行**反向传播 (backpropagate)**。

本质上，Sinkhorn算法只是一个堆叠的神经网络，其中所有的中间层都是矩阵-向量乘积或逐元素运算，并且所有层基本上共享参数（$K$ 是固定的）。

这将结束这次讲座。

## 问答环节 (Q&A)

**问 (主持人):** 好的，非常感谢Marco，这太棒了。让我们来回答一个问题，Vivek 想要问一个问题。

**问 (Vivek):** 我的问题是，您已经指出了最优传输与k-means聚类的一些联系，您也谈到了熵正则化的最优传输。这是否与模糊聚类 (fuzzy clustering) 有任何联系？以及，如果我们有限制，比如聚类中的稀疏性，像字典学习那样，这与最优传输有什么联系？

**答 (Marco Cuturi):** 是的，这是一个有趣的问题。最优传输与聚类之间的联系如下：当你进行最优传输时，你既需要知道你要传输到哪里（位置），也需要知道权重。如果我给你一个简单的类比：想象你有一张地图，上面有人，你想在地图上放置食品卡车。如果你对人的位置运行聚类算法（比如k-means）来找到应该放置食品卡车的位置，这可能会导致一种情况：一个食品卡车（簇中心）可能服务很多人，而一些可能被放置在异常值附近的食品卡车可能只服务两三个人。某种程度上，当你做聚类时，你并不真正关心每个簇的容量。如果这是你不关心的，那就像你在做传输，但你实际上在说：“我不关心目标测度的权重是什么，我只指定它的位置。” 而当我们开始将传输方程纳入考虑时，就好像你真的想要不仅控制位置，还要控制每个簇的容量。这就是为什么k-means和最优传输之间存在联系。某种程度上，最优传输提供了一些更通用的东西，帮助你处理那些你想要对簇施加容量限制的情况。这是第一个关于联系的问题。

关于熵，确实，软聚类 (soft clustering) 和正则化（熵）最优传输之间也存在联系。联系有点类似：当我们进行正则化最优传输时，我们会加入一些模糊性，但我们仍然会强制执行目标测度的权重。而在软聚类中，就像k-means一样，我们不关心（权重），通常你会根据点到所有簇中心的距离的指数负值（像softmax那样）来以软方式分配点。所以存在联系。

关于字典学习，如果你有稀疏性约束，比如只有少数人可以去那个特定的桶，或者只能去k个桶，诸如此类？在稀疏性方面，实际上你应该使用线性规划形式。那里的传输本身就嵌入了稀疏性。你会发现分配通常是非常稀疏的。好的，谢谢。

**问 (主持人):** 好的，目前似乎没有更多问题了。那么让我们再次感谢Marco。是的，电影院宣传一下大约30分钟后的圆桌会议，我希望在那里见到大家。谢谢大家，再见。

**答 (Marco Cuturi):** 谢谢Michael。