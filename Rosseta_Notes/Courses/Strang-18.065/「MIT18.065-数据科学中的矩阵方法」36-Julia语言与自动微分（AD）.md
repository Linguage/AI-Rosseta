

# 「MIT18.065-数据科学中的矩阵方法」36-Julia语言与自动微分

- 课程链接：[Lecture 36: Alan Edelman and Julia Language](https://www.youtube.com/watch?v=rZS2LGiurKY&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k&index=36)
- 官方频道：[MIT OCW](https://www.youtube.com/@mitocw)

### 内容介绍

本次讲座由 Julia 语言联合创始人之一、MIT 教授 Alan Edelman 主讲，内容源自他在 MIT 18.065 课程（数据分析、信号处理与机器学习中的矩阵方法）上的一次客座演讲。讲座的核心围绕着为高性能科学计算而设计的 Julia 语言展开，探讨其在现代计算，特别是机器学习领域中的独特优势和潜力。

Edelman 教授首先从一个线性代数的小例子切入，巧妙引出精确计算和抽象表达的重要性。随后，他引用了 Google 对机器学习适用语言的评估，强调了 Julia 在性能和易用性方面的突出地位，并对比了 Python 等其他流行语言。讲座的主要技术部分深入浅出地介绍了自动微分（Automatic Differentiation, AD）这一机器学习优化的核心技术。Edelman 教授分享了他个人对 AD 的理解过程，澄清了它既不同于传统的符号微分，也非数值有限差分，而是一种精确计算导数的独特方法。

通过经典的巴比伦平方根算法实例，讲座生动演示了前向模式自动微分如何利用 Julia 的类型系统和多重派发，仅需几行代码即可在计算函数值的同时精确获得其导数值，并展示了其底层的高效率。接着，讲座转向反向模式自动微分（即反向传播），以神经网络为例，从线性代数的视角揭示了反向传播的本质——求解一个三角线性系统，并指出 Julia 如何能利用其强大的线性代数库简化并加速这一过程。



### 内容纲要

```
讲座：Alan Edelman 谈 Julia 语言
├── 引言与背景
│   ├── 主讲人 Alan Edelman 介绍与课程渊源
│   ├── 提及 Gilbert Strang 的秩证明
│   │   └── 以零矩阵为例（$3 \times 0$ 空矩阵）引出概念严谨性
│   └── 提及学生已在实验中接触 Julia
├── Julia 语言与机器学习
│   ├── 引用 Google 博客观点
│   │   ├── 技术优点筛选 (淘汰 Python, Java 等)
│   │   └── 易用性筛选 (淘汰 C++, Rust)
│   │   └── 结论：Julia 与 Swift 最适合 ML
│   ├── 对比其他语言 (Python, Swift)
│   └── 提及语言选择的心理学
├── 自动微分 (AD) 概念引入
│   ├── Edelman 的个人理解历程
│   │   ├── 早期误解 (以为是符号计算或有限差分)
│   │   └── 关键认知：AD 是第三种不同的方法
│   │   └── SVD 雅可比矩阵计算的启发
│   └── 核心论点
│       ├── 语言在数学意义上的重要性
│       └── 线性代数的基础性地位
├── 前向模式自动微分 (Forward Mode AD)
│   ├── 示例：巴比伦平方根算法
│   │   ├── 算法介绍与 Julia 实现
│   │   └── 目标：不显式求导获得导数值
│   ├── “魔法”实现：对偶数 (Dual Numbers)
│   │   ├── 定义 D 类型 (值, 导数值) 对
│   │   ├── 利用 Julia 类型系统和多重派发重载运算符 (+, /)
│   │   └── 仅用 8 行代码实现核心机制
│   ├── 演示与效果
│   │   ├── 将原算法直接用于对偶数，同时得到值和导数
│   │   └── 无需重写求导代码，非符号计算，非有限差分
│   ├── 效率与对比
│   │   ├── 展示对偶数计算生成的精简汇编代码 (高效)
│   │   └── 对比 Python 符号计算 (低效、表达式膨胀)
│   └── 前向 AD 原理阐释
│       ├── 并非计算导数表达式，而是同步计算导数值
│       ├── 将导数规则应用于算法的每一步
│       ├── Julia 的优势：类型系统自动处理，无需源码转换
│       └── 补充：类型系统的力量 (以 SymTridiagonal 为例)
├── 反向模式自动微分 (Backward Mode AD / Backpropagation)
│   ├── 引入神经网络背景
│   ├── 简化模型：标量神经网络
│   │   ├── 网络结构：$x_{i+1} = h(W_i x_i + b_i)$
│   │   └── 目标：最小化损失函数 $L = \frac{1}{2} (y - x_N)^2$
│   ├── 微分推导
│   │   └── 推导 $dx_{i+1}$ 与 $dW_i, db_i, dx_i$ 的关系
│   ├── 核心：线性代数视角
│   │   ├── 将微分传播写成矩阵形式：$d\mathbf{x} = D d\mathbf{P} + L d\mathbf{x}$
│   │   └── 求解梯度归结为解线性方程：$(I-L)^T \mathbf{y} = \mathbf{g}$
│   ├── 关键洞见：反向传播即线性代数的回代过程
│   │   └── Backpropagation 本质是解三角线性系统
│   │   └── 无需手动实现反向传播循环，可利用标准库
│   └── Julia 的结合优势
│       └── 语言设计与线性代数紧密结合，利用高效库
├── 总结与问答
│   ├── 核心信息：线性代数是关键
│   ├── Julia 的价值：语言特性助力先进计算
│   └── 问答环节
│       ├── Q: 二阶及更高阶导数？ A: 可以通过技巧实现。
│       └── Q: 类型的泛用性？ A: 是核心思想，不仅限于 AD (以 Tridiagonal 为例)。
└── 结语 (Gilbert Strang)
    └── 课程总结与感谢
```


---

# 讲座：Alan Edelman 谈 Julia 语言

## 一、 引言与背景

大家好，我是 Alan Edelman。去年我曾协助教过这门课一点点，很高兴看到今年课程进展顺利。

Strang 教授来教 18.06，也就是入门线性代数课程，你们中有些人可能知道。Strang 教授来做了一个关于行秩等于列秩的精彩演示。我想知道你们在这门课上是否见过这个演示？或者他们会看到吗？（观众：在笔记里有。）在笔记里有。好吧，总之，就在 Strang 教授演示完之后——我拿一下这个，这是真的。我本来打算开始写代码来做那个四边形的例子，但我没有足够的时间。你可以看到我开始了，但这里是关于三角形的 0.5，那个比较简单。

那么，Julia 在这门课里的情况怎么样？你们用得多还是少，还是——（观众：在实验里用过。）在实验里你们用过 Julia。但那只是 MATLAB。（修正）哦，那是——好吧。

所以 Strang 教授展示了这个证明，他写下一个 $3 \times 3$ 的矩阵，秩为 2。他取了前两列，它们是线性无关的。然后很容易就证明了行秩等于列秩。Strang 教授出去后，我问，这个证明对零矩阵也适用吗？

这是零矩阵。因为我并没有真正告诉你们那个证明过程，我就直说了，如果我要用这个矩阵的线性无关列来构造一个新矩阵，我该怎么做？零矩阵可能看起来有点棘手，但其实不是那么难。但这是你们刚才走出门后我立马做的事情。

是的，我有一个 $3 \times 3$ 的零矩阵，我需要做的第一步，无论那个证明是什么，就是需要把这个矩阵的线性无关列取出来，放到它们自己的一个矩阵里。我会怎么做？

它会是一个空矩阵。这个空矩阵的维度是多少？不完全是 $0 \times 0$。因为每一列仍然是在 $R^3$ 空间里的，你明白吗？所以正确的答案——我希望这说得通——是一个 $3 \times 0$ 的空矩阵。这个概念在 MATLAB、Julia，我想 Python 里肯定也有，以及任何计算机语言里都存在。

所以如果你有一个满秩的 $3 \times 3$ 矩阵，它的线性无关列构成的矩阵就是 $3 \times 3$。如果秩是 2，那就是 $3 \times 2$。如果我有一个秩为 1 的矩阵，那就是 $3 \times 1$。所以如果我一列都没有，那么 $3 \times 0$ 就说得通了。

为了完成证明，再说一次，我并没有告诉同学们具体过程——显然它在笔记里——下一个矩阵会是 `rand(0, 3)`（一个 $0 \times 3$ 的随机矩阵）。当然，你把它们乘起来，就会得到一个 $3 \times 3$ 的零矩阵。所以很有趣，这个证明即使对于零矩阵也无需特殊处理就能成立。这就是今天发生的事。

## 二、 Julia 语言在高性能计算与机器学习中的地位

另外，让我说一两句关于 Julia 的最新进展？我开始准备一个演讲，还没完全准备好，但还是想和你们分享一下。

上周 Google 给 Julia 社区帮了个大忙。我是说，这事意义重大。你们都知道机器学习现在很火，这可能也是你们来上这门课的原因。我大概不用多说。然而，如果你们中有人希望这整门课是用 Python 或者别的什么语言，比如 MATLAB 授课，我一点也不会惊讶。我毫不怀疑你们中有些人可能希望如此。

我们经常被问到，你知道，为什么不用 Python。现在问 MATLAB 的不多了。但你知道，“为什么不用 Python”是经常出现的问题。我可以一直说到地老天荒，但没人会相信我。但是 Google 上周出来说，在机器学习方面，真正强大到足以进行机器学习——做你想做的那种机器学习任务——的语言其实只有两种。

在某种意义上，我今天要讲的剩下内容或许就是对此的例证。但如果你想看，可以去看看他们的这篇博客。他们基本上是从一大堆编程语言开始比赛。有 Python，有 R，Java，JavaScript。他们审视了所有这些语言。如果你读了这篇博客，你会看到。他们会根据技术优点来筛选它们。很快，很多语言就被淘汰了，包括 Python 和 Java。如果你去看博客，你会发现他们花了大量篇幅来讲述 Python 的故事。因为他们知道人们想听这个。我的意思是，人们需要被说服。所以实际上有好几屏的内容都在解释为什么 Python 对于机器学习来说就是不够好。

于是只剩下四种语言——Julia、Swift、C++ 和 Rust。然后如果你看博客的下一部分，他们根据易用性进行筛选。然后又有两种语言出局了。所以 C++ 和 Rust 消失了。接着他们继续说，他们认为只有这两种语言适合机器学习。他们还引用了一句很棒的话，说它们“共享许多共同的价值观”。他们还详细阐述了机器学习真正需要什么。我建议你们去看看。

当然，最后他们肯定会推广 Swift，他们也应该这样做。他们在某个地方写到——等等等等——关于更多人使用 Swift。也许是真的，我不知道。他们真正说的是，他们对 Swift 比对 Julia 更熟悉，这……你知道，如果是我发言，我会说我比 Swift 更熟悉 Julia。所以也许这很公平。

然后我开始画了一个关于编程语言心理学的小漫画，只是因为它是我一直遇到的事情。人们总是说所有语言都一样好，用什么并不重要。但事实是，如果你提到一种你还没在用的语言，你会自动屏蔽它，至少在 Google 发话之前是这样。所以情况就是这样。

好了，关于这个就说这么多吧。我刚把这些整合起来，在你们这儿试试效果。

## 三、 核心技术探讨：自动微分 (Automatic Differentiation, AD)

好的。现在让我再讲两件数学方面的事情。

首先我想和你们谈谈前向模式自动微分（Forward Mode Automatic Differentiation）。你们在这门课上接触过自动微分吗？（观众：很少。）好的，我觉得这个很有趣。希望你们会喜欢。我有一个关于前向模式自动微分的 Julia notebook。这个 notebook 的诞生是因为我花了很长时间试图理解这到底有什么大不了的。我遇到了一些困难。我是说，通常的故事是，逐行理解很容易，但“这有什么大不了的”的部分，有时候更难把握。我将要展示的第一个 notebook，可以说是我试图把握全局图景的成果。

第二件事——我想我直接在黑板上讲吧，它甚至还没完全准备好，但我还是打算先在你们这儿试试看——是向你们展示如何做一个反向模式自动微分（Backward Mode Automatic Differentiation）的具体例子，就是你们在神经网络里看到的那种。我猜你们在这里见过一些神经网络吧？（观众：是的。）所以我认为现在每个人都见过神经网络了。我想两年后，高中就会教了。再过三年，幼儿园可能都会教。我不知道。神经网络似乎是——它们并不难理解。

好的，那么让我开始吧。我真正想说服你们的两件事是——让我找一下——这是我的自动微分（auto diff）的东西。我真正想说服你们的两件事——也许你们已经相信其中一些了——第一，嗯，也许你们还不信这个——但是语言在数学意义上是重要的。正确的计算机语言能为你做的不仅仅是把黑板上的某个算法实现出来。它可以做得更多。这是我希望通过几个例子来说明的事情。

另一件事，我敢打赌你们现在都相信了，因为你们上过这门课，那就是线性代数是一切的基础。每门课都应该从线性代数开始。我是说，对我来说，感觉像是历史的一个不幸的意外，线性代数因为太多原因出现得太晚了。所以很多时候，本可以用线性代数做得更好的事情却没有这样做。我的意思是，对我来说，这感觉就像没有微积分就去学物理。我就是不理解。我知道高中有这样做。但这似乎是错误的。对我来说，所有的工程，所有的——都应该是线性代数。我是说，我就是相信——几乎所有。也许不是全部，但是相当多。比大多数人意识到的要多得多，我会这么说。

好的，让我从自动微分开始。我先讲个故事，以前我去参加会议，去数值分析的会议，听到人们谈论自动微分。老实说，我那时在看邮件，心不在焉。就觉得，谁在乎这个——我懂微积分。你可以教计算机去做。对我来说似乎挺容易的。我是说，我确定有一些技术细节。但教计算机微分似乎没那么有趣。我有点想当然地认为，这跟我上微积分课学的是一样的微积分。你知道，你背下那个表，教给计算机。你学习链式法则、乘法法则和除法法则。然后，砰，计算机做的就跟我用纸笔做的一样。所以，有什么大不了的。我没去关注。

而且，无论如何，我大脑后部有个小神经元在说，嘿，也许我错了。也许它是在做有限差分，你知道，就是那种你用 $\frac{dy}{dx}$ 的方式。用某种数值方法，你做有限差分。在数值分析里，他们应该告诉你如果 $h$ 太大，你会得到截断误差。如果你 $h$ 太小，你会得到舍入误差。而事实是从来没人告诉你什么才是好的 $h$。但你去上数值分析课，希望有人能告诉你。但不管怎样，我以为也许是那种数值有限差分。

我想对我来说最大的意外是，自动微分既不是第一种，也不是第二种东西，实际上存在第三种东西，某种不同的东西，既不是第一种也不是第二种。我发现这太迷人了。

也许我甚至可以告诉你们我是怎么突然意识到这个故事的。因为我当时真的没在注意。但我非常喜欢奇异值分解（SVD）。我很高兴看到人们在画抛物线和四分之一圆，并找出最小 SVD 值是多少。奇异值简直是——简直是上帝赐予人类的礼物。它就是一个非常好的分解。

我用 Julia 玩的一个东西就是计算 SVD 的雅可比矩阵（Jacobian matrix）。你知道，所有的矩阵分解都只是变量变换。所以如果你有一个 $n \times n$ 的方阵，SVD——我肯定你们知道这个——U 矩阵实际上有 $n \times (n-1)/2$ 个变量。V 也是。而 $\Sigma$ 有 $n$ 个变量。把它们加起来，你得到 $n^2$ 个。所以这只是一个变量变换。每次你做变量变换，你都可以构建那个大的 $n^2 \times n^2$ 的矩阵 $\frac{dy}{dx}$，计算它的行列式，得到一个答案。

我想知道——我其实知道那个理论上的答案。我想看到计算机确认那个理论答案。我和一些用非 Julia 语言编写自动微分的人交流过。他们的回答让我很惊讶。他们说，哦，是的。我们可以把答案教给我们的系统。我说，你什么意思你可以教答案？为什么它不能直接计算出答案？你为什么需要教答案？我觉得这完全错了。因为在 Julia 里，我们不需要教它。它会实际计算出来。然后我开始更多地理解自动微分在做什么，以及 Julia 在做什么。所以这个 notebook 就是这样诞生的。

## 四、 前向模式自动微分 (Forward Mode AD) 详解

我说太多了。让我从一个可能比较贴切的例子开始。

### 示例：巴比伦平方根算法

我要计算 $x$ 的平方根，一个非常简单的例子。你知道，平方根挺容易的。我要用一个人类已知的最古老的算法之一，巴比伦平方根算法。它说，从一个初始猜测值 $t$ 开始。也许它对于 $\sqrt{x}$ 来说有点太小了。计算 $x/t$。那就会太大。然后取平均值，然后重复。好的。这等价于用牛顿法求平方根。这个方法人类已经知道了数千年。所以它绝不是计算平方根的最新研究成果。但它非常有效。

这里有一小段 Julia 代码，可以实现它。它可能看起来像任何语言的代码。所以我要从 1 开始。字面上，我就是取 `(1 + 初始的x值) / 2`。然后我要重复。好吗？我们可以检查一下算法是否有效。这里 $\alpha = \pi$。所以我用巴比伦算法的结果和 Julia 内置的函数比较。你看它给出了正确的答案。这里是用 $\sqrt{2}$ 来测试。检查你的代码是否工作总是好的。

好的？我喜欢图形化地看东西，所以我对很多 $x$ 值运行了这个算法。我非常喜欢这样做。我有点希望在上一个演讲中——如果我动作够快的话，我想建一个小 GUI，可以在你们眼前移动那些点。也许你们在 MATLAB 里有。我打赌你们有。但我想自己建一个。但我没来得及。但这里就是这种东西。我喜欢看到收敛过程。所以你可以看到数字在收敛，底部的抛物线。当然，那个块就是平方根。所以，就是这样。这就是巴比伦算法。

### “魔法”实现：对偶数 (Dual Numbers)

我想得到平方根函数的导数。但游戏规则是，我不会用方法 1 或方法 2。我不会——你绝不会看到我输入 $1/2 x^{-1/2}$。对吧？你们都知道那是导数。我不会输入那个。我不会。它不会从 Julia 的任何地方来。好吗？第二件事是，我不会做有限差分。明白吗？我会得到那个平方根（的导数），但不是通过你们肯定会想到的那两种方法中的任何一种。对吧？

我是这么做的。我会用一点 Julia 代码。会有八行 Julia 代码。但我还不会完全说明它是如何工作的。我会让你们保持悬念大概五分钟。然后我会告诉你们它是如何工作的。好吗？

所以这里有八行 Julia 代码，可以帮我得到平方根（的导数）。在这三行里，我要创建一个 Julia 类型。我叫它 `D`，表示对偶数（Dual Number），这个名字至少可以追溯到一个世纪前，甚至更久。所以我要创建一个 `D` 类型。它就是一个浮点数的对。所以它是一个包含一对浮点数的元组。它将是某种数值上的函数值和导数值对。所以我八行代码中的三行是用来创建 `D`。在 Julia 语言里，这意味着使用 `Number` 的子类型，所以我们会像对待数字一样对待它。对吧？我们希望能够对这些有序对进行加、乘、除运算。但它只是一个数字对。别被 Julia 吓到。它只是一个函数值-导数值的数值对。好吗？

另外五行是什么呢？嗯，我想教它加法法则和除法法则。你们都记得加法法则。我猜那是简单的那个。除法法则——我高中老师的声音还在我耳边回响。“分母乘以分子导数，减去分子乘以分母导数……” 你们脑子里是不是也有这个口诀？我打赌有。“除以分母的平方”。我甚至无法把它从脑子里赶走。所以这就是除法法则。

那么这五行代码在做什么呢？首先，我想重载（overload）加号、除号以及其他一些东西。Julia 想让我确认一下，你确定吗？所以表示你确定的方式是，我要导入（import）`+` 和 `/`。因为玩弄加号是很危险的。所以这里我要对两个对偶数相加。我们会把函数值和导数值分别相加。两个对偶数相除。我们会用除法法则计算，函数值相除，然后分母乘以分子（的导数），等等等等，你懂的。好的。这是八行中的六行。

第七行是，如果我有一个对偶数，我想转换它。你知道实数是如何嵌入复数中的吗？我们需要告诉 Julia 如何把一个普通数字看作对偶数，方法是给它附加一个零作为导数部分。然后对偶数和普通数字就可以很好地一起工作了。而这实际上是说，如果我有一个对偶数和一个普通数字参与运算，把它们都提升（promote）成对偶数，这样它们就能一起工作了——所以总共八行代码。

所以我要告诉你们的第一件事是，提醒你们，我从未输入过 $1/2 x^{-1/2}$。你们同意吗？没有人——我没有导入任何包。它不是从——我没有从旁边偷偷把它弄进来。没有 $1/2 x^{-1/2}$。而且也肯定没有任何数值求导，对吧？可以说，这里的规则几乎感觉像是符号运算，除法法则和加法法则。但是这里完全没有数值有限差分。好的。

首先，让我在这里展示一下，我把巴比伦算法应用到一个对偶数上，而**没有重写代码**。之前我们把它应用于普通数字。但现在我要把它应用于我刚发明的这个对偶数。我要在 $(49, 1)$ 处应用它，因为我知道答案。然后我要和——我计算了 $1/(2\sqrt{x})$ 只是为了比较，而不是用在我自己的算法里。当然，你看我神奇地得到了正确的答案，而从未——所以你应该好奇，我是怎么做到的？我是怎么得到导数的？

我们可以用任何你喜欢的数字。这是 100。如果你更喜欢像 $\pi$ 这样的数字，我们也可以做。我的意思是，我们可以做任何你想做的。它都会工作。所以你看，这是 $\sqrt{\pi}$。而这在数值上应该是 $1/(2\sqrt{\pi})$。所以当你看到它在足够多的小数位上与这些数字匹配时，实际上是所有小数位都匹配。是的。所以这个东西神奇地工作了。

你们都应该好奇，那是怎么发生的？我没有重写任何代码。我实际上只写了一个计算平方根的代码。我从未写过计算平方根导数的代码。顺便说一句，这有点像我们在数值计算领域推广的 Julia 魔法。在这个世界上，人们经常会写一个代码来做某件事，然后如果你想做更多的事情，比如求导，就得有人再写另一个代码。用 Julia，很多时候，你实际上可以坚持使用原始代码。如果你只是正确地、智能地使用它，你可以做到神奇的事情而无需编写新代码。你稍后会再次看到这一点。

### 效率与对比

但这是导数——这是 $1/(2\sqrt{x})$ 的图像（黑色）。你也可以看到这里的收敛情况。好吧。我还是不打算现在就告诉你它为什么能工作。我保证再过几分钟就会。

但我首先要做的是，向你们展示一些大多数人永远不会看的东西。我从来不看。我想给你们看——这是同样的巴比伦代码。我想给你们看计算导数的汇编代码（assembler）。所以我要在一个对偶数上运行巴比伦算法。我们来看这里。我不知道这里有没有人读汇编。我打赌你们中只有零个或一个人真的读这东西。你们中有多少人读汇编？好的。不是 0 或 1。我们有半个。就在那儿，半个。他有点像这样（比划）。这里是 0。这里是 1。他像这样。好的。所以我认为 0 或 1 就像是记录。但我打赌你们会相信我，如果我告诉你们，当你看到像这样简短的汇编代码，并且它不是很长时，那么你就拥有了高效的代码。它非常紧凑。它会运行得非常快。所以无论这个东西在做什么，它都很短。这是你从其他任何语言都得不到的。如果你试图在 Python 中做同样的事情，我保证会有满屏满屏的东西，即使你能得到它。所以这是在对偶数上运行的巴比伦算法。这是它的汇编代码，而且很短。所以我想说的另一件事是，它不仅能工作，而且 Julia 还让它变得高效。

在我最终告诉你们到底发生了什么以及它为什么能工作之前，我要引入一个 Python 符号计算包，它能很好地与 Julia 配合。我要用 Python 符号计算来运行同样的代码，并向你们展示——这些是你们得到的迭代过程。所以你实际上看到了趋向平方根的迭代。这里是正在被计算的导数的迭代。这里的关键点是，当然，这是一个符号计算。我们没有做符号计算。这在数学上等价于如果我们去画图或者做些什么会得到的函数。但当然，符号计算效率非常低。我是说，你会得到这些巨大的系数。我是说，看看这个数字。这是多少？五百万还是什么？总之，你会得到这些大数字，这里还有更大的数字。看看这些巨大的数字，对吧？拖着这些 $x$ 需要大量的存储空间。这对内存是个巨大的拖累。我是说，这不是——这就是我们为什么要做数值计算。但是巴比伦算法，在没有舍入误差的情况下，等价于计算——在横线上面，它在这里计算平方根。然后在下面这里，这些是趋向导数的迭代。所以它实际上并没有计算 $1/2 x^{-1/2}$。它实际上在做一个迭代的事情，这个迭代过程近似于 $1/2 x^{-1/2}$。

### 前向 AD 原理

好吧。现在让我告诉你们。让我揭示一下到底发生了什么，这样我就可以向你们展示它是如何得到答案的。就像我说的，是 SVD 让我确信这是怎么发生的。因为 SVD 也是一个迭代算法，就像这个巴比伦平方根一样。但是用巴比伦平方根更容易向你们说明这一点。

所以我要做一件我绝不想做的事情，那就是显式地写一个求导版的巴比伦算法（`dBabylonian`）。我所做的是，对我的代码的每一行，关于 $x$ 求导。所以如果每一行——我总是分不清奇偶。但原始代码行是 `(1 + x) / 2`。现在我要对它求导。我会得到 $1/2$。这里我原来有这行代码 `(t + x/t) / 2`。如果我对它求导，我会用到除法法则，而这（指代码中对应的行）就是导数。

如果我运行这段 `dBabylonian` 代码，我实际上在做什么呢？我只是在使用普通的加、乘、除，没什么花哨的。看不到平方根。但我做的是，在我运行我的（原始）算法的同时，我也在运行——我实际上在同时计算导数。所以如果我有一个无限迭代的算法将收敛到平方根，那么这个导数算法将收敛到平方根的导数。但我只用了加、减、乘、除来实现这一点。

所以如果你重写任何代码，你可以有任何代码——迭代的、有限的，都没关系。如果你只是对代码的每一行关于你的变量求导，那么你就可以得到一个导数。就像我说的，它不是一个符号导数，比如，你知道，所有 18.01（微积分课程）或者我们现在教微积分的地方学的那种。它也不是一个数值导数，像数值课程，18.3xyz 什么的。它是一个不同的物种。它在每一步都使用除法法则和加法法则来得到答案。

这就是 `dBabylonian` 算法。你可以看到它运行。它给出了正确的答案。（哦，我必须先执行代码才能得到正确的答案。）但你看，它给出了正确的答案。（哦，我刚在伊斯坦布尔，他们挑战我做正弦函数。我忘了那个了。它还在我的 notebook 里。我在大家面前做了。成功了。我得到了余弦。）好的。但让我跳过所有这些。

现在让我回头告诉你们，这一切是怎么运作的。嗯，发生的事情是——让我们回到那八行代码，现在，也许，你们能看明白发生了什么。我最初的那八行代码在哪里？我得注意时间。我还想给你们看另一件事。所以希望我有足够的时间来做那个。但是，这里，让我想想。我的八行代码在哪儿？

找到了。这是那八行代码。所以我在做的是，与其让人类通过对每一行代码求导来重写你所有的代码，我说的是，为什么软件不能用某种自动的方式来做这件事呢？这就是自动微分（Automatic Differentiation）的用武之地。在很久很久以前，当人们——所有的数值代码都是用 Fortran 写的时代，会有那种源代码到源代码的转换器（source-to-source translators），它们会输入代码并输出导数代码。Julia 的方式，更现代的方式，是让即时（JIT）编译器为你做这件事。

所以，这里，我需要 `+` 和 `/`。当然，我还需要加上 `-` 和 `*`。但你只需要添加几个东西，然后砰，你就不需要重写 `dBabylonian` 了。因为原始的巴比伦算法，配合这个 `D` 类型，就能自动为你完成工作。好吗？这就是好的软件的魔力所在。所以你不需要写一个转换器。你不需要手写它。你只需要给出规则，然后让计算机去做。对吧？这就是计算机应该擅长的事情。所以这就是正在发生的事情。

### 类型系统的力量

（在回答观众提问时阐述）

**问：** 您刚才演示的这种（使用类型和重载的）方法，这种泛化的思想，仅仅是用于计算不同阶的导数，还是有其他应用实例？

**答：** 哦，这是世界上最大的技巧！它可不是什么小把戏。创建一种类型来做你想做的事情这个想法——我是说，你们在这门课上看到过克罗内克积（Kronecker products）吗？（观众：没有。）没有？（观众：[听不清]）好的。让我想想。你们在这门课上会看到什么？

你们看到过三对角矩阵（tridiagonal matrices）吗？你们最喜欢的那个？好的。那么这里。这是一个内置类型。假设 $n = 4$。哦，$n$ 不必是 4。我要创建一个 Strang 矩阵，如果我能拼对的话。它将是一个 `SymTridiagonal` 类型，这是一个 Julia 类型。我们会用 `2 * ones(n)` 创建对角线，用 `-1 * ones(n-1)` 创建次对角线。

这是一个类型。我是说，这是内置的。但你也可以同样轻松地自己创建它。我不喜欢叫它——它肯定不是一个密集（dense）矩阵。我也不喜欢叫它稀疏（sparse）矩阵。我更倾向于叫它结构化（structured）矩阵。虽然“稀疏”这个词在这里有点微妙。但我不喜欢叫它稀疏矩阵的原因是，我们没有以任何方式存储索引——我是说，有很多花哨的方案用来存储稀疏矩阵的索引。而我们只存储了一个对角线向量。这是对角线上的 2。这是一个包含四个 2 的 4 维向量。这里是一个用于次对角线的 3 维向量。而且，你知道，你不需要存两遍。大多数稀疏矩阵结构会把负一向量存两次，上三角和下三角部分。但实际上，只存储了所需的核心信息。

在某种程度上，人们在 Julia 中使用类型基本上就是为了——你只存储你需要的东西，不多也不少。然后你定义你的操作来工作。所以，举个例子，如果我要计算 `inv(strang) * rand(4)`。我要做一个线性求解。你会想用一个特殊的求解器（`\`，反斜杠运算符在 Julia 中通常表示线性求解），它知道这个矩阵是对称三对角矩阵。

所以，能够创建类型并根据你自己的目的使用它们，而没有任何浪费，这是一个宏大的故事。这是那种，虽然你可以在像 Python 这样的语言中做到，但在 MATLAB 中，如果你能看到汇编代码——MATLAB 永远不会让你看，Python 的话你会后悔去看——但你会看到这样做有多少开销。所以不会有性能提升。但从某种意义上说，这才是你想做的。你想用这些东西来匹配数学结构。所以这才是能够做到的好事情。

## 五、 反向模式自动微分 (Backward Mode AD / Backpropagation) 与神经网络

好的。这就是前向模式自动微分。我还有 10 分钟来讲反向模式。

（对之前内容的提问环节）

**问：** 同样的方法对二阶导数也适用吗？
**答：** 有一个技巧基本上能让你推广到更高阶，是的。你基本上可以把它变成两个一阶导数的组合。所以，是的，可以做到。

（承接上文关于类型系统力量的问答）

**问：** 您刚才演示的这种（使用类型和重载的）方法，这种泛化的思想，仅仅是用于计算不同阶的导数，还是有其他应用实例？
**答：** （回答见上文“类型系统的力量”部分，这里不再重复，但问答发生在这个时间点）

（继续正文）

但是，让我看看有没有——有人对此有任何问题吗？这真是魔法，对吧？但这是非常美妙的魔法。我不知道你们听说过多少关于机器学习的东西，但老实说，如今的机器学习，先不管人类是否会变得无用——顺便说一句，我不相信会这样。但机器学习的大事在于，它实际上只是一个巨大的优化问题。仅此而已，对吧？一个巨大的最小最大值问题，你们从微积分中学到过，你需要做的就是求导数。你知道，令它们等于零，对吧？在多元情况下，是梯度。你令它等于零。所以实际上所有这些机器学习，所有的大故事和一切，最终都归结为自动微分。它有点像是整个事情的主力。因此，如果我们能有一种语言，能以一种好的方式提供这个主力工具，那么机器学习真的会从中受益。所以我希望你们都看到了机器学习的大局。它确实归结为求导数。这就是终点——这就是你如何优化的方式。

有什么快速问题吗？否则我就要换话题了，我要移步到黑板前了。

好的，我只有五分钟了。我不知道我能不能完成。但让我看看我是否能在五分钟内给你们讲清楚反向模式微分的主要思想。但是，既然你们熟悉神经网络，让我看看我能不能非常快地做这件事。

### 简化模型

我要从标量（scalars）开始。好吗？我要做一个全是标量的神经网络。但这只是为了开始时简单起见，但我认为你们会看到这可以推广到向量和矩阵，那才是真正的神经网络。

所以我要做的是，我想象我们有输入。我们会有一堆标量权重和偏置。这里是 $W_1$，一直到 $W_n$ 和 $b_n$。好吗？所以我们这里有一堆权重和偏置。好的？我们也会有一个 $x_1$，它将作为我们神经网络的起点。

我们要计算——我用类似 Julia 或 MATLAB 的记号来写，对于 $i$ 从 1 到 $n$。我会通过取当前输入的某个函数来更新 $x$：
$x_{i+1} = h(W_i x_i + b_i)$
也许像这样。用什么函数 $h$？我不太在意。在过去，人们常谈论 sigmoid 函数。如今，经常用的是 $max(0, t)$。它有个荒谬的名字 ReLU，我实在受不了。但不管怎样，修正线性单元（Rectified Linear Unit）。但总之，它就是这样一个函数：当 $t \ge 0$ 时是 $t$，否则是 0。但无论你喜欢哪个函数。这里我只是更新。

好的。最终，你可能还会有一个数据 $y$。你会希望，如果一切都是标量的话，就像我说的，这可以很快推广。但我们可以做的是，我们可以最小化，比如说，$\frac{1}{2} (y - x_N)^2$。你会想要找到能最小化这个值的参数（权重和偏置）。所有这些都可以推广到矩阵和向量，这也是大多数神经网络所做的。

### 微分推导

由于我时间不多，也许我可以直奔主题。如果我对这里的关键一行求微分，我这里有一点 Julia（的影子）。但如果我对关键一行求微分，我会写什么？我会写——嗯，实际上，让我用通常的记号。

令 $\delta_i = h'(W_i x_i + b_i)$。好吗？这就是 $\delta_i$。然后你可以看到 $dx_{i+1}$ 是 $\delta_i$ 乘以——我会有 $dW_i x_i + W_i dx_i + db_i$。这将是微分。

$dx_{i+1} = \delta_i (x_i dW_i + W_i dx_i + db_i)$

这就是——我快完成了，这是个好消息。所以如果我做一个小小的改变——我喜欢把这想象成，比如 0.001 的变化。我不喜欢无穷小。我喜欢 0.001。我是这么想的。但你在这里做一个小改变，这里一个小改变，这里一个小改变。你会在这里得到一个改变。你会得到这个扰动的线性函数，它会给你这里的扰动。好吗？

### 线性代数视角

嗯，我只有一分钟了。所以我要用线性代数把所有这些写出来，因为用线性代数写出来一切都更好。所以我要写下——我要写下我实际上对最后一个元素感兴趣。但是 $dx_{N+1}$（表示最终输出的微分，这里用 N+1 似乎是为了与输入 $x_1...x_N$ 区分，但按原文的 $x_{i+1}$ 更新逻辑，最终是 $x_{N+1}$，虽然公式里写的是 $x_N$）将会等于，我这里会有几个矩阵。让我把结构弄对。这里会有 $dx_2, ..., dx_{N+1}$。（抱歉写得有点挤。）

但这里——事实上，我想用一点分块矩阵。这里我要有 $[dW_1, db_1]^T, ..., [dW_N, db_N]^T$。我要把偏置放在一起——抱歉写得乱。Julia 允许你创建分块矩阵。你实际上可以直接使用它们。那里会有一个特殊的类型。好的？

然后这里会是什么呢？你实际上可以看到它会是——我希望我做对了。但这里会有一个包含 $\delta_1 x_1$ 和 $\delta_1$ 的块，一直到 $\delta_N x_N$ 和 $\delta_N$ 的块。这会是一个某种意义上的对角（或块对角）矩阵（记作 $D$）。

好的？然后我这边有什么呢？这里我会有一个包含 $\delta_i W_i$ 的矩阵（记作 $L$，它会是下三角结构）。如果你检查一下，你会看到这将是——我不会把索引写对，而且我没时间了。所以我只能这样写了。

### 核心洞见：反向传播即线性代数

现在我只给你们故事的结局，因为我时间用完了。你可以把所有这些写成 $d\mathbf{x} = D d\mathbf{P} + L d\mathbf{x}$。
其中 $d\mathbf{x}$ 是所有 $x_i$ 微分的向量，$d\mathbf{P}$ 是所有参数 $(W_i, b_i)$ 微分的向量，$D$ 是一个（块）对角矩阵，$L$ 是一个严格（块）下三角矩阵。

所以如果你想解这个，线性代数就完成了传播。你得到 $(I-L) d\mathbf{x} = D d\mathbf{P}$，或者 $d\mathbf{x} = (I-L)^{-1} D d\mathbf{P}$。

如果我只想要最后一个元素——假设 $e_N$ 是取出最后一个元素（对应 $x_{N+1}$ 或最终输出 $x_N$ 的微分）的向量，那么我只需要计算 $e_N^T d\mathbf{x} = e_N^T (I-L)^{-1} D d\mathbf{P}$。这就是我得到所有导数（梯度，即损失函数对参数 $d\mathbf{P}$ 的导数）所需要的。

而这个故事的寓意是什么？抱歉我超时了一分钟。但寓意是，与通过你自己辛苦编写的代码进行反向传播（back propagating）相比，你可能知道当你求解一个下三角矩阵系统时（比如 $Ax=b$），人们会写代码来反向求解（back solves）这个下三角矩阵（实际上求解 $(I-L)^T \mathbf{y} = \mathbf{g}$ 是求解上三角系统，同样是回代）。那个“反向”，那个核心的“反向”部分，已经被为你实现了。如果线性代数已经有了那个“反向”（指高效的回代求解器），为什么还要重新发明轮子呢？你明白吗？

### Julia 的结合

所以如果你就这样做，并且你用一种能让你获得完全性能的语言来做，你就不需要自己做反向传播。因为一个简单的反斜杠（`\` 运算符，代表线性求解）就能为你完成。

## 总结

所以我为超时道歉。我不知道 Strang 教授是否还有最后的话要说。但总之，线性代数是一切的秘密。这就是最重要的信息。

（观众鼓掌）

## 结语 (Gilbert Strang)

好吧，既然这是我们 18.065 课程的最后两分钟，或者说负两分钟。我希望你们喜欢这门课。我当然很享受，正如你们可以看出来的。教这门课，看看它会如何发展，并写关于它的东西。所以我会让你们知道关于写作的进展。同时，我会收到你们关于项目的写作，我对此非常感激。当然，成绩会很好。我希望你们都乐在其中。所以谢谢大家。没错。谢谢。

（观众鼓掌）


---

# 要点回顾

**讲座框架与要点：Alan Edelman 谈 Julia 语言**

**一、 引言与背景**
- 主讲人 Alan Edelman 介绍自己与课程的联系。
- 提及 Gilbert Strang 关于矩阵行秩等于列秩的证明，并用零矩阵的例子（需要 $3 \times 0$ 的空矩阵）说明该证明的普适性，引出线性代数思维的重要性。
- 提及学生在实验中已接触过 Julia（但当时可能更像 MATLAB）。

**二、 Julia 语言在高性能计算与机器学习中的地位**
- **Google 的观点：**
    - 引用 Google 近期博客文章，强调在机器学习领域，性能和易用性是关键考量因素。
    - Google 的筛选过程：
        - 技术优点筛选：淘汰了 Python、Java 等语言（博客中详细解释了为何 Python 不够理想）。剩下 Julia、Swift、C++、Rust。
        - 易用性筛选：淘汰了 C++、Rust。剩下 Julia 和 Swift。
    - 结论：Google 认为 Julia 和 Swift 是最适合机器学习的两种语言，它们共享许多共同价值。
- **语言选择的心理学：** 人们倾向于忽略未使用过的语言，直到有权威（如 Google）推荐。

**三、 核心技术探讨：自动微分 (Automatic Differentiation, AD)**
- **Edelman 的个人理解历程：**
    - 最初对 AD 不以为然，认为只是简单的微积分规则编程或有限差分。
    - 误解1：以为 AD 等同于符号微分（像手算微积分，低效）。
    - 误解2：以为 AD 等同于有限差分（$dy/dx \approx (f(x+h)-f(x))/h$），存在截断误差和舍入误差问题，且难以选择最优 $h$。
    - 关键认知：AD 是**第三种方法**，既非符号微分也非有限差分，它在机器精度内计算精确导数。
    - 顿悟时刻：尝试计算 SVD（奇异值分解）的雅可比矩阵时，发现 Julia 可以直接计算，而其他语言的 AD 系统需要被“教”结果，从而理解了 AD 的真正机制。
- **核心论点：**
    - 语言在数学意义上很重要，好的语言不只是实现算法，更能拓展可能性。
    - 线性代数是许多领域的基础，应更广泛应用。

**四、 前向模式自动微分 (Forward Mode AD) 详解**
- **示例：巴比伦平方根算法**
    - 算法描述：一个古老的迭代算法 $t_{new} = (t_{old} + x/t_{old})/2$ 来计算 $\sqrt{x}$。
    - Julia 实现：展示了计算 $\sqrt{x}$ 的标准 Julia 代码。
- **“魔法”实现：对偶数 (Dual Numbers)**
    - 定义新的数据类型 `D`（对偶数），它是一个包含两个浮点数的元组：`D(函数值, 导数值)`。
    - 利用 Julia 的类型系统和多重派发，为 `D` 类型重载基本运算符（如 `+`、`/`），并实现对应的导数规则（加法法则、除法法则）。总共仅需约 8 行代码定义类型和规则。
    - **关键操作：** 将原始的巴比伦平方根算法 **直接** 应用于对偶数 `D(x, 1)`（表示输入值为 $x$，其导数为 $dx/dx = 1$）。
    - **结果：** 算法输出 `D(sqrt(x), 1/(2*sqrt(x)))`，即同时得到了函数值 $\sqrt{x}$ 和导数值 $1/(2\sqrt{x})$，**无需** 编写专门求导的代码，也**无需** 使用符号计算或有限差分。
- **效率与对比：**
    - 展示了对偶数版本巴比伦算法生成的**汇编代码**非常简短紧凑，表明其高效性。指出若在 Python 中尝试类似操作会产生大量开销。
    - 对比 Python 符号计算库：符号计算虽然能得到迭代过程的显式表达式，但会导致表达式膨胀（系数巨大），效率低下，不适合数值计算。
- **前向 AD 原理：**
    - AD 并非计算出导数的封闭形式（如 $1/(2\sqrt{x})$），而是将导数规则（链式法则、加法法则、乘法法则、除法法则）应用于算法的**每一步**计算。
    - 它随着原始值的计算，同步地、逐步地计算出导数值。
    - 对于迭代算法，值的迭代序列收敛到目标值，导数的迭代序列则收敛到目标值的导数。
    - Julia 的优势：通过类型系统和运算符重载，自动完成了这个过程，无需手动修改原算法代码或使用源码转换器。
- **类型系统的力量：**
    - 举例 `SymTridiagonal`（对称三对角矩阵）类型：只存储必要信息（对角线和次对角线向量），并定义针对该结构的优化操作（如线性求解），避免存储冗余和计算浪费。这是 Julia 高效利用类型实现数学结构的一个例子。

**五、 反向模式自动微分 (Backward Mode AD / Backpropagation) 与神经网络**
- **简化模型：** 使用标量（scalar）神经网络进行说明（易于推广到向量和矩阵）。
    - 网络结构：$x_{i+1} = h(w_i x_i + b_i)$，其中 $h$ 是激活函数（如 ReLU: $max(0, t)$），$w_i, b_i$ 是权重和偏置。
    - 目标：最小化损失函数，如 $L = \frac{1}{2} (y - x_N)^2$。
- **微分推导：**
    - 定义 $\delta_i = h'(w_i x_i + b_i)$。
    - 推导出微分关系：$dx_{i+1} = \delta_i (x_i dw_i + w_i dx_i + db_i)$。这表示输出的变化 $dx_{i+1}$ 与参数变化 $dw_i, db_i$ 和前一层输出变化 $dx_i$ 之间的线性关系。
- **线性代数视角：**
    - 将整个网络的微分传播过程写成矩阵形式：$d\mathbf{x} = D d\mathbf{P} + L d\mathbf{x}$。
        - $d\mathbf{x}$ 是各层输出 $x_i$ 的微分向量。
        - $d\mathbf{P}$ 是所有参数 $w_i, b_i$ 的微分向量。
        - $D$ 是一个对角矩阵，包含 $\delta_i x_i$ 和 $\delta_i$ 等项。
        - $L$ 是一个严格下三角矩阵，包含 $\delta_i w_i$ 等项，表示层间依赖。
    - 求解梯度：需要计算损失函数 $L$ 相对于所有参数 $\mathbf{P}$ 的导数。这最终归结为求解一个线性方程组。从 $d\mathbf{x} = (I-L)^{-1} D d\mathbf{P}$ 出发，计算 $dL/d\mathbf{P}$ 需要求解形如 $(I-L)^T \mathbf{y} = \mathbf{g}$ 的方程。
- **核心洞见 (反向传播即线性代数)：**
    - 求解 $(I-L)^T \mathbf{y} = \mathbf{g}$（其中 $(I-L)^T$ 是上三角矩阵）的过程就是**反向回代 (back-substitution)**。
    - **反向传播 (Backpropagation) 本质上就是求解一个三角线性系统的过程。**
    - **优势：** 高效的三角系统求解器是标准线性代数库（如 BLAS/LAPACK，Julia 底层调用）的核心功能。如果语言能将问题自然地表达为线性代数运算，就可以直接利用这些高度优化的库，而**无需手动编写反向传播的循环代码**。
- **Julia 的结合：** Julia 语言设计紧密结合了线性代数，允许用户利用类型系统和高效库函数，自然地实现此类计算。

**六、 总结**
- **核心信息：** 线性代数是理解和实现现代数据分析、机器学习中许多核心算法（如自动微分、反向传播）的关键。
- **Julia 的价值：** Julia 作为一个现代高性能语言，其类型系统、多重派发以及与线性代数的深度整合，使其能够高效、简洁地表达和执行这些先进的计算思想。