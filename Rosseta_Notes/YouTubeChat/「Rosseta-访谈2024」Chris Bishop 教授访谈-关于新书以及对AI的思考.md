
- 视频链接：[Prof. Chris Bishop's NEW Deep Learning Textbook!](https://www.youtube.com/watch?v=kuvFoXzTK3E)
- 博主：[链接](https://www.youtube.com/@MachineLearningStreetTalk)
- 时间：2024年4月10日

### 访谈介绍

以下内容是对著名人工智能学者 Chris Bishop 教授访谈的详细记录。Bishop 教授以其开创性的教科书《Pattern Recognition and Machine Learning》(PRML) 而闻名于机器学习领域。本次访谈的核心围绕他与其子 Hugh Bishop 合著的新书《Deep Learning: Foundations and Concepts》展开。

访谈不仅介绍了新书的创作背景、核心理念和内容重点，例如对基础概念的提炼以及对 Transformer、卷积网络等关键架构的深入探讨，更提供了一个宝贵的窗口，让我们得以了解 Bishop 教授从理论物理学转向机器学习的独特职业历程及其观点演变。

在对话中，Bishop 教授就当前 AI 与机器学习领域的若干核心议题分享了他的深刻见解，包括连接主义与符号主义的融合、贝叶斯方法的理论价值与实践挑战、大型语言模型的惊人能力与通用性之谜，以及对 AI 创造力等问题的哲学思考。

此外，访谈还重点介绍了 Bishop 教授目前领导的微软 AI for Science 项目，探讨了 AI 在加速科学发现（如药物研发、物理模拟）中的巨大潜力，并特别讨论了归纳偏置在科学数据稀缺背景下的重要性，这与广为流传的“惨痛教训”观点形成了有趣的对比。最后，对话也触及了深度学习有效性的未解之谜、模型安全与可解释性等前沿挑战。

### 内容纲要

```
Prof. Chris Bishop 关于其新深度学习教科书的访谈
├── 一、 Chris Bishop 教授介绍
│   ├── 身份与成就 (微软、高校、院士、AI 委员会)
│   ├── 学术背景 (物理学、理论物理博士)
│   └── 对 ML 领域的贡献 (PRML, Neural Networks for Pattern Recognition)
├── 二、 新书《Deep Learning: Foundations and Concepts》介绍
│   ├── 基本信息 (与子 Hugh 合著)
│   ├── 目标与内容特点 (全面理解、基础与前沿、提炼核心、强调基础、CNN/Transformer 章节)
│   ├── 编写背景与过程 (源于 PRML 更新、疫情期间启动、认识到需全新著作、注重提炼而非堆砌、避免追逐热点、ChatGPT 后加速完成、NeurIPS 发布)
│   ├── 物理特性 (缝合装订、可平摊)
│   ├── 可能的遗漏 (强化学习)
│   └── 个人偏好章节 (扩散模型、Transformer、生成框架整合、CNN)
├── 三、 Chris Bishop 的职业生涯与观点演变
│   ├── 早期影响 (《2001太空漫游》、对早期 AI 的看法)
│   ├── 研究转向 (理论物理 -> 核聚变 -> 受 Hinton 论文启发 -> 全职投入神经网络)
│   └── 职业生涯融合 (连接物理与 ML，投身 AI4Science)
├── 四、 AI/ML 关键概念探讨
│   ├── 连接主义 vs. 符号主义 (历史讨论、现代大模型能力、统一基底观点、“尚未”实现而非不能)
│   ├── 贝叶斯方法
│   │   ├── 理论基础与指导意义
│   │   ├── 实践局限 (计算成本高、$ \int p(\theta|D) p(y|x,\theta) d\theta $ 的困难)
│   │   ├── 实践权衡 (贝叶斯 vs. 更大模型规模)
│   │   ├── 实际应用倾向 (点估计 $ \theta_{MAP}/\theta_{MLE} $, SGD)
│   │   ├── 集成学习作为近似
│   │   └── 学习贝叶斯基础的重要性 (概率论基础)
│   ├── 大型模型（如 LLM）的特性
│   │   ├── 单一模型观点 (分布式、全息)
│   │   ├── 通用能力与“智能涌现” (多任务、推理能力)
│   │   ├── 通用模型优于专门模型 (代码生成、数学能力例子)
│   │   └── 与“没有免费午餐”定理的思考
│   ├── AI 的创造力
│   │   ├── 开放态度与类比人类学习
│   │   ├── 对双重标准的质疑 (“随机鹦鹉” vs. 学生)
│   │   ├── "人工智能的火花" (Sparks of AGI)
│   │   ├── AI 作为“认知放大器”/“副驾驶”
│   │   └── 对齐与限制对创造力的影响
│   └── 概率论的统一力量 (HMM 与卡尔曼滤波器的例子)
├── 五、 AI for Science (AI4Science)
│   ├── 动机与重要性 (科学发现的根本作用、AI 加速潜力、模拟器作为关键突破)
│   ├── AI4Science 计划 (微软团队、Bishop 领导、跨国跨学科)
│   ├── 归纳偏置 (Inductive Priors) 在科学中的作用
│   │   ├── 与“惨痛教训”观点的对比
│   │   ├── 科学领域成为例外的理由 (严谨先验知识如物理定律 $E(\text{Rot}(x)) = E(x)$, 数据稀缺)
│   │   └── 融入先验知识的重要性与挑战性
│   ├── 应用实例：药物发现 (Drug Discovery)
│   │   ├── 挑战 (巨大化学空间 $10^{60}$)
│   │   ├── AI 加速机制 (计算生成与筛选)
│   │   └── 具体案例：结核病药物 (分子语言模型、几何/等变性编码、VAE、迭代优化、合作验证、效果显著)
│   └── 应用实例：模拟器 (Emulator) - Tokamak 控制
│       ├── 问题与挑战 (实时控制需求 vs. 模拟器 $O(e^N)$ 慢速)
│       ├── 解决方案 (用慢模拟器生成数据训练快速 NN 模拟器)
│       └── 结果 (数量级加速、实现世界首次 NN 控制 Tokamak)
├── 六、 深度学习的开放性问题与挑战
│   ├── 深度学习为何有效 (基本理解 vs. 深层疑问、过参数化与泛化、Grokking、SGD 的隐式正则化、缺乏预测性理论)
│   ├── 模型的可解释性、偏见与安全 (不可测知性、对齐/护栏的必要性)
│   ├── Transformer 架构的未来 (主导地位但非终点、新架构可能)
│   ├── 控制与规划 (实体 AI/机器人学前沿、基于模型的方法、模拟)
│   └── 科学知识的基础性 (定律的近似性、科学无尽前沿、认知局限、数学的角色)
└── 七、 总结
    └── 访谈结束语 (激动人心的时代)
```



---

# Prof. Chris Bishop 关于其新深度学习教科书的访谈



## 一、 Chris Bishop 教授介绍

**问（Tim/主持人介绍）:** 今天，我们有幸邀请到人工智能和机器学习领域的杰出人物 Chris Bishop 教授。Chris 是微软研究院剑桥 AI for Science 部门的技术院士兼总监。他同时也是爱丁堡大学计算机科学名誉教授和剑桥大学达尔文学院的院士。

**答（Chris Bishop）:** 你好，Tim，很高兴见到你。

**问（Tim/主持人介绍）:** 他在 2004 年当选为英国皇家工程院院士，2007 年当选为爱丁堡皇家学会院士，2017 年当选为皇家学会院士。Chris 曾是英国 AI 委员会的创始成员，并于 2019 年被任命为首相科学技术委员会成员。在微软研究院，Chris 负责全球范围内的工业研究与开发组合，重点关注机器学习和自然科学。Chris 在牛津大学获得物理学学士学位，并在爱丁堡大学获得理论物理学博士学位，其论文研究方向是量子场论。Chris 对机器学习领域的贡献确实非凡。他撰写了该领域可以说开创性的教科书之一——《Pattern Recognition and Machine Learning》（简称 PRML），这本书已成为全球无数学生和研究人员的重要参考资料。

**答（Chris Bishop）:** （补充信息）PRML 是我的第二本教科书，在此之前我还写过一本备受赞誉的第一本教科书《Neural Networks for Pattern Recognition》。

**问（Tim/主持人）:** Chris 在访谈中解释了 PRML 如何在当时引导该领域走向更概率化的视角，他还提到了他的第一本教科书《Neural Networks for Pattern Recognition》及其在推广神经网络作为强大机器学习工具方面的作用。

## 二、 新书《Deep Learning: Foundations and Concepts》介绍

**答（Chris Bishop）:** 这就是我和我儿子 Hugh 合著的关于深度学习基础和概念的新书。

**问（Tim/主持人）:** 你带了什么道具？

**答（Chris Bishop）:** 乙醇。（笑）我不知道会不会用到它，但我们准备谈谈不变性，这很棒。

**问（Tim/主持人）:** 这太棒了，因为总得在某个时候深入一些技术细节，不是吗？我们的观众喜欢这个。

**答（Chris Bishop）:** 是的。这本书是《Deep Learning: Foundations and Concepts》。我们对这本书感到自豪的一点是它的制作质量。我们确实与出版商合作，确保这本书具有很高的物理品质。特别是，它采用了所谓的“缝合装订”（stitch signatures）方式制作。如果你看书脊边缘，你会发现书页不仅仅是胶粘进去的。相反，它使用了胶印技术，在大的纸张两面印刷 16 页内容，所以有些页面是倒置的，然后这张纸被折叠、再折叠、再折叠一次，然后裁切。由此产生的一叠被称为一个“书帖”（signature）。它们实际上是用线绳缝合起来的。这样做的意义在于它能让书平摊开来。这意味着这本书更容易阅读，也意味着它应该能使用很长时间。

**问（Tim/主持人）:** Chris，书中你最喜欢的插图是哪些？

**答（Chris Bishop）:** 当然是我儿子制作的那些是最好的。这里有一张关于 Transformer 架构的漂亮图片，这就是 GPT，所以你可以说这是书中最重要的一张图之一吧。我非常喜欢他绘制这张图的方式。

**问（Tim/主持人）:** 你是如何为这本书做研究的？

**答（Chris Bishop）:** 这是个好问题。我认为写这样一本书的一大挑战是知道该包含什么，不该包含什么。每个月都有成千上万的论文发表，这对作者来说已经够让人应接不暇了，更不用说读者了。所以我认为我们在书中增加的价值在于试图提炼出我们认为是核心概念的东西。这部分工作确实是审阅了该领域的关键论文，看看有哪些相对较新的想法，但也试图专注于我们相信能够经受住时间考验的技术和思想。我们不希望这本书在一两年内就过时了。我们希望它具有持久的价值。当然，很可能下周就会有突破，出现一个非常重要的新架构。但总的来说，许多核心概念实际上可以追溯到很久以前。所以我们真正做的是将领域的一些基础带入现代深度学习时代。比如概率的思想、基于梯度的方法等等。这些思想已经存在了几十年，它们在今天和以往任何时候一样适用。

是的，我真正喜欢的一点是关于卷积网络的章节。我儿子 Hugh 在这一章做了很多工作。他工作中就使用卷积神经网络等技术，作为他自动驾驶汽车工作的一部分。我认为这里对卷积网络有一个非常好的描述，真正从基础讲起，解释了基本概念，并且也给出了动机，不仅仅是说卷积网络是这样构建的，而是为什么要这样构建？我们是如何真正激发这种想法的？所以这也是我最喜欢的章节之一。

这本新书叫《Deep Learning: Foundations and Concepts》。PRML 写于 2006 年，它早于深度学习革命。令我一直惊讶的是，尽管从某种意义上说它已经严重过时，因为它没有提到机器学习领域最重要的事情（深度学习），但它仍然如此受欢迎。所以我早就觉得是时候更新这本书，出第二版，增加一些关于深度学习的内容了。但生活很忙碌，任何写过书的人都会告诉你，如果你没有亲身经历过，它所花费的精力远超你的想象。所以我一直没有真正着手去做。

然后是 COVID-19 大流行，我们都进入了封锁状态。我觉得自己是那次封锁中非常幸运的人之一。我们全家一起在剑桥被封锁。当你在家被封锁好几个月时，你有点需要一个项目。我想这可能是思考 PRML 第二版的好时机，因为在封锁期间你还能做什么呢？这后来成了我和我儿子的一个合作项目，因为那时他和我在一起。他已经获得了机器学习硕士学位，积累了很多经验，并且一直在从事自动驾驶汽车技术方面的工作。从某种意义上说，他在深度学习方面的实践经验比我当时要多得多。

所以我们开始把这作为一个联合项目。但我们很快意识到，所需要的不是在 PRML 上增加几章，而是整个领域已经发生了如此大的变化。而且，我们也不想写一本只是不断累积越来越多材料的书，那会变成一本巨大的、庞大的著作。我认为一本书的价值在于提炼，在于它将你的注意力吸引到特定的事物子集上。这是你真正需要理解的一小部分东西，然后你就具备了进入该领域的能力。所以我们省略了什么几乎和我们添加了什么一样重要。

我们很快意识到这是一本新书。所以我们把这本书命名为《Deep Learning: Foundations and Concepts》。我们取得了很大进展。但当然，封锁结束了。我在微软开始了名为 AI for Science 的新团队。Hugh 加入了 Wave Technologies，为他们的自动驾驶汽车构建核心机器学习技术。我们都太忙了。

接下来发生的事情是 ChatGPT 时刻，在几周内，一亿人开始使用它，突然间 AI 和机器学习进入了公众意识。我们意识到，如果要完成这本书，现在就是最佳时机。所以我们真的下定决心，努力完成了这本书，赶在 2023 年 NeurIPS 会议前出版。我们就像往常一样，在最后一刻赶上了。书在 NeurIPS 会议上展出，Hugh 和我整个星期都在一起逛会议，和海报展示者交流，度过了一段美好的时光。所以，能够和我儿子一起写这本书，确实是一份巨大的荣幸。

**问（Tim/主持人）:** 你最喜欢的章节是什么？有没有哪些你觉得是疏漏，本想加入但最终不得不划上句号的内容？

**答（Chris Bishop）:** 就最喜欢的章节而言，当然是那些更新的架构特别有趣。我非常喜欢写扩散模型（diffusion models）那一章，Hugh 对那一章也有很多贡献。当然还有 Transformers。以及思考如何整合不同的生成框架，如何看待 GANs、变分自编码器 (VAEs) 和归一化流 (normalizing flows) 等等，如何将它们置于一个统一的框架下思考，并以更连贯的方式呈现它们。对我来说，那部分是有趣的学习经历。我总是喜欢学习新事物，我在写那本书的过程中学到了东西，我想 Hugh 也是。所以从某种意义上说，那是我最喜欢的部分。是我学到了新东西或看待已知事物的新方式的部分。

真正的决策过程是决定放什么进去，不放什么进去，同时控制书的篇幅。因为我认为现在机器学习领域每个月大概有数千篇论文发表。这对初学者来说是压倒性的。所以这本书的目标确实是提炼出那些少数的核心概念，这意味着总会有一些事情，哦，我们是不是该加上这个？我们是不是该加上那个？我们想做的是避免加入那些最新的、可能目前非常热门但三个月后就可能消失的架构。我希望我们抵制住了那种诱惑。

但有些领域，也许当我们在某个时候，如果我们着手写第二版时，可能会考虑加入。强化学习是一个日益重要的领域，如果能有一章关于强化学习，并且与书的其他部分很好地整合起来，那将是很有价值的。现在有关于强化学习的书籍，有综述文章，有很多地方可以去学习它们，但一些与本书整合的东西，我认为可能会有价值。所以这是我们未来可能会考虑的事情。但目前，我们只专注于我们认为是任何领域新手——无论是硕士生、自学者，还是进入该领域想要理解基础知识的实践者——都应该掌握的核心原则。所以目标是努力让书尽可能地简短，但又不能更短了。

## 三、 Chris Bishop 的职业生涯与观点演变

**答（Chris Bishop）:** 是的，这是一段非常有趣的职业生涯。在职业生涯的这个阶段，我现在终于可以回过头来看，并理解它的意义。但在当时，感觉有点像随机游走。实际上，当我还是个青少年的时候，我去看了《2001太空漫游》。我深受那个相当抽象的人工智能概念的启发，它与好莱坞通常描绘的机器人非常不同。所以我从小就对人工智能这个想法很感兴趣。但我对当时的人工智能领域非常不感冒，那时它很大程度上是基于规则的，似乎并没有走在通往智能的道路上。

然后我在爱丁堡大学攻读了量子场论的博士学位，那在当时是一个非常热门的领域，规范场论。我在那里度过了一段美好的时光。但在博士生涯结束时，我想做一些更实际、更有用的事情。于是我进入了核聚变项目。我是核聚变的坚定支持者。那时它大概还有 30 年才能实现，现在某种程度上仍然还有 30 年，但我仍然坚信不疑。我去研究托卡马克物理，基本上是等离子体的理论物理，试图理解其不稳定性并加以控制。所以我当时作为一名理论物理学家工作得很开心，度过了愉快的时光。

在大约做了十年理论物理学家之后，Geoff Hinton 发表了反向传播（backprop）的论文，引起了我的注意。我发现那非常鼓舞人心，因为我在那里看到了一个通往智能的非常不同的方法。所以我开始将神经网络应用于核聚变项目的数据，因为在当时那算是大数据。我在靠近 JET 托卡马克的地方工作，他们有许多高分辨率的诊断设备。所以我有很多数据可以玩。

我对神经网络越来越着迷。然后我做了一件完全疯狂的事情：我放弃了一个非常受人尊敬的理论物理学家的职业生涯，全职投入到神经网络领域，而当时这并不是一个真正受人尊敬的领域。我会说它不是主流计算机科学，当然也不是物理学，它什么都不是。但我就是觉得它非常鼓舞人心。我尤其受到 Geoff Hinton 工作的启发。所以我进入这个领域已经，你知道，三十五年了。当然，近期的历史表明这可能是一个不错的职业选择。

现在，最近，我把我职业生涯的两端结合起来了，因为我现在对神经网络和机器学习对自然科学，包括物理学，产生的影响感到非常兴奋。

## 四、 AI/ML 关键概念探讨

**问（Tim/主持人）:** Hinton 是著名的连接主义者，他相信知识是次符号的 (subsymbolic)。我前几周和 Nick Chaytor 聊过，他有本书叫《The Mind Is Flat》，讨论的是我们大脑的不可测知性。你觉得情况发生了怎样的变化？你刚才谈到 AI 中这些不同思想的融合。

**答（Chris Bishop）:** 我认为一件非常有趣的事情是，从 2012 年左右开始，当深度学习显然非常成功时，出现了很多讨论，认为它缺少了那种符号化的方法，我们必须找到一种方法来结合这种连接主义方法——用那些可能已经过时的术语来说——也就是那种神经网络方法，与更传统的符号方法结合起来。我认为我们通过像 GPT-4 这样的模型看到的是，它完全有能力在更符号化的层面上进行推理，当然不是在人类的水平上，但它可以进行那种更抽象、更高层次的推理。

所以我认为我们正在神经网络上看到的情况，有点像人脑。人脑并没有一个连接主义的神经网络部分，然后是其他一些进行符号推理的机制；而是同一个基底（substrate）能够进行所有这些不同种类的推理和不同种类的智能。我们现在开始看到这种情况在神经网络中出现。所以，对我来说，关于我们是否应该以某种方式将符号推理与连接主义结合起来的讨论，那对我来说已成为历史。现在的问题是，我们如何扩展神经网络的能力。

**问（Tim/主持人）:** 这太有趣了。我记得 Pylyshyn 有一篇论文，好像是 1988 年的《连接主义批判》(The Connectionist Critique)，我当时很认同系统性 (systematicity) 和生产力 (productivity) 等观点。即使现在，那个学派的人仍然认为我们的大脑是图灵机，具有处理潜在无限性的能力。我猜我从你所说的理解到的是，这种区别实际上已经不存在了。你可以用神经网络来进行那种推理。

**答（Chris Bishop）:** 嗯，我的看法很简单，那就是神经网络，特别是自 2012 年以来，已经被证明具有惊人的能力，而且看不到尽头。现在的进步速度比以往任何时候都快。所以，认为机器学习和深度学习突然就在今天这个时间点结束了，这似乎很奇怪。这只是 S 曲线的开始。所以，认为我们应该如此担心神经网络的局限性以及它们做不到什么，我认为我们只需在后面加上“尚未”（yet）这个词。神经网络还不能做 X、Y 和 Z，尚未。但我认为没有任何迹象表明我们已经达到了神经网络能力的极限。而且它是迄今为止最成功、或者说发展最快的技术。所以对我来说，你应该在灯柱下找钥匙。我们拥有这项强大的技术，并且它每周都在变得更好。我们为什么不去看看能把它推向多远，而不是担心它的局限性呢？

**问（Tim/主持人）:** 绝对是。Bishop 教授，你因你的书 PRML 而极其著名。但当然，正如你刚才提到的，那不是你的第一本书。但是，你能告诉我们你写那本书（PRML）的动机和思考过程吗？

**答（Chris Bishop）:** 是的。正如你所说，它不是我的第一本书。我的第一本书出版于 1995 年，名为《Neural Networks for Pattern Recognition》。那本书有一个非常具体的动机，那就是我当时是这个领域的新人。我之前提到过，我对反向传播感到兴奋，并从理论物理学转向了机器学习。写书是我学习这个领域的方式。你知道，如果你是一位大学教授，学习某样东西的一个好方法就是去教一门关于它的课程，因为它迫使你非常仔细地思考它。你会收到聪明学生的刁钻问题，你会非常有动力去真正理解它。对我来说，写书就类似于此。

PRML 则相当不同。到 2006 年出版的时候，这个领域已经变得更大，从某种意义上说，它更加成熟，成为一个更加成熟和受人尊敬的领域。有很多关于机器学习的课程。那时的目标非常不同。我只是想写一本，可以说，每个人都会用来学习这个领域的书。所以它试图做到全面，但又试图尽可能清晰地解释概念。所以这确实是目标。目标是，在某种意义上，取代早期的《Neural Networks for Pattern Recognition》这本书，我认为它在当时扮演了重要的角色，但真正要做的是提供一个单一、连贯的文本，让人们可以用统一的符号来学习不同的主题，并希望我能尽可能清晰地解释事物。

我们知道在理论物理学中，你可以写下一个方程，但要解它可能极其困难。你必须借助近似方法。但拥有那个指引你的北极星、那个指南针仍然是件好事。对我来说，我尝试以类似的术语来思考机器学习。有一些基础，它们随着时间的推移变化不大，是非常好的指导原则。我们处理的是数据，我们处理的是不确定性。我们希望是定量的。所以你非常自然地，甚至是唯一地，被引向概率论。如果你始终如一地应用概率论，那就是贝叶斯框架。

所以对我来说，贝叶斯框架是一个非常自然的基石，你可以在此基础上构建和思考机器学习。然而，就像理论物理学一样，你通常不能精确地解决问题。当然，贝叶斯范式要求对神经网络中所有可能的参数值进行积分或边缘化，比如  $\int p(\theta|D) p(y|x,\theta) d\theta$ 。但你总是在固定的计算预算下操作，对吧？这个预算可能很大，但你总是受到计算预算的限制。你应该把这个预算花在一个小型神经网络上进行非常彻底的贝叶斯边缘化，还是应该用相同数量的计算周期来训练一个大得多的网络？如果你有足够的数据来训练更大的网络，那么后者在实践意义上似乎要有效得多。

所以，虽然从实践的角度来看，贝叶斯方法在某些领域仍然有特定的应用，但在大多数情况下，它并不是我们今天在主流机器学习中想要使用的框架。我们现在更感兴趣的是规模、进行点估计以及使用随机梯度下降等等。所以我仍然认为学生应该学习贝叶斯推断的基本思想，因为他们必须学习概率。我认为你不可能成为一个机器学习者而不懂概率。一旦你理解了概率并统一地应用它，那实际上就是贝叶斯框架。所以我认为它是基础。但之后你会进行近似。特别是你会进行点估计，比如最大后验估计 $\theta_{MAP}$ 或最大似然估计 $\theta_{MLE}$。所以在实践中，你实际上并没有执行完整的贝叶斯范式。

**问（Tim/主持人）:** 我同意贝叶斯推理很优美，它甚至是命题逻辑在不确定性领域的延续。它是基础性的。但存在这样一个问题，世界是一个非常棘手的地方，有人认为大脑是一种贝叶斯推理机器，但它不可能解决棘手的贝叶斯问题。问题就在这里。所以有很多混合方法，甚至深度学习方法也可以被看作是某种延续，或者介于最大似然点估计和贝叶斯模型之间的某个谱系上。你如何看待这个谱系？

**答（Chris Bishop）:** 我认为这是个很好的问题。我觉得你说的很对。如果你回顾一下有很多竞赛的时期，比如这是一个数据集，我们会保留测试集，你必须在测试集上获得尽可能高的分数。你应该使用什么方法？获胜者总是集成模型（ensemble）。你应该尝试十种不同的方法，最好是多样化的，然后适当地组合它们，也许是取平均值或某种更智能的组合。那个集成模型总是会胜过任何一个单一模型。所以如果你不受计算资源的限制，在其中一些竞赛中你确实不受限制，那么集成总是获胜。你可以把那个集成看作是对预测中所有不确定性进行完全边缘化的一个粗略的、现成的近似。

所以我认为那里有一丝贝叶斯方法的闪光。但同样，在现代，你可能最好训练一个单一的大型模型，而不是训练十个较小的模型然后取平均。所以，了解贝叶斯范式并理解你可以从中学习到什么，在今天仍然是有价值的。但尽管如此，在大多数应用中，你不太可能想要应用完整的贝叶斯机制，因为它计算成本太高了。

**问（Tim/主持人）:** 太迷人了。关于这个，再问一点。你认为大型的，比如说大型语言模型，但大型深度学习模型，你认为它们是一个模型，还是一个不可测知的一捆模型？因为我们有点涉及到“没有免费午餐”定理了。来自贝叶斯世界，我们使用原则来设计模型，而对于神经网络，我们只是训练这些巨大的黑盒子。所以你认为它们是一个模型还是很多模型？

**答（Chris Bishop）:** 我当然总是把它们看作是单一模型。我从未想过把它们看作是分离的模型，除非你明确地构建了一个混合专家（mixture of experts）或类似的东西，其中有内部结构。我猜所有东西都是非常分布式的，某种程度上是全息的、重叠的。关于 GPT-4 一个显著的事情是，你经常看到人们第一次使用它时，会问一些问题，“埃菲尔铁塔有多高？”它可能会给出正确的答案，然后你会说，“哦，这有点意思。”然后你对这项技术有点失望。但这就像给了你一把非常昂贵的跑车的钥匙，你注意到了杯架，注意到它可以很好地支撑一个杯子。你没有意识到你需要启动引擎并驾驶它离开，才能真正获得完整的体验。

所以直到你意识到，实际上你可以进行对话，它可以写诗，可以解释笑话，可以写代码，它可以做很多很多不同的事情。所有这些能力都嵌入在同一个模型中。我认为过去几年一个非常有趣的教训是，像 GPT-4 这样的模型胜过了专门的模型。例如，在我的实验室里，我们曾有一个多年的项目，基本上是说，嗯，这是微软，世界上最大的软件公司。我们有很多源代码。我们可以用源代码作为机器学习的训练数据。我们将能够做各种各样的事情，比如发现错误、做自动补全，各种你能用一个好的源代码模型做的事情。那个项目相当成功，运行得还算不错。

但我们学到的是，当你构建一个巨大的模型，是的，它看到了源代码，它看到了科学论文，它看到了维基百科，它看到了很多很多不同的东西，某种程度上，它在编写源代码方面变得比专门为编写源代码而设计的模型更好。甚至还有一些消融研究（ablation studies），人们有一个模型被训练来解决数学问题，它做得还不错，然后你给它一些看起来不相关的信息，比如说来自维基百科，但去掉了任何与数学相关的内容，你发现它在数学方面实际上做得更好了。所以我认为这里有些东西我们还不真正理解。但总的来说，我认为教训相当清楚：当你有一个更大、非常通用的模型时，它可以胜过一个特定的模型，我认为这非常有趣。

**问（Tim/主持人）:** 我之所以谈论“没有免费午餐”定理，是因为我觉得，正如你所说，模型在输入敏感的方式下表现得相当不同。你问它们关于这个特定事物的问题，它几乎就像是一个不同的模型，因为模型的不同部分被激活了。然后就有一个问题，嗯，“没有免费午餐”定理是否被违反了？是否存在这样一个通用的基础智能体，可以在机器人学中，在任何游戏或任何环境中都做得非常好？或者你认为仍然需要某种程度的专业化？

**答（Chris Bishop）:** 另一个好问题。我认为，这些确实是开放的研究问题。老实说，我不确定是否有人真正知道答案。但我认为其中一个教训是，通用可能比特定更强大。所以很明显，我们应该推动的一个研究前沿是越来越强的通用性，看看效果。GPT-4 不会骑自行车。但如果我们有能够做机器人技术的模型，它们应该是分离和不同的模型，还是如果我们以某种方式将所有东西组合成一个单一的模型，它会更强大？后者很可能是正确的，它会更强大。所以这当然是我们应该推动的一个研究前沿。

我近来非常感兴趣的一个领域是深度学习用于科学，用于科学发现。科学，除其他外，涉及非常精确、详细的数值计算。现在，如果你想把一些数字相乘，GPT-4 将是一种糟糕的方式。它可能会给你错误的答案，即使它得到正确的答案，你也在燃烧大量的计算周期来做一件可以用少得多的计算周期完成的事情。所以在可预见的未来，在某些领域，专门的模型仍然会有一席之地。但即使那样，我也可以看到它们与像大型语言模型这样的东西集成在一起，部分是为了提供人机接口，因为语言模型的一个奇妙之处在于它们非常容易交互。你不必是计算机程序员，你只需要和它们进行自然的对话。

而且，大型语言模型的另一个显著之处，我认为有两点。首先是它们在人类语言方面如此出色。也许这不太令人惊讶，因为它们某种程度上就是为此设计的。但通过被迫有效地压缩人类语言，它们成为了推理引擎。这是一个非凡的发现，对吧？这确实是一个巨大的惊喜。当然对我来说是这样。我想对许多人，也许对该领域的每个人来说都是如此，即它们可以作为推理引擎发挥作用。

所以即使你，比如说，正在进行一些专业的科学计算，你仍然可以考虑将大型语言模型作为科学家的某种副驾驶（copilot），帮助科学家在日益复杂的空间（非常高的维度，许多不同模态的数据）中进行推理。人类越来越难以完全理解这些。我认为这正是大型语言模型可以发挥价值的地方。但我仍然认为在可预见的未来，它会调用专门的工具。

**问（Tim/主持人）:** 你谈论的是统计泛化，但你可以说语言模型做不到，比如说，计算 pi 的第 n 位数字，因为它们没有可扩展的内存。它们不是图灵机。这是一个计算上的限制。但它们或许能够做到我们刚才谈论的那种统计泛化，即使它实际上可能是一种奇怪的专业化形式，以大型语言模型内部方法和模型的集成形式存在。但在语言和推理方面，这很迷人。我认为语言是一堆通过模因（memetically）嵌入的程序。我们玩语言游戏，建立认知范畴。我们将它们嵌入并在社会上分享。就好像外面有一个小小的模拟，我用它来思考。但问题始终是，在多大程度上，这是一堆先前人类完成的处理，我们可以使用它。但是语言模型能够创造像那样的新程序吗？

**答（Chris Bishop）:** 我认为这是一个引人入胜且更广泛讨论的一部分。我确实听到很多，“哦，它做不到 X、Y 和 Z”。通常这是真的。我总是在后面加上“尚未”（yet）这个词，因为我不知道有任何物理定律规定它做不到。有些事情也许目前的架构确实证明了做不到，但在不同架构方面有很多探索。在扩展和泛化神经网络方面有很大的空间。所以我总是想，它还不能做某件事，尚未。

但是很多关于模型局限性的问题和评论，我对此有一个假设。也许我可以跟你验证一下。我可能在这方面大错特错。但是很多对模型看似做不到的事情的批评，特别是当涉及到“哦，它们永远做不到这个。它们不能有创造力，或者它们不能推理，或者它们不能这样那样”时，我想知道这是否很大程度上归结为一个更根本的点，实际上不是技术性的。它实际上与人类，人类过去几千年的旅程有关。因为我们，你知道，几千年前，我想，大多数人会认为人类是宇宙的中心。地球是宇宙的中心。宇宙是为了人类的利益而创造的。我们对自身的重要性持有这种非常傲慢的看法。

而我们几个世纪以来学到的，特别是从天文学等领域学到的，当然是，与整个宇宙的存在相比，整个人类存在的历史只是短暂的一瞬间。而我们在宇宙中的物理位置，就长度尺度而言，我们只是在一个巨大的宇宙中，围绕着一颗不起眼的恒星运行在一个相当无聊的星系中的一粒微小尘埃上。所以我认为，作为人类，我们继续执着于那些我们觉得让我们与众不同的东西是很自然的。我们当然不是地球上最快的生物，也不是最强壮的，但似乎是我们的 大脑让我们独一无二。我们是地球上迄今为止最智能的生物。所以我们认为我们的智能是非常特别的东西。是的，好吧，我们明白了我们只是生活在宇宙一个无聊的角落里，但无论如何，是我们的 大脑让我们与众不同。

让我给你讲个小故事。因为我在微软工作，我有幸很早就接触到了 GPT-4，当时它还是一个经过严格测试、高度机密的项目。所以我接触 GPT-4 的时候，只能和极少数特定的同事讨论它。对其他人，我甚至不能谈论它。那是一个相当令人震惊的时刻。理解和生成语言的能力，某种程度上并不那么令人惊讶，因为我当然一直在关注 GPT-2 和 GPT-3，知道这项技术在不断进步。但是这种推理能力，我有一种发自内心的反应，把我带回了那部电影《2001太空漫游》的感觉，那种感觉是，我正在与某种东西互动，我的同事 Sebastian Bubeck 称之为“人工智能的火花”（sparks of artificial general intelligence）。所以没有人声称 GPT-4 接近人类智能或类似的东西。但这只是第一次瞥见某种东西。这是我一生中第一次与一个非人类的东西互动，而它带有一丝这种高水平智能的微光。

并且意识到这可能是一个新时代的黎明，这个时代可能比 2012 年深度学习黎明的时刻更为重要。正在发生一些非常特别的事情。我想知道我们对这些模型的反应，是否一部分源于那种对我们作为人类所感受到的特殊性的威胁感。我可能完全错了。这纯粹是猜测。

但有趣的是，我们谈论人们使用像“随机鹦鹉”（stochastic parrot）这样的短语。有人声称它只是在复述它以前见过的东西。或者，当然，它有时会产生幻觉（hallucinates）。它会编造一些错误或没有意义的东西。但是想想下面这个情况。假设有一个非常聪明的物理系学生。去了顶尖大学，努力学习了四年。他们会做什么？他们会读书，读论文，听讲座，与教授和其他学生讨论。然后他们参加期末考试，得了 95 分，全年级第一。我们不会说，“哼，95% 的时间他们是复述爱因斯坦和麦克斯韦的随机鹦鹉，另外 5% 的时间他们在产生幻觉。”不，我们会说，“恭喜你，你获得了一等荣誉学位。你以优异成绩毕业了。这是一项了不起的成就。”

所以有趣的是，我们似乎用几乎不同的标尺来看待神经网络的能力和人类的能力。虽然没有人认为目前的模型在许多智能维度上接近人类，但我确实看到了人工智能的最初火花。

最后一点评论。人工智能（Artificial Intelligence）这个术语已经流行了很多年。我过去很讨厌它。我过去总是说，那是机器学习。这些系统没有一个是智能的。它们在识别图像中的猫方面非常出色。从某种意义上说，这并没有什么真正的智能。然而现在，我第一次觉得谈论人工智能很自在，因为我认为我们已经朝着我认为的真正人工智能迈出了最初的婴儿步。

**问（Tim/主持人）:** 我仍然认为能动性（agency）和创造力是区分特征，不一定是因为我们是生物存在。更多的是因为我们是独立的行动者，我们从我们当地的世界中随机抽样事物，并将它们以有趣的方式组合在一起。这样做的时候，智能是关于构建模型、共享模型并将模型嵌入我们文化的过程。所以我觉得 GPT 在训练时是在构建模型。这就是它所做的一切。我可以想象一个世界，有很多 GPT。我们口袋里都有 GPT，也许那时它会更像仿生智能。

**答（Chris Bishop）:** Tim，你提到了很多有趣的点。我认为关于创造力的一点是，这些系统有创造力吗？它们的存在确实是因为人类。它们是由人类创造的。我们应该承认这一点。但我不认为这意味着它们本质上没有创造力。如果我请一位艺术家给我画一幅画，画一些人在日落时分的海滩上散步之类的，几天后他们带着一幅漂亮的画回来了，我可能会讨厌它。他们可能用了非常鲜艳的颜色，而我可能喜欢柔和的粉彩色。但这是个人看法的问题。我不会否认那里存在创造力。

但他们的专业知识来自于，嗯，他们去了艺术学校。他们研究了其他艺术家的作品。他们练习，他们进步了。那种创造力很大程度上归功于前人的工作。但我不认为这会削弱它。同样，一个能解释相对论的物理系学生，你不能说，“哦，你没有发明相对论。”不，是爱因斯坦发明的。你只是从爱因斯坦那里学到的。但这并不削弱他们拥有理解力这个事实，他们传达理解力的事实，以及他们潜在地能够以新的方式思考并具有创造力的事实。

所以我对关于这项技术总体局限性以及它能发展到何处的讨论不那么信服。我并没有特别看到任何限制。大脑是一台机器。它使用我们之前用过的那个术语——连接主义方法。它使用这些细粒度的神经网络。所以它与我们现在拥有的技术有相似之处。也有巨大的差异。其中一些差异表明人工神经网络比生物神经网络强大得多。Hinton 最近强调了这一点。我认为这是一个非常有趣的视角。

所以我会第一个说，是的，我们拥有的技术在许多维度上远不及人类。在许多维度上，它们要好得多。GPT-4 可以比任何人类更好地创作文本。我的意思是，在几秒钟内生成一页连贯的、标点正确、语法良好的文本。我认为没有多少人能做到这一点。所以在越来越多的维度上，系统显然胜过人类，而在其他维度上，还有很长的路要走。

但我认为像这样的技术，生成式 AI 技术，无论是用于创建视频的 Sora，还是 GPT-4，或者其他什么，一个好处是它们确实依赖于提示（prompt）。有一个明确的角色。它们是副驾驶，正如我们所说。它们待在那里什么也不做，你把它们用作一种认知放大器。你有一个半生不熟的想法，现在你可以参与对话，果然，它可以提出不同的思考方式，你会说，“嘿，这真的很好。我喜欢这个想法。”现在让我们采纳它，把它融入进来，再试一次。所以它现在变成了一个伴侣，一个副驾驶，一个增强你认知能力的东西。但人类仍然在很大程度上参与其中，扮演着关键角色，并且实际上启动了这个过程。当然，最终，在一天结束时，是你选择了那个，你知道，十个视频片段中你喜欢的那一个。所以人类在整个循环中都深度参与。我认为这是这项技术一个非常好的特点。

**问（Tim/主持人）:** 我完全同意。目前，人工智能嵌入在人类的认知网络中。所以我们拥有能动性，我们驱动这些东西，它们帮助我们思考。而且，我也同意你的观点，把这些东西看作是有限的计算形式是没有意义的。我们应该考虑集体智能。所以我们是图灵机，我们驱动这些东西，我们共享信息。所以当你审视整个系统时，它是一种新型的模因智能。事实上，在某种程度上，GPT-4 并非运行在微软的服务器上；它存在于我们所有人之中，对吧？这是一种思考它的绝妙方式。但对我来说，它在多大程度上限制了我们的能动性和创造力，这是我着迷的地方。比如 GPT 会说“解开 X、Y、Z 的奥秘”和“X、Y、Z 错综复杂的舞蹈”以及所有这些奇怪的主题和结构。也许这只是我们的生活限制了模型的方式。或者它说明了拥有这些低熵模型（它们会剪掉很多有趣的路径）的一般性限制力。所以我们非常有创造力。GPT-4 有点抵制创造力。这是个问题吗？

**答（Chris Bishop）:** 嗯，我认为这里有一些设计选择。你谈到了通过人类反馈进行强化学习（RLHF）。作为对齐过程的一部分，我们希望以一种有益的方式创造这项技术，以最小化伤害。所以自然地，我们确实会限制它。所以可以肯定的是，一个受约束的 GPT-4 表现得，你可能会说，创造性较差，但也许更有帮助和有益。我们这样做是适当的。也许我们在此过程中失去了一点创造力。所以这里有一个平衡。有一个选择要做，一个关于我们想要如何创造这项技术的设计选择。我们应该对此非常深思熟虑，而不是为此道歉。我认为我们正在做出这些设计选择是件好事。

**问（Tim/主持人）:** 但人们有时会有一种直觉，认为它没有创造力。与此形成对比的是，我使用 DaVinci Resolve，我使用所有这些节点，我有所有这些滤镜和处理变换。区别似乎在于我在设计架构。所以我使用认知基元，并以新的方式将它们组合在一起。通过调整滤镜上的参数，我有点偏离常规。我在自己创造结构。而在神经网络中，结构是隐式的。我不知道结构是什么。

**答（Chris Bishop）:** 嗯，我认为你是在对比两种不同类型的工具。视频编辑工具的设计是让它非常精确地遵循你的指令。你可能更喜欢一个工具而不是另一个，也许是因为界面更易于使用，你能更快地得到结果。但你已经完成了创造性工作。你设计了你想要的视频编辑效果。现在这个工具是为了让你尽可能快、尽可能准确地达到那个效果。

但有时我们需要更多。有时，你知道，如果你遇到了写作瓶颈，不知道从何开始，拥有像 GPT-4 这样的工具可能会非常强大。你并不是把整个过程都委托给技术。你是把它当作一个副驾驶、一个助手来合作，它肯定能帮助你完成那个创造性过程。它会想出一些疯狂的东西。你可能不喜欢其中的大部分，但也许其中一个，你也不喜欢，但它让你思考了一些你本来不会想到的东西。所以两者协同工作肯定能更有创造力。

所以我认为，当然，作为与人类协同工作的工具，它肯定能增强创造力。这当然是我的经验，我认为毫无疑问。而且，如果你想一个简单的例子，我认为大多数人都能体会到的，那就是图像生成。你要做一个演讲，需要一些图像来说明。你知道，你可以去图库网站找图片，但那是固定的集合，你不能轻易调整它。或者你自己编辑图像，那是一个缓慢而痛苦的过程。但现在你只需要一个简单的提示，就能得到一堆例子。如果其中一个不完全是你喜欢的，你可以修改提示并微调它。现在这个过程，一个创造性的过程，就变得可行了。你可以说人类是主导者，但整体的创造力肯定增强了。

当你输入一个文本提示，机器生成了这张漂亮的、逼真的照片时，我的意思是，我们中有多少人没有被过去十年生成式 AI 令人难以置信的进步所震撼？你为什么不称那是有创造力的呢？如果是一个人做的，你会称其有创造力。为什么我们不允许将机器描述为有创造力的呢？这是我不完全理解的部分。

**问（Tim/主持人）:** 你可以说创造力仅仅是作品本身的纯粹新颖性。所以它只是作品中包含了多少熵。但是，你可以把 GPT-4 的散文看作是一种类别。所以里面有很多变化。但也有某些固定的模式（motifs）。现在当人们看到这些模式时，他们会说，“哦，这个我已经见过一百万次了。”所以我曾经认为它是新颖有趣的，现在不这么认为了。但这就是问题所在。所以现在当我写博客文章之类的东西时，我刻意尝试做一些真正有创意的事情。你知道，就好像内在的创造力不重要了。我不想让人们认为我用了 GPT-4，所以这驱动着我。你明白我的意思吗？

**答（Chris Bishop）:** 是的。我的意思是，显然创造力关乎新颖性，而新颖性是我们在这里所渴望的。但这种新颖性是否有价值，那是一个主观的看法。就你而言，是它是否达到了你想要的目标。所以我认为毫无疑问，即使你说我们只是在用新的方式组合现有的想法，人类所做的一切，我认为都建立在他们自己先前经验和他人工作的基础上。我认为这完全没问题。这是关于人类的一个奇妙之处，我们一代又一代地建立在先前工作的基础上。我们现在制造的机器在很大程度上依赖于先前人类的创造力和工作，因为它们向人类学习，并且由人类设计。我认为这完全没问题。这是一件美妙的事情。它们增加了人类创造力的总和。那是一件美妙的事情。

回顾你过去的两本书，回想起来，你对哪些部分最感自豪？又有哪些部分，你觉得当时做决定时，可能错误地预测了某件事的成功程度？

**答（Chris Bishop）:** 非常有趣。我最自豪的其实是第一本书，叫做《Neural Networks for Pattern Recognition》。原因是我认为这本书在引导该领域走向更概率化、更统计化的机器学习视角方面颇具影响力。今天的人们可能很难体会到这一点，但情况并非总是如此。当我最初进入机器学习领域时，很多灵感来自于神经生物学，这本身没什么问题，但它缺乏数学上的严谨性，缺乏任何数学基础。所以当时有很多尝试去多了解一点大脑，然后在算法中模仿它，看看效果是否更好。有很多试错。当然，机器学习中仍然有很多经验性的试错，但至少我们有了概率论这个基石。

所以我认为那本书是第一本真正从统计学、从概率论的角度来探讨机器学习和神经网络的书。我认为在这方面，这本书非常有影响力。那时这个领域要小得多。今天我们认为这是显而易见的。但就我最自豪的事情而言，可能就是那本 1995 年的第一本书的影响力。

至于我回首过去可能会做得不同的事情。我想如果我看看，比如说 PRML，再看看这个领域的发展轨迹。我们看到神经网络在 1980 年代中期到 1990 年代中期风靡一时，然后它们有点被其他技术超越了。然后我们经历了所谓的寒武纪大爆发，你知道，支持向量机（SVMs）、高斯过程（Gaussian Processes）、贝叶斯方法、图模型（graphical models）等等所有这些。我认为有一件事，我认为 Geoff Hinton 真正做对的是，他真正理解了神经网络是前进的方向。他真的坚持了这个观点，可以说是历经风雨。

我有点分心了，特别是我们之前谈到的贝叶斯方法，它们是多么优美和优雅。对于一个理论物理学家来说，从贝叶斯视角思考一切非常有吸引力。但实际上，我们今天看到的是，给我们带来这些非凡进步的实用工具是神经网络。而这些想法中的大部分都可以追溯到 1980 年代中期，比如梯度下降的思想等等。只有一些新的调整，你知道，我们有了 GPU，我们有了 ReLU 激活函数，我们有了一些，但基本上大多数想法在 1980 年代末就已经存在了。

我们当时并没有真正理解需要使用它们的惊人规模，但它们只有在你拥有这种庞大规模的数据和计算能力时才能真正发挥作用。当然，那时我们并没有真正拥有 GPU，也不知道如何使用它们。所以有一些关键的发展解锁了这一切，使其成为可能。

但我想，如果我能以惊人的后见之明做些不同的事情，除了投资某些股票之类的，以及所有其他如果你有完美后见之明可以做的事情之外，我想另一件事可能就是我会一直专注于神经网络，因为最终是这项技术取得了成功。

但我总是回到概率论，它是一个非常统一的思想。让我给你举一个来自 PRML 的具体例子。当时有两种不同的技术，一种叫做隐马尔可夫模型（Hidden Markov Models, HMMs），在当时的语音识别领域非常流行。另一种技术叫做卡尔曼滤波器（Kalman filters），多年来一直被用来导航航天器、在雷达上跟踪飞机等等。

结果发现它们本质上是相同的算法。不仅是相同的算法，而且它们可以从最美妙、最简单的原理推导出来。你只需要用到概率的和法则与积法则 $p(x) = \sum_y p(x,y)$ 和 $p(x,y) = p(y|x)p(x)$，然后是联合概率分布可以通过有向图来描述其因子分解的思想。如果你想... 当我准备 PRML 时，我翻阅了一些叫做《卡尔曼滤波器》、《卡尔曼滤波器导论》的书，它们会一章接一章地讲前向方程，然后一章接一章地讲后向方程等等。非常非常复杂，非常晦涩难懂。

但你可以推导出卡尔曼滤波器，并且几乎免费得到隐马尔可夫模型，只需几行代数，从概率论和因子分解的思想出发。这是一种深刻的数学原理在起作用。你会发现消息传递算法（message passing algorithm）。如果它是一个树状结构的图，它是精确的，并且你只需要两趟传递。非常优美和优雅。所以我喜欢我们正在探索所有这些许多不同的前沿领域这个事实，但我更喜欢我们至少有一些指南针来引导我们，在我们探索这个组合爆炸般巨大的空间时。

## 五、 AI for Science (AI4Science)

**问（Tim/主持人）:** Keith Duggar 博士（我的联合主持人）总是说他不需要记住所有不同的统计量，因为他可以从第一性原理重新推导它们。这很好。但我们应该转向 AI for Science。你正在微软研究院领导这个项目。能给我们介绍一下吗？

**答（Chris Bishop）:** 是的。就我个人而言，这当然把我早期对理论物理、化学和生物学的兴趣与机器学习结合了起来。许多人在几年前意识到，在机器学习将影响的众多领域中，科学发现领域，在我看来，将是最重要的。

我这么说的原因是，实际上是科学发现真正让过去几千年的人类走上了那样的发展轨迹，不仅仅是理解我们在宇宙中的位置，而且更能掌控我们自己的命运，使我们的寿命翻倍，治愈许多疾病，给我们带来更高的生活水平，给我们一种比人类传统上享有的更光明的未来前景。而这一切都是通过科学发现，然后将这些关于世界的知识和理解以技术形式应用于农业、工业等等而实现的。

所以我认为，对于 AI 来说，没有比这更重要的应用了。但真正有趣的是，很明显，许多科学发现领域正在被颠覆（disrupted）。当我说颠覆时，我只举一个简单的例子：神经网络、机器学习模型作为模拟器（emulators）的能力，它们可以模拟以前非常昂贵的数值模拟器，这通常能带来 1000 倍的加速。比如，我们现在预测天气可以比几年前使用深度学习之前快 1000 倍，而且精度相同。

现在，如果这是唯一发生的事情，那本身就将是一场颠覆。那本身就值得成立一个 AI for Science 的团队。但我认为这实际上只是触及了表面。但任何时候，当某个非常核心、非常重要的东西速度提高了 1000 倍，这意味着你可以在几十个小时内完成以前需要数年才能完成的事情。这确实是一场颠覆。这确实是变革性的。

所以，几年前，我向我们的首席技术官提议说，看，这是一个非常重要的领域。我很高兴能卸任微软欧洲研究院实验室主任的职务。我想领导一个新的团队，专注于 AI for Science。这个提议得到了极大的热情响应。所以我们一直在发展和建设这个团队。这是一个非常有趣的团队。它非常跨国。我们在许多不同的大陆和不同的国家都有人。我们在阿姆斯特丹和柏林开设了新的实验室。我们在北京和上海有团队，在西雅图也有人。所以，非常多学科交叉，非常跨国，但有一个共同点，那就是对机器学习和 AI 将如何真正改变和加速我们进行科学发现的能力抱有真正的兴奋和热情。

**问（Tim/主持人）:** 你刚才提到了归纳偏置（Inductive priors）。我最早了解到在机器学习中设计归纳偏置的艺术，是从 Max Welling 的小组那里。他们说，非凡之处在于，你可以利用，比如说来自物理学的原理，设计这些归纳偏置，我们可以缩小我们正在逼近的假设类别（hypothesis class）的大小。因为我们知道目标函数（target function）在这个类别内，我们没有引入任何近似误差。我们通过使问题变得易于处理，在某种程度上克服了机器学习中的一些诅咒，这太神奇了。但这说的是将领域知识（domain knowledge）融入这些系统的这种有原则的方法。

**答（Chris Bishop）:** 这真的很有趣。Max 和我有相似的轨迹。我们都拿到了理论物理学的博士学位，然后都进入了机器学习领域。我想我们都觉得，在科学领域使用机器学习时，归纳偏置扮演着非常重要的角色。

我想每个人都熟悉 Rich Sutton 的那篇名为《惨痛教训》（The Bitter Lesson）的博客文章。如果观看这个视频的任何人不熟悉，他们应该在看完这个视频后立刻去读那篇博客。那是一篇很短的博客。不作过多剧透，他基本上是说，每一次人们试图通过构建先验知识、将我们称为归纳偏置的东西构建到模型中来提高机器学习性能的尝试，都会带来一些改进。但很快，它就会被仅仅拥有更多数据的其他人所超越。这确实是一个惨痛的教训。这是一篇精彩的博客，人们应该，我读了很多遍，我认为人们应该，可能每个月读一次。它非常鼓舞人心。

但我认为可能存在例外。我认为科学领域就是这样一个例子，在可预见的未来，归纳偏置将极其重要，这几乎与“惨痛教训”相反。有几个原因。

其一，我们拥有的归纳偏置不是那种，比如说语言学或其他任何基于人类通过经验获得的专业知识的领域的那种。因为一个在多年中积累了大量经验并形成了一些指导性经验法则的人，这正是机器学习非常擅长做的事情——处理大量数据并归纳出数据中的规则，或者说模式。所以我认为那种归纳偏置通常是有害的。我认为“惨痛教训”肯定适用于那里。

但在科学领域则相当不同。首先，我们拥有的归纳偏置非常严谨。我们有能量守恒、动量守恒的概念。我们有对称性。如果我有一个处于真空中的分子，它有特定的能量。如果我旋转这个分子，计算机中所有原子坐标的表示会发生剧烈变化，但能量是相同的，即 $E(\text{Rot}(x)) = E(x)$。所以我们有这种非常严谨的归纳偏置。我们还知道，原子层面的世界可以用薛定谔方程极其精确地描述。再加上一些相对论效应，你就得到了对世界惊人准确的描述。但直接解它太复杂了，或者说它的成本随电子数量呈指数增长，即 $O(e^N)$。但尽管如此，我们拥有这个真正理解支配宇宙定律的基石。所以这是第一点，我们有我们深信不疑的非常严谨的先验知识。并不是我们认为能量守恒不起作用，我们知道它是真的。

第二点是，我们通常在数据稀缺（data scarce）的情况下操作。大型语言模型能够使用互联网规模的大量人类创造的数据，无论是维基百科、科学论文，还是几乎所有人类的输出，都可能是大型模型可以依赖的潜在材料。它们处于一个数据非常丰富的（data rich）状态，可以扩展规模。所以我认为“惨痛教训”在那里确实会起作用。

而在科学领域，数据可能来自计算成本高昂的模拟，或者来自昂贵的实验室实验。数据是有限的。所以我们通常在数据稀缺的状态下操作。所以我们拥有的数据相对有限，而我们有非常严谨的先验知识。因此，数据和归纳偏置之间的平衡就非常不同。因为当然，“没有免费午餐”定理告诉我们，你不能纯粹从数据中学习。你必须有某种形式的归纳偏置。对于 Transformer 来说，这是一种非常轻量级的归纳偏置。我们相信存在一个深层级结构，存在一些数据依赖的自注意力机制，但基本上就是这样了。其余的由数据决定。

在科学领域，引入这些归纳偏置的空间要大得多。引入归纳偏置的需求也大得多。而且，顺便说一句，这再次，以我个人非常偏颇的观点来看，使得将机器学习和 AI 应用于科学成为 AI 和机器学习最激动人心的前沿，因为它是最具创造性的领域，也最需要引入那些支撑宇宙的美妙数学。

**问（Tim/主持人）:** 这太有趣了。我们能在这上面稍微停留一下吗？Rich Sutton 在他的《惨痛教训》文章中，明确提到了对称性，你知道，他在警告不要在这些模型中加入人类设计的构件。而正如你所说，Max Welling 著名地利用他的物理学知识构建了这些规范等变神经网络（gauge equivariant neural networks）。我只是想理解一下高分辨率物理先验和我们学到的那种宏观的、可能是脆弱的人类知识之间的谱系。是不是只是因为我们认为这些物理先验是基础性的，所以这是一种完全可以接受的约束搜索空间的方式，而这些高层次的先验是脆弱的？

**答（Chris Bishop）:** 是的。我认为来自人类经验的先验知识更像是那种脆弱的类型，因为机器可以看到比任何人在一生中看到的都多得多的例子，并且可以更系统地审视所有这些数据。我们不会受到比如近期偏见（recency bias）之类因素的影响。所以我认为那种先验知识是规模和数据将获胜的地方。

而我们从物理定律中得到的先验知识，在某种意义上，要严谨得多。对称性非常强大。有时人们说物理学或多或少就是对称性。这几乎是对的。对吧？守恒定律源于对称性。时空中的平移不变性（translation invariance）导致能量和动量守恒。电磁场的规范对称性（gauge symmetry）导致电荷守恒等等。所以这些是通过对称性得出的非常非常严谨的定律。

但是，即使你采用数据驱动的方法，人们也经常使用数据增强（data augmentation）。如果你知道一个物体的身份不依赖于它在图像中的位置，你可能会对你的数据进行大量随机平移来增强你的数据。所以数据增强可以是一种数据驱动的方式来构建这些对称性。

但现在当我们拥有非常丰富的先验知识时，我再回到薛定谔方程。它在原子层面上以极其精确的方式描述世界。但解它非常非常昂贵。所以我们可以做的是，我们可以缓存（cache）那些计算。我们称之为科学发现的第五范式（fifth paradigm），这是一个相当花哨的术语。但想法很简单，就是与其使用传统的数值求解器来解决像薛定谔方程或所谓的密度泛函理论（density functional theory, DFT）这样的问题，与其直接解决它来解决你的问题。不如用那个模拟器来生成训练数据，然后用那些训练数据来训练一个机器学习模拟器。然后那个机器学习模拟器现在可以模拟那个模拟器，但速度通常快 3 到 4 个数量级 ($10^3$ 到 $10^4$ 倍)。所以只要你大量使用它，并且摊销了生成训练数据和进行训练的一次性成本，如果你打算使用它很多很多次，总的来说，它会比使用模拟器快得多、高效得多。这只是我们在这个领域看到的突破之一。

**问（Tim/主持人）:** 首先，如你所说，存在一个谱系：我们可以只在大量数据上训练，或者我们可以增强数据，或者我们可以为数据制作一个模拟器，然后我们可以训练一个机器学习模型。正如我们刚才谈到的，这些归纳偏置具有如此高的分辨率，以至于我们没有限制我们想要学习的目标函数。我们可以就此做出非常有原则的论证。但对我来说，一个问题是，有一种，我不知道最好将其描述为探索与利用（exploration versus exploitation），但需要有一定程度的偏离常规。所以我们定义了结构，我们基本上建立了一个生成模型，我们可以生成一大堆轨迹。但是否可能出现这样的情况：我们没有足够的多样性（variance）来发现有趣的东西？

**答（Chris Bishop）:** 关于整体的科学方法——提出假设、进行测试、评估这些假设、完善假设、进行更多实验等等那个科学循环，有一个非常有趣的问题。我认为机器学习将在那里扮演重要角色，因为数据正变得非常高维、非常高通量。人类再也无法分析这些数据了。一个人无法直接查看大型强子对撞机（Large Hadron Collider）的输出，它每秒产生 PB 级的数据。我们需要机器来帮助我们。

但我再次认为，人类提升到了，可以说是，乐团指挥的层面。他们不再需要手动做事情。机器正在帮助加速这一切。而且我认为机器可以通过指出异常、突出数据中的模式等等方式，潜在地帮助加速创造性过程，但人类科学家始终参与其中。

但即使从那些更崇高、更偏哲学的考量下降到实际操作层面，当我们谈论发现时，我们也对非常实际的方法感兴趣，比如我们如何发现一种新药，或者我们如何发现一种新材料。所以科学发现也意味着那种非常务实的、近期的（near-term）方法。在那里，我们通过这个模拟器的概念，在我们探索新分子和新材料的组合爆炸般巨大的空间的能力方面，看到了真正显著的加速，有效地探索这些空间以找到可能是新药或用于电池或其他绿色能源形式的新材料的潜在候选者。所以这本身就是一个非常激动人心的前沿。

**问（Tim/主持人）:** 这太有趣了。搜索空间，药物发现是一个有趣的例子。我想你也谈到了可持续性作为另一个可以谈论的应用。但你如何识别？找到一种有趣的药物？

**答（Chris Bishop）:** 药物发现过程首先从疾病开始。首先尝试决定我们要解决某个特定的疾病，然后找到一个合适的目标（target）。标准的小分子范式（small molecule paradigm）——今天大多数药物都属于此类——是合成的有机小分子，它们与特定的蛋白质结合。所以制药公司会花费大量时间来识别靶点。比如一个蛋白质，它有一个特定的区域，分子可以与之结合，从而影响该蛋白质的行为，开启或关闭疾病通路中的某个部分，打破疾病链条。

那么挑战就是找到一个小分子，它首先具有与目标蛋白质结合的特性。这是第一步。但它还必须满足许多其他条件。它必须能被身体吸收。它必须能被代谢和排泄。它尤其不能有毒性。它不能与身体中许多其他蛋白质结合并导致坏事发生。

所以你面对的是一个非常巨大的分子空间，通常估计大约有 $10^{60}$ 种潜在的类药分子。在这个 $10^{60}$ 的巨大空间中，你试图找到一个满足所有这些众多标准的例子。所以一种方法是计算生成大量的候选分子，然后逐一筛选它们的不同属性。这个筛选过程，越能通过计算机模拟（in silico）而不是在湿实验室（wet lab）中完成，就能越快地完成。搜索空间就能越大，因此你探索那个可能性空间的部分就越大，从而有望增加找到好候选者的机会。因为许多为某种疾病寻找药物的尝试最终都失败了。最终一无所获。所以提高成功的概率，提高那个发现过程的速度。在所有这些方面，机器学习都有很多可以颠覆的地方。

**问（Tim/主持人）:** 在那个过程中，我猜你描述的是，你生成候选者，然后你几乎是辨别出有趣的那些，然后你以一种迭代的方式不断重复。

**答（Chris Bishop）:** 让我给你举一个具体的例子。我们做了一些关于结核病（tuberculosis）的研究。结核病在 2022 年，这是我们有数据的最近一年，非常不幸地导致了大约 130 万人死亡。这可能看起来令人惊讶，因为我们有抗生素，我们有治疗结核病的药物。为什么还有这么多人死亡？一个核心原因是细菌正在进化产生耐药性。所以人们正在寻找新的药物。

也许我会花点时间解释一下架构，并深入一些技术细节。我们想要一种方法来寻找... 我们知道目标是什么。我们被告知了目标蛋白质是什么。目标蛋白质有一个叫做“口袋”（pocket）的区域，我们寻找能与蛋白质上那个口袋区域紧密结合的分子。

我们处理这个问题的方式是，首先，构建一个语言模型，但不是人类语言的语言模型，而是分子语言的模型。我们首先获取一个叫做 SMILES 的表示方法。这是一种缩写，它只是一种将分子描述为一维字符串的方法。所以你首先获取一个包含，比如说，一千万个以 SMILES 字符串表示的分子的庞大数据库，并将它们视为 Transformer 模型的标记（tokens）。通过让它预测下一个标记，即 SMILES 字符串的下一个元素，你构建了一个基于 Transformer 的语言模型，它可以说分子的语言。所以它可以生成式地运行，可以创建新的分子作为输出。你可以把它看作是一种基础语言模型，但说的是分子的语言。

现在我们想生成分子，但不仅仅是任何分子。我们想要能与特定目标蛋白质结合的分子。所以我们有目标蛋白质，特别是我们感兴趣的口袋区域。我们可以将蛋白质的氨基酸序列作为输入。但我们需要更多信息。我们需要口袋的几何形状。这就是一些归纳偏置发挥作用的地方。我们需要有构成那个口袋的原子的几何形状的表示。但需要一种能表示这些等变性（equivariances）的方式。它们被编码为 Transformer 模型的输入，该模型学习蛋白质口袋的表示。

最后我们需要的部分，正如你所说，我们想迭代地做这件事。我们想把一个好的分子变成一个更好的分子，而不是在一个 $10^{60}$ 可能性的空间中盲目搜索。所以我们想提供的另一项输入是描述一个已知能与该口袋结合的小分子的描述符。但我们想以一种能产生变异性（variability）的方式来做这件事。我们实际上使用了一个变分自编码器（variational autoencoder, VAE）来创建那个表示。这是一个将分子转换到潜空间（latent space）的编码器。我们可以从那个潜空间中采样。

然后这个语言模型，SMILES 语言模型，可以通过交叉注意力（cross-attention）同时关注变分自编码器的输出和使用蛋白质编码器编码的蛋白质的输出。所以我们在这里所做的，我认为，是相当有品味地结合了来自，你知道，最先进的现代深度学习的一些元素。

然后结果可以通过一个包含已知能有效结合其他蛋白质的小分子的数据库进行端到端的训练。一旦系统训练好了，我们现在可以提供已知的结核病靶点和一些已知能与之结合的分子作为输入。然后我们可以迭代地改进那些分子。在输出端，我们得到具有更好结合效率的分子。我们能够将结合效率提高两个数量级。所以我们现在拥有了在与这个靶点蛋白质结合效率方面达到世界领先水平的分子。

当然，我们自己无法进行湿实验室实验。我们与一个名为 GEDIT（全球健康药物发现研究所）的组织合作。他们合成了我们生成的分子，并测量了它们的结合效力。所以我们对此非常非常兴奋。当然，下一阶段现在是以此为起点，进一步改进和优化那些分子，并尝试解决我们对药物实际在人体上进行测试所需的所有其他要求，比如毒性、代谢等等所有其他方面。但我认为这只是一个非常好的例子，几乎是使用现代深度学习架构加速药物发现过程的第一步。鉴于我们某种程度上是这个领域的新手，与拥有湿实验室经验和能力的领域专家合作，这已经取得了，我认为，相当惊人的成功。对我来说，这是一段非常激动人心的旅程的开始。

**问（Tim/主持人）:** 模型之间是否存在某种表示迁移（representational transfer）？例如，你谈到了这个几何先验模型，它生成标记输入到语言模型中。因为顺便说一句，仅仅使用语言模型是一种引人入胜的方法。我与 Christian Szegedy 聊过，他仅使用语言模型进行数学猜想推演，你知道，只是将数学构造放入语言中。他们过去常用图神经网络来做这个。所以我猜问题是，你是否可以用一个归纳的、有原则的模型来某种程度上引导（bootstrap）它，然后之后只使用语言模型进行训练？

**答（Chris Bishop）:** 我认为，这里的普遍原则是一个非常强大的原则。即从其他领域借鉴力量（borrowing strength）的想法。我认为我们在深度学习中一次又一次地看到这一点，机器学习模型能够从一个领域提取一些通用模式，并将它们转换到一个完全不同的领域。我们之前谈到大型语言模型在编写代码方面变得更好，如果它们也接触过诗歌或一些看起来完全不相关的东西。这里正在发生一些非常深刻和微妙的事情。但也许以一种不那么微妙的方式，很明显存在一种分子的语言，一种材料的语言，通过构建对那种语言有更广泛接触的模型，它们几乎总是会在我们想要应用的特定任务上变得更好。所以我认为这里有一个核心的普遍原则。

**问（Tim/主持人）:** 这太有趣了，因为我过去认为这些归纳先验模型的缺点可能是一个模型对应一个归纳先验，但这种潜在的能力，即引导一个可以做所有事情的基础模型，真的很有趣。

**答（Chris Bishop）:** 我认为最强大的归纳偏置是我们关注的那些，确实是那些非常通用的偏置，比如对称性。那只是宇宙非常基本的属性。我们希望这些真正地融入到模型中。我认为，我们对更具体领域的那些直觉，它们可能会误导我们，因为它们基于我们对更有限领域的经验。我认为这正是机器可以做得更好的地方，可以更系统地处理和解释大量数据，并从中提取规律性。

**问（Tim/主持人）:** 好的。在我们结束这个话题之前，这是一个有点“脑洞大开”（galaxy brain）的问题，顺便说一句，这是现在孩子们都在用的俚语。但是，我们的物理知识有多基础？你知道，问题是，我们设计这些归纳偏置，就好像它们是基础性的一样。但是像 Stephen Wolfram 这样的人认为，存在一个更深的本体论现实（ontological reality）。你知道，它可能是一个图、元胞自动机（cellular automaton）或类似的东西。这是你思考的问题吗？即我们的模型和现实之间的差距？

**答（Chris Bishop）:** 所以我认为，首先，有史以来最伟大的科学发现之一是宇宙可以用简单的定律来描述这个事实。这并非先验地显而易见。这本身也许是最深刻的发现，真的可以追溯到牛顿。但我们一次又一次地发现这一点。

我们还发现，我们今天对宇宙的理解，几乎就像洋葱一样，我们正在一层层地剥开。你知道，牛顿，如果你想导航一艘航天器到木星，你仍然使用牛顿运动定律和牛顿引力定律。它完全够用。但这并不意味着我们相信它是对自然的精确描述。我们现在对自然有了更深入的描述。例如，我们理解了相对论。广义相对论告诉我们，实际上牛顿第二运动定律，或者说牛顿引力定律，只是一个近似。平方反比定律是一个相当好的近似，但我们现在有了更好的描述。

但是，很难说我们已经找到了最终答案。更确切地说，人类知识总是站在我们不理解的事物的边缘。科学发现总是关于探索我们不理解的事物，弄清楚，你知道，这些定律是否真的成立，以及我们在数据中看到的异常是否是因为我们尚未观察到的某种现象。我是说，海王星就是这样被发现的，通过观察到行星的行为与根据牛顿定律应有的行为不符。牛顿定律本身没问题，只是有另一颗行星在扰动它们。或者，水星近日点的进动是因为... 是因为有另一颗行星吗？不，是因为实际上牛顿引力定律不完全正确。我们需要相对论来理解这一点。

所以我认为科学探索，据我所知，没有特定的终点。更确切地说，我们有我们理解的事物，并且总有新的前沿。你知道，当我还是个青少年，对物理学感到兴奋时，我喜欢阅读关于相对论和量子物理学的东西，但这有点令人沮丧，因为我觉得自己好像晚生了，你知道，50 年或者多久。所有激动人心的事情都发生在 20 世纪初，而且好像都已经被完成了。但现在我们有了，你知道，暗物质和暗能量，我们意识到宇宙的大部分并不在我学校学到的元素周期表上。实际上，我本不必担心，你知道。我想是 Vannevar Bush 称之为“无尽的前沿”（The Endless Frontier）？科学是一个无尽的前沿。总有更多东西等待探索，总有更多东西等待学习。

所以，你提到的那些具体想法是否有实质内容，我不知道。最终，科学方法会告诉我们。如果它们具有预测能力，能够预测我们以前不知道的新现象，那么，嗯，那么它们就具有可信度，至少在科学家看来是这样。但最终，我们仍然坚持科学方法。它是关于我们做出能够通过实验检验的预测的能力。如果它们经受住了实验的检验，那么我们就会给予那些假设更多的权重，最终它们会被提升到理论的地位。

## 六、 深度学习的开放性问题与挑战

**问（Tim/主持人）:** 我常常思考我们认知的边界。我们能够理解什么，我们倾向于使用高层次的隐喻来理解事物。信息就是一个很好的例子。所以，很多人把宇宙说成是信息。这种本质主义的观点很有趣。把一切都建模为智能体。很可能宇宙就是如此奇怪和陌生，以至于我们永远无法理解它。所以在我们的可理解性、我们的模型和现实本身之间存在一点相互作用。而宇宙显然是完全不可理解的，因为没有人能真正思考量子物理学。它完全违背了我们在这个宏观层面上学到的日常直觉。

**答（Chris Bishop）:** 我认为我们必须承认宇宙已经是用数学来描述的。那是我们精确的描述。然后我们有关于波、粒子等等的隐喻。但它们没有一个真正能恰当地运作。它们只是我们赖以支撑的拐杖。但最终，它是一个数学描述。但这本身也非常有趣，世界是由数学描述的这个事实，通过在纸上画一些小标记，你就可以发现一颗新的行星。这相当不可思议。

**问（Tim/主持人）:** 更广泛地转向深度学习。我们已经触及了这一点，但现在格局由 Transformers 架构主导。你对此有什么大致看法？

**答（Chris Bishop）:** 像任何领域一样，我认为机器学习有它的潮流和浪潮。某样东西效果非常好，然后每个人都抓住它并利用它。这都很好。如果 Transformer 是深度学习的最终定论，是我们未来将永远使用的架构，我会有点惊讶。但它显然效果非常好。我们还远未达到其能力的终点。所以在应用中利用 Transformer 架构，看看我们能从中获得多少收益，这是非常有意义的。

同时，显然也有机会思考 Transformer 的局限性，计算成本。我们是否可以用更好的扩展性来做同样的事情，如果我们想要更长的上下文窗口等等。所以我认为在新的架构方面也有大量有趣的研究可以做。所以我认为两者都需要。

**问（Tim/主持人）:** 那么，这里是另一个“脑洞大开”的问题。深度学习为什么有效？因为从表面上看，它不应该有效。它不应该能训练，它不应该能泛化。而它们却取得了绝对非凡的成功。为什么？

**答（Chris Bishop）:** 我认为首先，在某个层面上你可以说，嗯，我们理解它们为什么有效。我们在拟合非线性函数。我们某种程度上是在高维空间中做曲线拟合。所以我们需要一些泛化能力。这又回到了“没有免费午餐”定理，需要一些归纳偏置，也许是平滑性、连续性。也许是比那更具约束性的东西。所以在某个层面上，这并不奇怪。我可以将多项式拟合到一堆数据点上，通过梯度方法，并且可以对中间点做出很好的预测。只是将其推广到更多数据和更高维度。所以在某个层面上我说，不，它们有效一点也不奇怪。

在另一个层面上，当然，它们效果如此之好是显著的。但它们运作的方式非常有趣。有一件事，如果我们回到机器学习的早期，当然也回到统计学的世界，那种拟合参数数量远超数据点数量的模型的想法，对于任何有自尊的统计学家来说，显然是疯狂的。我们绝不会这样做。也许这就是为什么没有人真正尝试过。

然而我们有这些奇怪的现象，比如说，训练误差降到零，但测试误差却继续下降，即使训练误差已经是零了。关于随机梯度下降（stochastic gradient descent, SGD），实际的训练过程显然在那里很重要。不仅仅是这里有一个成本函数，我们找到全局最小值。这不仅仅是全局最小值的属性。不，有许许多多的全局最小值都具有零误差。有些解显然会过拟合。其他的则泛化得很好。所以训练过程本身有一些东西需要我们去理解。

所以我认为在“它们为什么效果这么好”这个问题上，还有很多研究要做。我认为这是一个悬而未决的问题。我们可以描述模型。我们可以说很多关于模型的事情。我们可以说因为它有这样那样的层数。因此，空间的结构具有这样那样的属性，它将空间划分为这样那样的区域等等。这些都是对的。我不知道这是否给了我们关于它为什么有效的真正洞见。我认为那里有一些非常悬而未决的问题。

这让我想起有点像神经科学。你知道，我们有人类大脑。它能做这些令人惊奇的事情。我们可以获得越来越多、越来越丰富的关于哪些神经元在何时放电以及放电如何相关的数据。我们可以学到一些关于底层机制的东西。这有点像神经科学，只不过我们可以在我们的人工大脑的每个神经元中放入探针，并收集非常非常丰富的信息。所以我再次认为，在更好地理解它们为什么能够如此好地泛化，以及为什么我们有这些看似过参数化却不过拟合反而具有很好泛化能力的模型的奇怪现象方面，存在一个非常有趣的研究前沿。还有很多研究要做。

**问（Tim/主持人）:** 你提到你可以训练一个深度学习模型，在训练误差收敛之后，测试损失还会继续改善。这似乎有点不合常理。还有 Grokking 现象，那是另一个... 几乎就像我们讨论物理学时说的，在优化算法的运作之外，正在发生一些我们不理解的事情。

**答（Chris Bishop）:** 嗯，你可以讲故事，对吧？你可以说有一个巨大的空间。空间中的每个点都是模型所有参数的一种设置。也就是模型的所谓权重空间。也许你开始时在原点附近，带有一些小的随机初始化。然后你遵循由随机梯度下降定义的某个轨迹。在这个空间中有很多很多地方，所有这些地方都具有零训练误差。它们是相连的。所以存在某种零训练误差的流形（manifold）。你从原点出发。随机梯度下降不知何故并没有随机地带你走。也许它把你带到了，比如说，这个流形上最近的点之类的地方。也许那是一种正则化。也许那个地方具有某些导致良好泛化的平滑特性。

所以你可以讲这类故事。我认为挑战在于将这些故事转化为可预测的理论。所以我认为当我们有一个理论，当我们有一个关于正在发生什么的理论时，我们会知道我们拥有了一个理论，因为它能预测新事物，而不仅仅是讲述我们已经通过经验发现的事情的故事。我认为这仍然是一个非常悬而未决的问题。

**问（Tim/主持人）:** 那么，你对神经网络在偏见、公平性和安全性方面的可理解性（intelligibility）怎么看？因为你可以把这些东西就看作是不可测知的神经元袋子。但我们需要有一些护栏，对吧？

**答（Chris Bishop）:** 我们绝对需要创造对人类有益的技术。这一点毫无疑问。并且有机制可以做到这一点，来对齐（align）系统，无论是通过人类反馈（RLHF），还是通过提供更传统的检查方式的外部护栏（guardrails）来监控事物的使用方式。这显然是必要的。

我发现投入如此多的精力和努力在这方面，非常令人鼓舞。是的，路上会有颠簸，途中肯定会有失误。但总的来说，我们似乎正朝着一个非常好的方向前进。但我认为，人们对这项非常强大和非常通用的新技术相关的潜在风险给予了大量关注，这让我有希望我们能避免大多数最大的风险。

**问（Tim/主持人）:** 你能给我举一个模拟器（emulator）的具体例子吗？

**答（Chris Bishop）:** 是的，可以。一个非常好的例子。实际上这是我在核聚变项目工作时参与的最后一个项目。我当时把核聚变作为进入机器学习的一个跳板。我们想对一个核聚变实验，一个叫做托卡马克（tokamak）的东西，进行实时反馈控制。那是温度极高的等离子体。我们想用神经网络来做非线性反馈。

那里的挑战是，获取一个等离子体，它像一个甜甜圈形状的高温等离子体环。当时已知如果你能改变它的横截面形状，就能提高它的性能。在牛津郡卡勒姆（Culham）有一个叫做 Compass Compact Assembly 的实验装置。这个实验旨在产生非常有趣、奇特的横截面形状，以探索其性能。我们想用神经网络来做那个反馈控制。

好消息是我们有一个很棒的归纳偏置，叫做 Grad-Shafranov 方程。它是一个二阶椭圆偏微分方程。但关键是它非常精确地描述了等离子体的边界。对吧？你从等离子体周围数百个小拾取线圈（pickup coils）进行一系列测量。那些是边界条件。你求解 Grad-Shafranov 方程，你就知道了等离子体的形状。

目标是预先决定你想创建一个圆形等离子体，然后改变它的形状，并且如果形状不完全是你想要的，就进行修正。你会改变大的控制线圈电流并改变其形状。问题在于，在当时最先进的工作站上求解 Grad-Shafranov 方程需要 2 到 3 分钟，而我们必须以大约 20kHz 的频率进行反馈。它大概慢了六个数量级 ($10^6$)。

所以我们采取的方法是，我们在工作站上多次求解 Grad-Shafranov 方程，花了几天几周的时间，直到我们建立了一个包含已知解及其磁测量值的大型数据库。然后我们训练了一个神经网络，在当时只是一个简单的双层神经网络，可能只有几千个参数。按照现代标准来看微不足道。但它被训练来接收磁测量值并预测形状。我们可以把它放入一个标准的反馈回路中。

当时我们与另一个做类似事情的组织，另一个也在做同样项目的不同核聚变实验室，有点竞争关系。所以这很有动力。我很高兴地说我们先到达了终点。我们做了世界上第一次使用神经网络对托卡马克等离子体进行实时反馈控制。

但那是一个模拟器的绝佳例子。我们可以获得 5 到 6 个数量级的加速，不是通过直接求解方程来进行反馈控制，而是通过使用数值求解器生成训练数据，并使用训练数据来训练模拟器，然后使用模拟器。即使那样，对于当时的硅片来说仍然要求很高。没有足够快的处理器。所以我们实际上构建了一个神经网络的物理实现，信不信由你。所以它是一个混合的模拟-数字系统。它有一个模拟信号通路，带有模拟 S 型（sigmoidal）单元，但权重是用数字设置的电阻器来设定的。所以我们可以获取模拟器的数值输出，将其下载到这个定制的硬件物理神经网络中，并进行实时反馈控制。所以对那个项目感到非常兴奋。

**问（Tim/主持人）:** 这太迷人了。你现在对控制有什么看法？你对，比如说，模型预测控制（model predictive control）有什么意见吗？

**答（Chris Bishop）:** 控制是一个极其重要的领域。控制问题和整体的规划（planning）问题都非常重要。我认为，尽管 GPT-4 取得了所有非凡的进步，实体 AI（instantiated AI）和机器人学等领域仍然是一个非常非常广阔的前沿。我们还没有真正拥有能够，甚至还不能，在伦敦市中心驾驶汽车的机器人。这仍然是一个重大的挑战，尽管我们最近看到了一些非常显著的进展。

**问（Tim/主持人）:** 是的。更广泛地说，我一直在和一些神经科学家交流，他们说我们头脑中就有“矩阵”（matrix），所以我们总是在运行模拟。据推测，未来这将是构建智能体的一种有原则的方式。所以智能体会运行反事实模拟（counterfactual simulations），选择看起来不错的轨迹，然后这个过程会迭代。

**答（Chris Bishop）:** 我认为这非常强大。我的意思是，那种所谓的第一类思维和第二类思维（type one and type two），快思考和慢学习（fast learning, slow learning）的想法，我们模拟世界并将模拟与现实进行比较，我们可以从我们自己的模拟器中学习等等。我们不太清楚如何最好地利用它，但它感觉是一个如此强大和引人入胜的概念。我们认为类似的事情正在大脑中发生。这再次感觉是一个适合探索的领域。我认为以某种形式，某种模型预测和世界模拟，感觉将日益成为未来 AI 系统的一部分。

## 七、 总结

**答（Chris Bishop）:** 对我来说，所有这一切的启示就是，现在身处这个领域是多么美好的时代。有这么多引人入胜的事情可以去做。

**问（Tim/主持人）:** Bishop 教授，能邀请您来到 MLST 是我们的荣幸。非常感谢您。

**答（Chris Bishop）:** 谢谢你，我很享受这次谈话。谢谢。

---


## 要点回顾

**一、 Chris Bishop 教授介绍**

-   **身份与成就:**
    -   微软研究院 AI4Science 技术院士兼总监 (剑桥)。
    -   爱丁堡大学计算机科学名誉教授，剑桥大学达尔文学院院士。
    -   英国皇家工程院院士 (2004)，爱丁堡皇家学会院士 (2007)，皇家学会院士 (2017)。
    -   英国 AI 委员会创始成员，曾任首相科学技术委员会成员 (2019)。
-   **学术背景:**
    -   牛津大学物理学学士。
    -   爱丁堡大学理论物理学博士（研究量子场论）。
-   **对机器学习领域的贡献:**
    -   撰写了领域内开创性的教科书《Pattern Recognition and Machine Learning》(PRML)。
    -   更早撰写了备受赞誉的教科书《Neural Networks for Pattern Recognition》，推动了神经网络作为模式识别工具的应用。
    -   PRML 对推动领域走向更概率化的视角起到了重要作用。

**二、 新书《Deep Learning: Foundations and Concepts》介绍**

-   **合著者:** 与儿子 Hugh Bishop 合著。
-   **目标:** 提供对快速发展的深度学习领域的关键思想和技术的全面理解，覆盖基础概念和最新进展。
-   **内容特点:**
    -   旨在提炼核心概念，筛选出能经受时间考验的技术和思想。
    -   强调基础，如概率论、基于梯度的方法，并将它们带入现代深度学习背景。
    -   包含对卷积神经网络 (CNN) 的深入解释，不仅说明如何构建，更阐述其动机 (Why)。
    -   包含对 Transformer 等重要架构的图解（例如书中 GPT 架构图）。
-   **编写背景与过程:**
    -   源于更新 PRML 的想法，特别是在 COVID-19 封锁期间与儿子合作启动。
    -   很快意识到需要一本全新的书，而非仅仅在 PRML 中增加章节。
    -   重点在于“提炼”而非“堆砌”，省略内容与包含内容同等重要。
    -   避免追逐可能很快过时的“热门”架构，力求基础性和持久价值。
    -   在 ChatGPT 引发公众对 AI 的广泛关注后，加速完成并在 NeurIPS 2023 发布。
-   **物理特性:**
    -   注重高品质制作，采用“缝合装订” (stitch signatures)，使书籍可以平摊，易于阅读且耐用。
-   **可能的遗漏:**
    -   目前未包含强化学习 (Reinforcement Learning)，未来可能考虑在第二版中加入，并与全书内容整合。
-   **个人偏好章节:**
    -   喜欢扩散模型 (Diffusion Models) 和 Transformer 章节，以及整合不同生成框架（GANs, VAEs, Normalizing Flows）的部分。
    -   喜欢 CNN 章节（由 Hugh 贡献较多）。

**三、 Chris Bishop 的职业生涯与观点演变**

-   **早期影响:** 青少年时期受电影《2001太空漫游》启发，对抽象的 AI 概念产生兴趣，但对当时基于规则的 AI 感到失望。
-   **研究转向:**
    -   完成理论物理博士后，转向更实用的核聚变研究（托卡马克等离子体物理）。
    -   受 Geoff Hinton 反向传播论文启发，看到实现智能的不同路径。
    -   开始将神经网络应用于核聚变项目的大数据分析。
    -   最终放弃理论物理生涯，全身心投入当时尚非主流的神经网络领域。
-   **职业生涯融合:** 近期将早期物理背景与机器学习结合，专注于 AI 在自然科学（包括物理学）中的应用 (AI4Science)。

**四、 AI/ML 关键概念探讨**

-   **连接主义 vs. 符号主义:**
    -   历史上关于结合神经网络（连接主义）和传统符号方法有很多讨论。
    -   观察到现代大型模型（如 GPT-4）已能展现出一定程度的符号式、抽象推理能力。
    -   倾向于认为神经网络本身可能像人脑一样，是能够执行不同类型推理的统一基底，而非需要明确结合两种范式。关注点在于扩展神经网络的能力（“yet” - 尚未实现，但非不能）。
-   **贝叶斯方法:**
    -   理论上是机器学习的基石（基于概率论的统一应用），提供重要的指导原则 ("North Star")。
    -   实践中，完全的贝叶斯推断（如对所有参数进行积分/边缘化 $\int p(\theta|D) p(y|x,\theta) d\theta$）通常计算成本过高。
    -   面临权衡：是在小型网络上进行彻底的贝叶斯边缘化，还是用相同计算预算训练一个更大的网络（后者在数据充足时通常更有效）。
    -   实际应用中（尤其在深度学习主流），更倾向于点估计（如 $\theta_{MAP}$, $\theta_{MLE}$）和随机梯度下降 (SGD)。
    -   集成学习 (Ensemble) 可视为对贝叶斯边缘化的粗略近似，通常优于单一模型，但在现代大模型背景下，训练单一巨大模型可能比集成多个小模型更优。
    -   学生仍需学习贝叶斯思想的基础，因为它基于概率论，是理解不确定性的关键。
-   **大型模型（如 LLM）的特性:**
    -   倾向于将其视为单一模型，能力分布在整个网络中，而非多个独立模型的集合（除非显式设计为混合专家）。
    -   展现出惊人的通用能力，能执行多种任务（写诗、编程、解释笑话等）。
    -   通用大模型往往优于专门针对特定任务训练的模型（例如，通用 LLM 在代码生成上可能优于只见过代码的模型）。
    -   这种通用性可能源于模型在压缩大量不同类型数据（代码、科学论文、维基百科等）时学到了更深层次的模式或推理能力。
    -   引发对“没有免费午餐”定理的思考，但实践表明通用性非常强大。
-   **AI 的创造力:**
    -   对“AI 是否具有创造力”持开放态度。
    -   类比人类学习：人类艺术家和科学家也建立在先前的知识和他人工作之上，但这不否定他们的创造力。
    -   质疑对 AI 和人类采用双重标准（如称 AI 为“随机鹦鹉”，但认可高分学生）。
    -   认为现代 AI（如 GPT-4）展现了“人工智能的火花”(Sparks of AGI)，是迈向真正 AI 的第一步。
    -   AI 可作为“认知放大器”或“副驾驶”，增强人类的创造力，人类在过程中扮演引导、选择的关键角色。
    -   AI 的某些“缺乏创意”可能是设计选择（如通过 RLHF 进行对齐，限制其行为以确保安全和有益）。
-   **概率论的统一力量:**
    -   概率论是理解和统一不同 ML 技术的强大框架。
    -   例子：隐马尔可夫模型 (HMM) 和卡尔曼滤波器 (Kalman Filter) 本质上是相同算法，都可以从基本的概率法则（和/积法则）和图模型（因子分解）简洁地推导出来，体现了数学原理的优雅和力量。

**五、 AI for Science (AI4Science)**

-   **动机与重要性:**
    -   科学发现是人类进步的根本驱动力（改善生活、延长寿命等）。
    -   AI/ML 有潜力极大地加速科学发现，被认为是最重要的应用领域之一。
    -   ML 模型作为模拟器（仿真器）的能力是关键突破点，可将复杂模拟加速数个数量级（例如 1000 倍），实现以前不可能的研究。
-   **AI4Science 计划:**
    -   微软研究院设立的专注于此领域的团队，由 Bishop 领导。
    -   团队跨国、跨学科，汇集人才共同推动 AI 在科学发现中的应用。
-   **归纳偏置 (Inductive Priors) 在科学中的作用:**
    -   与 Rich Sutton 的“惨痛教训” (The Bitter Lesson) 观点（通用计算和数据最终胜过人为设计的知识/偏置）形成对比。
    -   认为科学领域可能是“惨痛教训”的一个例外，归纳偏置将持续重要。原因：
        -   科学中的先验知识通常非常严谨（如能量守恒、动量守恒、对称性，如旋转不变性 $E(\text{Rot}(x)) = E(x)$），基于深刻的物理定律，而非启发式规则。
        -   科学研究常常面临数据稀缺（实验或模拟成本高昂）。
    -   在数据有限而先验知识强大的情况下，将先验知识（如物理定律、对称性）融入模型至关重要，也使该领域充满智力挑战和创造性。
-   **应用实例：药物发现 (Drug Discovery)**
    -   目标：寻找能与特定蛋白质靶点结合的小分子药物，同时满足毒性、吸收、代谢等多重标准。
    -   挑战：巨大的化学空间（约 $10^{60}$ 种潜在分子）。
    -   AI 加速：通过计算生成和筛选候选分子，减少对湿实验的依赖。
    -   具体案例（结核病药物）：
        -   构建分子语言模型（基于 SMILES 字符串的 Transformer）。
        -   使用考虑几何结构和等变性 (Equivariance) 的模型编码蛋白质口袋。
        -   结合 VAE 表示已知结合分子，并通过交叉注意力机制整合信息。
        -   迭代优化生成候选分子，显著提高与结核病靶点的结合效率（提高两个数量级）。
        -   与 GEDIT 等机构合作进行湿实验验证。
-   **应用实例：模拟器 (Emulator) - Tokamak 控制**
    -   问题：需要实时控制核聚变装置 (Tokamak) 中等离子体的形状。
    -   挑战：精确的物理模型（Grad-Shafranov 方程）计算太慢，无法满足实时反馈要求（数量级差异）。
    -   解决方案：
        -   使用慢速模拟器生成大量训练数据（磁测量值 $\rightarrow$ 等离子体形状）。
        -   训练神经网络（模拟器）来学习这个映射。
        -   该神经网络速度足够快，可用于实时反馈控制（甚至在当时需要定制硬件实现）。
        -   实现了 5-6 个数量级的加速。

**六、 深度学习的开放性问题与挑战**

-   **深度学习为何有效:**
    -   基本层面：可视为高维空间中的非线性函数拟合/曲线拟合。
    -   深层疑问：为何它们效果如此之好？尤其是在模型极度过参数化（参数远多于数据点）的情况下仍能很好地泛化，甚至在训练误差降为零后测试误差还能继续下降 (Grokking 现象)。
    -   随机梯度下降 (SGD) 的过程本身似乎起到了隐式的正则化作用，引导模型走向具有良好泛化性的解。
    -   需要能做出预测的理论，而不仅仅是事后解释。
-   **模型的可解释性、偏见与安全:**
    -   模型（尤其是大型模型）具有一定的“不可解读性”。
    -   确保 AI 系统的公平性、无偏见和安全性至关重要。
    -   需要对齐技术（如 RLHF）、外部护栏和持续的风险评估。
-   **Transformer 架构的未来:**
    -   目前占据主导地位，非常成功且潜力未尽。
    -   但不太可能是深度学习的终极架构，未来可能会有新的、更优或计算效率更高的架构出现。
-   **控制与规划:**
    -   实体 AI (Instantiated AI) 和机器人学仍是巨大的前沿领域。
    -   基于模型的方法，如模型预测控制 (MPC) 和在智能体内部运行模拟（反事实推断）是重要的研究方向。
-   **科学知识的基础性:**
    -   我们当前的物理定律是对现实的极佳描述和近似，但科学总是在不断发展，揭示更深层次的现实（如相对论之于牛顿力学，暗物质/暗能量的存在）。
    -   科学方法（预测、实验验证）是检验新理论（如宇宙更深层结构）的标准。
    -   人类认知可能存在边界，宇宙可能远比我们能直观理解的更奇特（如量子物理）。数学是描述宇宙的精确语言。

**七、 总结**

-   Chris Bishop 教授认为当前是投身 AI/ML 领域的激动人心的时代，充满了有趣的挑战和机遇。