
# Gilbert Strang访谈「JuliaCon2018」

- 视频链接：[A Conversation With Gilbert Strang | JuliaCon 2018](https://www.youtube.com/watch?v=gGYcSjrqbjc)
- 官方频道：[The Julia Programming Language](https://www.youtube.com/@TheJuliaLanguage)

### 讲座介绍

本篇内容整理自 2018 年 JuliaCon 大会上的一场特别访谈，由麻省理工学院教授 Alan Edelman 主持，与同校的传奇数学家 Gilbert Strang 教授进行了一场深入的“炉边谈话”。这次对话并非传统的学术报告，而是以一种轻松、交流的形式展开，探讨了数学、计算与教育等多个核心议题。

访谈的核心围绕着线性代数这一学科展开。Strang 教授分享了他对于线性代数在现代科学技术，特别是计算和数据科学领域重要性急剧上升的观察与思考。他详细介绍了他当时正在撰写的新书及开设的新课程——《线性代数与从数据中学习》，阐述了其涵盖的关键内容，如张量、随机线性代数、以及深度学习中核心的优化算法（如随机梯度下降及其理论挑战）。对话进一步延伸至机器学习与传统物理建模（如偏微分方程）交叉融合的前沿思考。

除了学科前沿，访谈还触及了教育与知识传播的话题，包括 Strang 教授通过 MIT OpenCourseWare 产生巨大影响的在线课程、他对教学和教科书未来形态的看法。同时，内容也涉及了对 Julia 语言的看法、矩阵结构的计算意义、矩阵乘法复杂度等具体计算问题。

### 内容纲要

```
A Conversation With Gilbert Strang | JuliaCon 2018
├── 引言与背景介绍 (Introduction & Background)
│   ├── 主持人开场与活动说明 (Host Opening & Event Details)
│   ├── Alan Edelman 介绍 Gilbert Strang (Alan Edelman Introduces Gilbert Strang)
│   │   ├── Strang 的学术背景与影响力 (Strang's Academic Background & Influence)
│   │   ├── 提及 OCW 的巨大浏览量 (Mention of OCW's High Viewership)
│   │   └── 提及 Julia 论文引用 Strang 的内容 (Mention of Julia Paper Citing Strang)
│   └── Strang 的新书预览与致意 (Preview of Strang's New Book & Greetings)
├── 线性代数的演变与核心地位 (Evolution & Core Status of Linear Algebra)
│   ├── 线性代数重要性的爆炸性增长 (Explosive Growth of LA's Importance)
│   ├── 计算在现代线性代数教学中的核心作用 (Core Role of Computation in Modern LA Teaching)
│   │   └── 矩阵分解的中心地位 (Centrality of Matrix Factorization - LU, SVD)
│   └── 核心理论举例：行秩等于列秩 (Example Core Theory: Row Rank = Column Rank)
├── 新前沿：线性代数与数据科学 (New Frontier: LA & Data Science)
│   ├── 新书与新课程介绍 ("Linear Algebra and Learning from Data")
│   │   ├── 课程起源与合作 (Origin & Collaboration)
│   │   ├── OCW 视频发布计划 (OCW Video Release Plan)
│   │   └── 课程核心内容 (Core Course Content - LA, DL, Optimization, Stats)
│   └── 涵盖的新兴主题 (Covered Emerging Topics)
│       ├── 张量与非负矩阵 (Tensors & Non-negative Matrices)
│       ├── 随机线性代数 (Randomized Linear Algebra for Large Matrices)
│       ├── 低秩近似与压缩感知 (Low-Rank Approximation & Compressed Sensing)
│       └── 卷积神经网络 (Convolutional Neural Networks - Added Later)
├── 机器学习中的优化挑战 (Optimization Challenges in ML)
│   ├── 核心算法：梯度下降与随机梯度下降 (Core Algorithms: GD & SGD)
│   ├── Adam 等自适应方法的讨论 (Discussion on Adaptive Methods like Adam)
│   │   └── 收敛速度 vs. 解的泛化能力 (Convergence Speed vs. Solution Generalization)
│   └── SGD 泛化能力背后的深层问题 (Deep Question Behind SGD's Generalization Ability)
├── 交叉领域：数据科学与物理建模 (Intersection: Data Science & Physical Modeling)
│   ├── 两个世界的对比：数据驱动 vs. 物理定律 (Contrast: Data-Driven vs. Physics Laws)
│   ├── 探索结合的可能性 (Exploring Combination Possibilities)
│   │   └── 使用学习到的函数作为有限元试探函数? (Using Learned Functions as FEM Trial Functions?)
│   └── 潜在结合点：湍流等复杂问题 (Potential Intersection: Turbulence, etc.)
├── 教学、传播与未来 (Teaching, Dissemination & Future)
│   ├── 在线教育的影响 (Impact of Online Education - OCW)
│   │   ├── 全球范围的触达 (Global Reach)
│   │   └── 对课堂出勤率的潜在影响 (Potential Impact on Class Attendance)
│   ├── 教学理念：清晰传达核心思想 (Teaching Philosophy: Clear Communication of Core Ideas)
│   ├── 教科书的未来与价值 (Future & Value of Textbooks)
│   └── 应用数学的未来展望 (Future Outlook for Applied Mathematics)
│       └── 新思想的持续涌现 (Continuous Emergence of New Ideas - e.g., FEM)
├── 计算视角与具体算法 (Computational Perspective & Algorithms)
│   ├── 对 Julia 的初步印象与体验 (Initial Impression & Experience with Julia)
│   ├── 矩阵乘法的不同视角 (Different Perspectives on Matrix Multiplication - Col x Row)
│   ├── 矩阵结构的重要性 (Importance of Matrix Structure)
│   │   ├── 稀疏 vs. 结构化 (Sparse vs. Structured)
│   │   └── 托普利兹矩阵及其计算效率 (Toeplitz Matrices & Computational Efficiency)
│   ├── 矩阵乘法的计算复杂度讨论 ($O(n^3)$ vs. $O(n^{2.3...})$ vs. $O(n^{2+\epsilon})$?)
│   └── 特定算法的理论理解：交替优化矩阵分解 (Theoretical Understanding of Specific Algorithms: Alternating Optimization for Matrix Factorization)
└── 个人经历与感悟 (Personal Experiences & Reflections)
    ├── 写作习惯与过程 (Writing Habits & Process)
    ├── 提及 John Nash (Mentioning John Nash)
    ├── 早年有限元方法研究经历 (Early Experience with Finite Element Method Research)
    └── 待人处世的哲学：谦逊与回馈 (Philosophy of Interaction: Humility & Paying Forward)
```


---

## 吉尔伯特·斯特朗访谈录 | JuliaCon 2018

**访谈者：** Alan Edelman
**受访者：** Gilbert Strang

### 引言与背景

**主持人 (Bogumił Kamiński):** ...快速提醒一下，我们今天天气看起来不错，所以我们将在午餐休息开始时，在前面那栋漂亮的主楼——UCL 的 Wilkins 大楼前的空地拍合影。就在罗伯茨大楼（就是你注册的那个主入口）外面集合，我们会带大家过去。这里有点绕，可能会迷路... 我们已经直播了吗？我的天，互联网的朋友们大家好！好了，我想 Alan 你来介绍吧。我们今天有个非常特别的环节，以前从未做过这样的事。在英国，我们称之为“炉边谈话”。虽然没有炉火，但会有一些谈话。Alan，请开始吧。

**Alan:** 我想说的是，你们中的许多人也会关注，希望是所有人。你们不必等到最后——也许这个群体里没人会等到最后——但我们不必等到最后才提问。介绍 Gil Strang，我不确定他是否还需要介绍。他的网站（OCW）浏览量接近三百万——也许还差一点点，但三百万的浏览量，我想全世界都知道 Gil Strang 了。至少，我得说，大约一个月前我在 YouTube 上发了点东西，看到点击量达到 2000 次时我非常兴奋，觉得格式真棒，然后我看到了这三百万次…… 这些 OCW 讲座——这是 MIT 对在线课程的说法——的奇妙之处在于，它们如何将线性代数带到了世界偏远的角落。有很多精彩的故事，也许 Gil 会给我们讲一些。他当然是 MIT 的教授。我第一次见到 Gil 是 33 年前，当时我是一年级研究生，听了他的线性代数课……

我还打开了其他几个标签页，只是想展示一下 Gil 的其他一些方面…… 比如昨天的 Keynote 演讲者 Nick Trefethen，他是 SIAM（工业与应用数学学会）的前任主席。Gil 当然是现任 SIAM 主席。你们都听说过暹罗国王的故事…… 这将是 Julia 相关的内容，这是从 Julia 论文里来的，但我们是“无耻地”从 Gil 的网站上“偷”来的。这对我来说不是奴隶矩阵（slavery matrix），如果你输入这个矩阵类型，你会得到 `cupcakes` 反斜杠 `that's right`。

这远非第一次与 Gil 的对话。Nick 好心地告诉我一次可以追溯到很久以前的对话，我想还有另一次……

也许最后一个标签页，我希望 Gil 能稍微谈谈。这是他最新的书和课程，我也稍微帮了点忙，看到很多很多学生喜欢这门课：《线性代数与从数据中学习》。我快速滚动一下，看，这里有个漂亮的神经网络，然后滚动到目录…… 那么，也许现在可以邀请 Gil 了。

**(掌声)**

**Gil:** 好的。最新的就是这本了。我想就你提到的 18.06 课程（MIT 线性代数课程代号）的三百万浏览量说点什么。我只是被告知了右边那些数字的含义，我想知道那三百个是谁？

### 线性代数的演变与核心地位

**Alan:** Gil，你能不能谈谈线性代数从一开始就在计算数学中被使用？你是否想谈谈，尽管很多教职员工可能还不知道，但学生们已经意识到了？

**Gil:** 是的。而且，真正让线性代数与众不同的一点是你可以进行计算。你知道，对于微积分…… 嗯，答案总是 $2\pi$ 或者零…… 但是线性代数，教学的一个自然部分就是计算。你可能从解线性方程组和 `LU` 分解等开始。实际上，认识到矩阵分解是描述算法的优美方式，这一点在早期并不广为人知。当然，现在矩阵分解已经内置于 Julia 中，是其主要部分。

**Alan:** 说到线性代数中的计算，我想在小学里，关于手算长除法的争论已经失败了。思考一下在大学线性代数教学中类似的问题，手算与依赖软件（比如 Julia）的平衡点在哪里？

**Gil:** 这很大程度上取决于教授。因为我很遗憾地看到，在很多大学，如果线性代数是一门大课——现在通常是这样——那就意味着它是由助教 (TA) 来教的。在很多地方，它没有得到专业人士的关注，无法展现线性代数真正的潜力。我不知道你是否记得，两年前，18.06 课程由不同的人来教。一位拓扑学家教了这门课。他拿过教学大纲，删掉了每一个应用，比如最小二乘法，然后加入了“同态”(morphisms)。总之，在数学系内部，这仍然是一场斗争。计算机科学系当然…… 我的印象是，那个系再也没有让他教过这门课了。他是一位好老师，优秀的老师，但……

我是否应该谈谈…… 那门新课程？

**Alan:** 是的，请讲。

**Gil:** 嗯，它从我们熟知的线性代数主题开始，但更侧重于构造性的线性代数。我还在写其中的一部分……

我想提一下证明。线性代数中第一个不那么明显的定理是：一个矩阵的独立列的数量等于独立行的数量，即行秩等于列秩。对于一个大矩阵来说，这不是一眼就能看出来的。我对这个定理的一个新证明感到挺满意，写在了新书的第四页。

既然你提到了证明，我能说一点关于矩阵分解的数学吗？即使是 SVD（奇异值分解）。一个非常受欢迎的问题是，如果你试图将矩阵 `A` 分解为 `B` 乘以 `C` 的形式，比如非负矩阵分解或其他分解，你如何找到 `B` 和 `C`？

自然的算法是：先猜测一个 `B`，然后找到最优的 `C`——这会是一个线性问题，因为 `C` 是唯一的未知矩阵。然后，用这个 `C` 去找到最优的 `B`。就这样在 `B` 和 `C` 之间交替进行，每一步都在自然范数下尽可能接近 `A`。这个算法，随处可见，但在数学上还没有被完全理解。我不认为它的收敛性得到了任何保证。它什么时候收敛？收敛到什么？这是一个你可以说纯粹的数学问题。也许在实际问题中，你就直接用它，看看效果如何。但有趣的是，对于这样一个关键的数值算法，我们对其何时有效还没有很好的理解。

我想强调的是，我不是一个疯狂追求证明的人。但是理解像向量空间这样的基本概念，看清基本思想是很重要的。

### 新前沿：线性代数与数据科学

**Alan:** 你能谈谈你的新书吗？

**Gil:** 可以。两年前，Alan 的学生 Raj Rao——他现在在密歇根大学任教，并且开设了一门关于计算思维的成功课程——正好有学术休假。他提议来 MIT 开设一门新课程。于是 Raj Rao、Alan 和我三人共同教授了一门课，学生们真的带着笔记本电脑来上课，并在整个学期中使用 Julia。那真的很有趣，和 Alan、Raj 一起做这件事很有趣。结果很多学生立刻就报名了。所以我们继续开设这门课。今年春天我教了这门课，并且实际上在今年春天录制了视频，我想几个月后就会发布在 OpenCourseWare (OCW) 上。当然，他们需要编辑，而且美国的法律要求提供字幕，这需要一些时间准备。

然后我就想，这很令人兴奋。这门课是线性代数和现在所说的深度学习的结合。我得再说一遍，我是如此幸运，线性代数这门学科——当我学习它并开始教它的时候，实际上 John Nash 很久以前差点在 MIT 教 18.06 的一个班——它的重要性简直是爆炸性增长，我想在座的各位可能都知道。正如我所说，很多数学系的教授，很多人还没意识到线性代数现在是应该掌握的知识。当然，无论我说什么，都只是我的个人观点。不过我说得多了，它就成了我的……嗯，总之就是这样。在制作那些早期视频的时候，我完全没有想到它们会变成什么样，然后线性代数就…… 不，实际上我是在 OpenCourseWare 启动前几个月做的。我最初的想法是，MIT 数学系有一些非常棒的讲座，比如 Gian-Carlo Rota，他是组合数学的先驱，一位超级演讲者。我想，好吧，我来树立一个榜样，展示一下这些，然后让他们也来做。但紧接着 OpenCourseWare 就启动了，很自然地就把视频移到了那个网站上。

这门新课程，始于线性代数，我们熟知的主题，但更侧重于构造性的线性代数…… 比如目录里的 1.12 节，张量 (Tensors)，非负矩阵 (Non-negative matrices)。你们中有多少人接触过张量？这是一个尚未进入通用线性代数课程的主题。如果再往下看一点，是随机线性代数 (Randomized linear algebra)。我们现在都知道，如果矩阵非常大，比如 $10^5 \times 10^5$ 量级，用常规方法计算 SVD 可能不行，但随机方法可以。这是一个主题。

**Alan:** 你是说不是每个人都知道随机线性代数有多重要吗？

**Gil:** 嗯，是的，这需要持续不断地告诉人们。然后是低秩主题 (Low-rank topics)，压缩感知 (Compressed sensing)。但如果你继续看下去，对我来说，机器学习、深度学习是线性代数（占主要部分）和优化、概率统计（占次要部分）的混合体。所以最后那些主题：梯度下降 (Gradient Descent) 和随机梯度下降 (Stochastic Gradient Descent, SGD) 是关键。

我最近意识到，新书中必须有一整节专门讲卷积网络 (Convolutional nets)，因为它们是神经网络中如此成功的一种结构。卷积网络是局部的，参数更少，而且非常成功。

### 机器学习中的优化挑战

**Gil:** 我是不是在说大家都知道的事情？随机梯度下降算法是（深度）学习的“主力军”。实际上，关于自适应方法，比如 Adam 和其他方法，在深度学习领域仍然存在争论。加州大学伯克利分校的 Ben Recht，在我看来是一位真正的超级明星，他发表了一篇论文指出，Adam 系列方法——人们喜欢它们是因为它们在开始时加速了收敛——但他说，如果你一直等下去直到它收敛，它可能不会收敛到一个好的解。

一个重大的问题是：当你有很多参数时，通常会有很多很多可能的解可以收敛到。为什么随机梯度下降会收敛到一个好的解，一个你可以在测试数据上使用并且成功的解？这确实是一个深刻的问题，很难把握。你知道，大多数数学证明，如果你证明了收敛性，OK，你完成了。但在这里，我们想知道它是否收敛到一组能够很好地泛化到新数据的权重。感谢上帝，随机梯度下降的答案是“是的，它确实如此”。

那么问题来了，Adam 及相关方法是否总能保持这种良好的泛化能力？现在看来，可能做一些调整后没问题，但标准的 Adam 方法可能会出错。实际上，是的，他们构建了一些相当现实的例子，在这些例子中他们可以做到这一点，并观察不同方法选择了哪个极限点。结果发现，用 Adam 得到了错误的极限点。这方面的论文出自 Recht 等人。这是一个了不起的领域。

**Alan:** 说到优化，毫无疑问，最近机器学习的发展和普及使其成为了优化的“杀手级应用”。当然，优化本身的应用范围远不止机器学习。你在 MIT 的同事 Dimitris Bertsimas 在商业分析领域做了很多工作，将优化提升到了新的水平，在商学院等领域变得非常流行。我想问你，你一直参与很多物理建模方面的工作。我在 Julia 这个世界里学到了很多，了解到过去仅仅让一个物理模型能够工作就已经足够令人兴奋了。但我听说，现在这已经是老黄历了，人们开始围绕物理模型进行优化，但他们常常受困于软件——直到 Julia 和 JuMP 的出现。你对于优化如何围绕物理学进行变革有什么想法吗？

**Gil:** 实际上，我夏天通常会去牛津访问一个月左右。那是个从一个办公室搬到另一个办公室的好地方，但办公室是不同的，而且我在牛津有公交卡，所以哪儿都去…… 但我和那里的一些应用数学家聊了起来。问题确实是，机器学习、深度学习如何与微分方程联系起来？假设现在它们是两个不同的世界。你会说，对于深度学习，你使用随机梯度下降来找到答案；而对于物理原理，你运用原理，推导出微分方程，然后我们有数值方法来求解它们，这是另一个庞大的科学世界。

但是否存在一些问题——我认为是有的——在这些问题中，你既想处理物理问题，但其背景又带有非常随机的特性？这就为深度学习、或者说梯度下降和优化，提供了一个发挥作用的契机。我不知道你对此有什么想法？

### 交叉领域：数据科学与物理建模

**Gil:** 我读过一些建议，相关的论文也开始出现。比如，你能不能把梯度下降产生的学习函数——一个非常复杂的函数，但它有很多很多参数——能不能用这个函数？它虽然复杂，但它是由简单的层级结构构建起来的：矩阵乘法，然后是 ReLU 这样的非线性操作。你能不能把这些函数用在有限元方法 (FEM) 里？用这种全新的、变量极多的、但具有特殊神经网络结构的函数来构造试探函数，去逼近物理问题的解？

**Alan:** 嗯，麻烦在于它们……

**Gil:** 是的，它们都是……

**Alan:** ……这令人担忧，我们失去了…… 我们没有向量空间。如果我们把这些函数加起来，我们不一定能得到另一个同类函数。所以这是一个问题。但似乎，如果这种学习方式将变得重要，而且我们知道物理定律和它们所导出的偏微分方程 (PDE) 是基础性的，那么这两者是否会相遇？

**Alan:** 湍流 (Turbulence) 会是一个自然的例子，对吧？

**Gil:** 是的，是的。湍流是一个很好的例子。

**(来自观众席的声音，提到 Chris Rackauckas 在研究这个)**

**Gil:** 哦，我们可以记下，Chris 准备给你做一个完整的 Keynote 演讲。哇，那太好了。

### 教学、传播与未来

**Alan:** 你能否谈谈技术对教学的影响？有哪些显著的成功案例？又有哪些值得注意的失败案例？例如，你所看到的，在教学或复杂学科中尝试应用技术的经历？

**Gil:** 嗯，在你开会前写信给我时，你提到了例如在线课程。曾经有过一次尝试，至少我认为这是一个来自我一位老老师的轶事。我想是在七八十年代的瑞典，他们有个想法，要把全国最好的讲座录在录像带上，然后就不再需要现场讲座了。你可以把电视机推进来…… 这听起来很糟糕，这是一个可怕的失败，因为它忽略了教学中的人际互动方面。

我不知道，我只做过真人讲座。一次奇特的经历是在伊斯坦布尔下电车时，刚下车就有人对我说：“谢谢你的讲座。” 这世界真奇妙！

**Alan:** 这是你遇到的最不寻常的地方吗？实际上有人在利用你的资源？在一些你从未预料到的奇怪领域或学校？

**Gil:** 总的来说，这是一段愉快的经历。当然，人们并不真正知道谁在看，为什么看，或者它们有多大用处。

**Alan:** 这里有个好问题。我有时会想，也许没有标准答案。你制作了这些讲座，触达了三百万人。而 MIT 的学生们却待在宿舍里看…… 我相信这里的各位昨天肯定没这么做过。

**Gil:** 是的，舒服的沙发上看…… 不，这确实是个问题。所以 MIT 的课程，我敢肯定，当你教 18.06 时，可能只有不到一半的注册学生在某一天会出现在课堂上。

**Alan:** 哦，是的，没错。

**Gil:** 但对于深度学习那门课，他们会来。这可能因为它更新颖，而且有点难以预测，所以仍然算是一次冒险。也许学生们认为他们不能只看去年的材料，因为他们觉得今年的内容会有所不同。

**Alan:** 你愿意谈谈你写教科书的职业生涯吗？我尝试写书 30 年了，差不多快放弃了。

**Gil:** 我起得早。我通常在清晨写作，大概早上 7 点到 10 点或 11 点。然后我就差不多筋疲力尽了。你必须喜欢这件事。我挺享受尝试的过程，这就像写小说，你真的不知道结构会是怎样。我还在修改目录，比如我刚意识到新书里必须有一整节关于卷积网络的内容，因为它们是如此成功的结构。有时我说我不知道，我最近把新书的两章顺序颠倒了。我不知道最终会怎么样。

你得感谢很多人愿意向你解释一些主题。

**Alan:** 成功的案例呢？

**Gil:** 哦，我的第一本书是关于有限元方法的。有人听说过吗？是的。唯一一本从未出版的书是“第零本”，关于有限元方法的，但它非常混乱。它是关于规则网格上的有限元方法，傅里叶方法在这种情况下占主导地位——规则网格，常系数，这是使用…… (看向窗外) 伦敦眼？哦，不，是在苏黎世的那个。所以那本书…… 最终我认识到，并非所有问题都是常系数、等距网格。有限元方法的全部意义在于它对弯曲区域的适应性非常强。所以最终出版的是那本（经过修正的）书。那次经历改变了我的人生，因为我遇到了很多工程师，有限元工程师。我不知道你是否知道 Zienkiewicz 这个名字，他在威尔士，听起来不像威尔士名字，但他写了主要的工程学著作。这对我来说很激动，因为我接受的教育相当…… 我不知道你是怎么受教育的，但我的本科是纯数学课程。我上过一门代数课，老师 Artin 进来，在黑板上完美地写下每一个定理。

**Alan:** 你对应用数学的未来有何看法？从你的角度来看，未来几年我们最有可能在哪些领域看到主要的进展和突破？

**Gil:** 你知道，令人高兴的是，总会有事情发生。总会有人做些什么，总会有新的、基础性的想法出现，需要新的思维方式。你肯定知道，当我们有了用于偏微分方程 (PDE) 的有限差分法时，可能觉得“就这样了”。然后有限元方法出现了，这完全归功于工程师。我认为有限元这个例子，创建这些大型代码来做有限元计算，是国际合作的一个非常杰出的范例。好的，有限元方法仍在发展，新的思想不断涌现，但之后我们又看到了其他发展，比如有限体积法。所以我认为还会有更多的新思想被发现。当然，像 Julia 这样的工具，看到你如何用它进行计算，现在成为了检验一个想法的标准——计算能为你做什么。

**Alan:** 我觉得我们还有时间再问观众一个问题，然后我妻子…… 我们实际上开始得有点晚，所以我们问两个问题吧。

**(来自观众的问题)** 您在新教学平台和各种新教学技术方面取得了很大成功。您认为书籍，尤其是教科书，在未来以及面对新技术时将扮演什么样的角色？对于每一位作者来说，这都是一个非常严肃的问题：书籍还有未来吗？

**Gil:** 我猜我觉得它们至少有现在。我已经 83 岁左右了，所以我可能不会知道最终答案。但是，是的，我对书籍的态度，和我走进教室讲课时的想法是一样的：我如何能把这个讲清楚？我如何能把它说出来，不是试图涵盖每一个细节，而是让听众或读者能够理解？所以，某种形式的清晰表达将永远伴随着我们。

**(来自观众的问题，关于 SGD 理解的局限性)** 您提到对于随机梯度下降存在一些深层次理解的缺乏。我想知道，在您不缺乏理解的范围内，您是否愿意与我们分享一下？

**Gil:** 不，我提到了 Ben Recht (R-E-C-H-T)。他的所有工作都发布在 arXiv 上，所以很容易看到。我只是在边做边学。我的一个想法是，让数学系能够开设关于这个（数据科学/机器学习）的课程成为可能。我认为需要有一本书，对于那些并非该领域专家的数学教员来说是易于管理的。这是一个如此激动人心的领域，你可以看到结构正在发展——就在我们说话的此刻仍在发展——去寻找最佳的函数族：易于构建，在逼近方面成功…… 是的，我只是在学习它。

在计算机科学系，他们说不带线性代数搞机器学习，就像不带微积分搞物理一样。当然有人这么做，但…… 概率和统计现在扮演着更大、更基础的角色。是的，这是一个奇妙的混合体。

### 计算视角与具体算法

**Alan:** 我如何将一个纯粹的线性代数观点，或者说您对纯粹线性代数的看法，与 Julia 联系起来？

**Gil:** 哦，是的。我对证明并不狂热。但要理解基础概念，比如向量空间，看清基本思想…… 是的。例如，现在我的第一堂课倾向于讲如何做矩阵乘法。因为我总是希望新学期开始时，课程能从一些他们（学生）认为自己知道的东西开始，比如如何计算 $A \times B$，两个矩阵相乘。然后向他们展示一种不同的、有用的方法。具体来说，我认为将两个矩阵相乘，最好的方法是：A 的一列乘以 B 的对应一行（`列 × 行`），这会得到一个秩为 1 的矩阵，一个非常简单的矩阵。然后是第二列乘以第二行，第三列乘以第三行，如此类推，然后把这些秩 1 矩阵加起来。

**Alan:** 我不知道哪种方式是最高效的……

**Gil:** 矩阵乘法是计算机架构与…… 之间一段漫长而密切的故事。它涉及到…… 我通常告诉学生的是：我想你们都相信计算机是为了 Facebook 和电子邮件而发明的。但实际上，它们是特别为矩阵乘法而构建的，因为这是几乎比任何其他操作都更有效率的一种运算。

**Alan:** 你这辈子见过的矩阵肯定不止几个了。Julia 真正让我们能够做到的是针对许多特定的矩阵结构进行优化。如果我们对矩阵了解更多信息，我们就能利用这些信息。但令人惊讶的是，现代编程或现代计算科学利用这些信息的方式实际上非常基础。我们要么说它是稠密的 (dense)，要么说它是带状的 (banded)，也许是块带状的 (block banded)，或者完全是稀疏的 (sparse)。但这中间肯定还有其他结构，对吧？我有点好奇，在你涉足数学的各种领域中，你是否看到了一些模式，让你觉得“哦，也许我们在计算上应该考虑如何利用这些模式”？或者说，我们是否遗漏了什么？

**Gil:** 了解哪些是有用的模式列表是很有趣的。让我就深度学习中的卷积网络问一下。它们确实非常好，因为它们实现了…… 它们保持了局部性，寻找局部属性。我所说的卷积，与信号处理中的滤波器、数学中的托普利兹 (Toeplitz) 矩阵是同一个意思——矩阵的对角线是常数。现在，Alan 可能比我更了解，从数学上讲，这当然是你想要利用的结构，因为你理解这些矩阵。但在计算上，利用这种结构是否划算？

**Alan:** 我认为整个应用数学都在利用结构。

**Gil:** 另一件你可能没见过的事…… 如果你看过那些讲座你就知道…… Gil 有一种扮演无知的惊人能力……

**Alan:** “稀疏”(sparse) 这个词现在很有名，而“结构化”(structured) 这个词却不那么出名。

**Gil:** 我是把托普利兹矩阵，常数对角线，作为一个非常清晰结构的例子。但它不是稀疏的。

也许一个有点难的问题，但你可以猜猜。复杂度理论中最大的主题之一是矩阵乘法的复杂度。那么你的猜测是什么？是 $n^3$ 还是 $2$？现在的指数是 $2$ 点几……

**Alan:** $2 + \epsilon$ 或者更大？

**Gil:** 现在大约是 $2.3$ 几。计算两个 $n \times n$ 矩阵相乘，显而易见的方法需要 $n^3$ 步。但事实证明，找到将指数从 3 降低到 2.6 或 2.7 或更低一点的方法并不太难。但目前它似乎卡在了当前的位置。我不知道，我对此没有任何洞察力。我想每个人都很难想象在达到 $2 + \epsilon$ 之前会有某个神奇的停止点。但很多复杂度理论研究者在这个问题上付出了艰苦的努力。所以，我猜是 $2 + \epsilon$，但我们拭目以待。但是我没说错吧，那个指数已经有一段时间没有变化了？好的。

**Alan:** 你用过多少 Julia？你觉得它怎么样？说实话。

**Gil:** 你想要真相吗？(笑声) 我记得我早期尝试在 JuliaBox 上使用它。它建议…… 反正我看到的时候它建议：“试试 $1+1$”。我看不到吸引力…… 我输入了 $1+1$，然后按了 Enter，结果 $2$ 没有出现。

**Alan:** 是 Shift + Enter。

**Gil:** 那我怎么会知道呢？这就是我的看法。就像我昨天从牛津来这里买火车票，大西部铁路公司说“好的”，我付了钱，结果是错误的车次。这就是我的经历。总之，我已经足够老了，可以说，好吧，就做你能做的。我的角色主要是写作。Julia 是个好伙伴，因为还有其他已经有二三十年历史的软件，你也曾努力过……

### 个人经历与感悟

**Gil:** 我稍微补充一下关于 Nash 的故事。你知道，他在 MIT 的那段日子非常艰难，情况每况愈下。所以他从未真正教过我们计划的那门线性代数课程。但当然，我们知道他后来奇迹般地康复了，并度过了许多美好的岁月。实际上，我不是研究博弈论的，但我会用两人零和博弈作为例子，因为冯·诺依曼、丹齐格等人都证明了这是一个优美的矩阵定理。所以，对我来说，两人零和博弈一直是一个例子。但 Nash 真正做的是 N 人博弈，那是无限困难的。

我应该在 2019 年 6 月之前完成这本书的初稿。

**Alan:** 你的读者是谁？

**Gil:** 关于我们开头提到的这本书，我带了 50 份目录，我会放在某个地方。但也许在线上，你可以查看的网站叫做 `math.mit.edu/learningfromdata`。那就是这本书的网站。`math.mit.edu/learningfromdata`。

**Alan:** 我妻子提出一个非常特别的问题…… 我在离开前跟我妻子提过，我将有这个机会…… 我妻子非常了解 Gil…… 这个问题一直在我脑海里。这真的是我妻子的话，但我也这么认为：你真的有一种天赋，能让你周围的人感到自己很重要。你拥有那种谦逊和优雅…… 我不得不说，我认为 MIT 的教授通常是按相反标准选拔的。

**Gil:** 我想我没有什么答案。我的意思是，我感受到的是很多人肯定都感受到的：在成长过程中，有那么多人帮助过我。所以我很自然地就会转过身去帮助下一个人。你知道，“把爱传下去”(pay forward)。所以感谢 Susan 提出这么好的问题，也感谢大家。

**(掌声)**


---

# 要点回顾

**框架：**

1.  **引言与背景**: 介绍 Gilbert Strang 教授及其影响力。
2.  **线性代数的演变与核心地位**: 探讨线性代数重要性的提升及其与计算的关系。
3.  **新前沿：线性代数与数据科学**: 聚焦 Strang 的新课程/书籍，及其涵盖的现代主题。
4.  **机器学习中的优化挑战**: 讨论 SGD、Adam 等优化算法及其理论问题。
5.  **交叉领域：数据科学与物理建模**: 探索机器学习与传统科学计算（如微分方程、有限元）的结合潜力。
6.  **教学、传播与未来**: 关于在线教育、教科书、教学理念及应用数学前景的思考。
7.  **计算视角与具体算法**: 涉及 Julia、矩阵结构、计算复杂度和特定算法（如矩阵分解）。
8.  **个人经历与感悟**: 分享写作、早期研究经历及个人哲学。

**要点内容：**

-   **引言与背景**
    -   Gilbert Strang 是 MIT 的数学教授，因其线上开放课程 (OCW) 中的线性代数讲座而享誉全球，观看次数达数百万 (2:37, 6:12)。
    -   他是工业与应用数学学会 (SIAM) 的前任主席 (4:04)。
    -   访谈者 Alan Edelman 提及 Julia 论文中曾引用 Strang 网站上的矩阵示例 (4:15)。

-   **线性代数的演变与核心地位**
    -   线性代数的重要性在现代计算和数据科学领域急剧增长 (9:08-9:16)。
    -   Strang 认为，许多数学系的教授可能还未充分认识到线性代数的当前重要性 (9:26-9:39)。
    -   线性代数的特别之处在于它可以进行实际的计算 (11:22-11:27)。
    -   矩阵分解（如 `LU` 分解、SVD）是理解线性代数的优美方式，也是现代计算的核心 (11:48-12:07, 33:54)。
    -   一个基础但重要的理论：矩阵的行秩等于列秩 (33:20-33:27)。

-   **新前沿：线性代数与从数据中学习**
    -   Strang 正在撰写一本新书并开设了一门新课程，名为“线性代数与从数据中学习” (5:20-5:40, 7:03-8:09)。
    -   这门课程结合了线性代数、深度学习、优化和概率统计 (8:32-8:45, 15:44-15:55)。
    -   课程涵盖的新兴主题包括：张量 (Tensors) (14:33)、非负矩阵 (Non-negative matrices) (14:39)、随机线性代数 (Randomized linear algebra)，尤其适用于大尺度矩阵（如 $10^5 \times 10^5$）(15:04-15:20)、低秩近似 (Low-rank topics) 和压缩感知 (Compressed sensing) (15:36)、以及后来意识到必须加入的卷积神经网络 (Convolutional nets) (29:11-29:17)。

-   **机器学习中的优化挑战**
    -   梯度下降 (Gradient Descent) 和随机梯度下降 (Stochastic Gradient Descent, SGD) 是（深度）学习算法的主力 (16:01-16:19)。
    -   关于 Adam 等自适应优化方法的讨论：它们初期收敛快，但可能不会收敛到泛化能力强的解 (16:25-17:07, 17:56-18:13)。
    -   一个核心问题是：为什么 SGD 在众多参数和可能解的情况下，倾向于找到泛化良好的解？(17:07-17:21, 17:48)。Ben Recht 的研究对此提供了见解 (16:32, 18:19, 50:30)。
    -   理解 SGD 为何有效的深层原因仍是一个挑战 (50:15-50:30)。

-   **交叉领域：数据科学与物理建模**
    -   探讨机器学习/深度学习如何与基于物理的微分方程模型相结合 (20:26-20:32)。
    -   目前这两个领域相对独立，一个依赖数据驱动和优化，另一个依赖物理定律和数值方法 (20:40-21:05)。
    -   思考能否将深度学习训练出的函数（由简单层级结构构成，如矩阵乘法+ReLU激活函数）用作有限元方法 (FEM) 中的试探函数 (21:50-22:30)。
    -   一个挑战是这些学习到的函数集合不构成向量空间 (22:49-23:01)。
    -   湍流 (Turbulence) 被认为是一个可能的结合点 (23:33, 23:51)。

-   **教学、传播与未来**
    -   在线课程 (如 OCW) 极大地扩展了教育的覆盖面，让全球各地的人受益 (2:49, 10:15, 25:35)。
    -   在线资源可能导致学生课堂出勤率下降 (26:41-27:31)，但新颖或不断发展的课程内容（如深度学习）可能仍会吸引学生到场 (27:38-28:03)。
    -   Strang 强调教学的目标是清晰地传达思想，让听众或读者能够理解 (48:52-49:03)。
    -   关于教科书的未来：Strang 认为书籍在当前仍有其价值 (48:20-48:52)。
    -   应用数学的未来：总会有新的基本思想涌现，推动领域发展，就像有限元方法一样 (46:20-47:13)。

-   **计算视角与具体算法**
    -   Strang 对 Julia 的使用经验不多，提到了早期使用 JuliaBox 的一个小插曲 (36:05-37:04)。
    -   提供了一种不同的矩阵乘法视角：`A * B` 可以看作是 A 的列向量乘以 B 的对应行向量得到的秩1矩阵之和 (40:00-40:41)。
    -   讨论了矩阵结构的重要性：稀疏 (Sparse) 结构已广为人知，但结构化 (Structured) 矩阵，如托普利兹矩阵 (Toeplitz，对应卷积操作)，其常数对角线性质在计算上是否总能被有效利用值得探讨 (42:41-43:24, 44:02-44:19)。
    -   矩阵乘法的计算复杂度：理论上已从 $O(n^3)$ 降至约 $O(n^{2.3...})$，Strang 猜测最终可能是 $O(n^{2+\epsilon})$ (44:44-45:52)。
    -   形如 `A = BC` 的矩阵分解问题中常用的交替优化算法（固定 B 解 C，再固定 C 解 B），其收敛性在数学上尚未被完全理解 (34:01-35:25)。

-   **个人经历与感悟**
    -   分享了写作的习惯：通常在早晨写作 (28:35)。
    -   提及了 John Nash 在 MIT 的一段经历 (9:00, 38:11)。
    -   早期关于有限元方法 (FEM) 的书写经历，以及与工程师的交流如何引导他从纯数学走向应用数学 (30:53-32:26)。
    -   Strang 的谦逊与待人方式被提及，他认为帮助他人是回报社会的方式 (“Pay it forward”) (52:15-53:19)。