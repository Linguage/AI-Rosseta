

# 下半场

**核心观点：我们正处于人工智能的中场休息。**

- 原文连接：[The Second Half](https://ysymyth.github.io/The-Second-Half/)
- 作者：[Shunyu Yao](https://ysymyth.github.io/)

几十年来，人工智能的发展主要围绕着开发新的训练方法和模型。这确实取得了成效：从在国际象棋和围棋比赛中击败世界冠军，到在SAT和律师资格考试中超越大多数人类，再到赢得IMO和IOI金牌。在这些载入史册的里程碑——深蓝（DeepBlue）、AlphaGo、GPT-4以及o系列模型——背后，是人工智能方法的根本性创新：搜索、深度强化学习（deep RL）、规模化（scaling）和推理（reasoning）。随着时间的推移，一切都在变得更好。

那么，现在突然发生了什么变化？

三个词概括：强化学习（RL）终于奏效了。更准确地说：强化学习终于具备了泛化能力。在经历了数次重大的弯路和一系列里程碑式的积累之后，我们终于找到了一种行之有效的配方，能够利用语言和推理来解决各种各样的强化学习任务。哪怕就在一年前，如果你告诉大多数人工智能研究者，单一的配方就能处理软件工程、创意写作、IMO级别的数学、键鼠操作以及长篇问答——他们会嘲笑你在痴人说梦。这些任务中的每一项都极其困难，许多研究者整个博士生涯都只专注于其中一个狭窄的领域。

然而，这确实发生了。

那么接下来会发生什么？人工智能的下半场——从现在开始——将把焦点从**解决问题**转向**定义问题**。在这个新时代，**评估**变得比**训练**更重要。我们不再仅仅问：“我们能否训练一个模型来解决X问题？”，而是问：“我们应该训练人工智能去做什么？以及我们如何衡量真正的进展？” 要在下半场取得成功，我们需要及时转变思维模式和技能组合，也许更接近于一个产品经理。

## 上半场

要理解上半场，看看它的赢家就知道了。你认为迄今为止最具影响力的人工智能论文是哪些？

我试着回答了斯坦福224N课程里的这个问题，答案并不令人意外：Transformer、AlexNet、GPT-3等等。这些论文有什么共同点？它们提出了一些根本性的突破，用以训练出更好的模型。而且，它们都通过在某些基准（benchmarks）上展示出（显著的）改进而成功发表。

不过，还有一个潜在的共同点：这些“赢家”都是**训练方法或模型**，而不是**基准或任务**。即使是 arguably 最具影响力的基准 ImageNet，其引用量也不到 AlexNet 的三分之一。方法与基准的对比在其他地方甚至更为悬殊——例如，Transformer 的主要基准是 WMT’14，其研讨会报告的引用量约为1300次，而 Transformer 的引用量超过了16万次。

![上半场](https://ysymyth.github.io/images/second_half/first_half.png)

这描绘了上半场的游戏规则：专注于构建新的模型和方法，而评估和基准是次要的（尽管对于论文发表体系的运作是必要的）。

为什么会这样？一个重要原因是，在人工智能的上半场，**方法比任务更难、更令人兴奋**。从零开始创建一个新的算法或模型架构——想想反向传播算法、卷积网络（AlexNet）或 GPT-3 中使用的 Transformer 等突破——需要非凡的洞察力和工程能力。相比之下，为人工智能定义任务通常感觉更直接：我们只需将人类已经在做的任务（如翻译、图像识别或下棋）转化为基准。这并不需要太多的洞察力，甚至工程量也不大。

方法也往往比单个任务更通用、适用范围更广，这使得它们尤为宝贵。例如，Transformer 架构最终推动了计算机视觉（CV）、自然语言处理（NLP）、强化学习（RL）等许多领域的进步——远远超出了它最初证明自己的那个单一数据集（WMT’14 翻译）。一个优秀的、简洁且通用的新方法可以在许多不同的基准上取得进展（hillclimb），因此其影响往往超越单个任务。

这场游戏已经持续了几十年，催生了改变世界的想法和突破，这些都体现在各个领域基准性能的不断提升上。那为什么游戏规则会发生改变呢？因为这些想法和突破的积累，在**创造一个解决任务的有效配方**方面，产生了质的变化。

## 配方

这个配方是什么？不出所料，它的成分包括：大规模语言预训练、规模（数据和计算能力），以及推理和行动（reasoning and acting）的思想。这些听起来可能像是你在旧金山每天都能听到的流行词，但为什么称它们为“配方”？

我们可以通过**强化学习（RL）**的视角来理解这一点。RL 常被认为是人工智能的“终局之战”——毕竟，理论上 RL 保证能赢得游戏，而经验上也难以想象任何超人系统（如 AlphaGo）没有 RL 的参与。

在 RL 中，有三个关键组成部分：**算法（algorithm）、环境（environment）和先验（priors）**。很长一段时间里，RL 研究者主要关注算法（例如 REINFORCE、DQN、TD-learning、actor-critic、PPO、TRPO……）——即智能体学习方式的智力核心——而将环境和先验视为固定的或最简化的。例如，Sutton 和 Barto 的经典教科书几乎全是关于算法，而很少涉及环境或先验。

![RL 教科书](https://ysymyth.github.io/images/second_half/rl_book.png)

然而，在深度强化学习时代，环境在经验上变得非常重要：一个算法的性能往往高度依赖于其开发和测试的环境。如果你忽略环境，你可能会构建出一个只在玩具环境中表现出色的“最优”算法。那么，为什么我们不先弄清楚我们真正想要解决的环境，然后再找到最适合该环境的算法呢？

这正是 OpenAI 最初的计划。它构建了 [gym](https://openai.com/index/openai-gym-beta/)，一个包含各种游戏的标准 RL 环境，然后是 [World of Bits 和 Universe 项目](https://openai.com/index/universe/)，试图将互联网或计算机变成一个游戏。这计划听起来不错，对吧？一旦我们将所有数字世界都变成环境，用智能 RL 算法解决它，我们就拥有了数字通用人工智能（digital AGI）。

计划虽好，但并不完全奏效。OpenAI 沿着这条路取得了巨大进展，使用 RL 解决了 [Dota](https://openai.com/index/openai-five-defeats-dota-2-world-champions/)、[机械手](https://openai.com/index/solving-rubiks-cube/)等问题。但它从未接近解决计算机使用或网页导航问题，并且在一个领域有效的 RL 智能体无法迁移到另一个领域。似乎缺少了什么。

直到 GPT-2 或 GPT-3 之后，才发现缺失的部分是**先验**。你需要强大的语言预训练来将通用的常识和语言知识提炼到模型中，然后可以通过微调（fine-tuning）将它们变成网页（WebGPT）或聊天（ChatGPT）智能体（并改变世界）。**事实证明，RL 最重要的部分甚至可能不是 RL 算法或环境，而是先验，而这些先验可以通过与 RL 完全无关的方式获得。**

语言预训练为聊天创造了良好的先验，但对于控制计算机或玩视频游戏则效果不佳。为什么？因为这些领域与互联网文本的分布相距较远，在这些领域上简单地进行监督微调（SFT）/ 强化学习（RL）的泛化能力很差。我在 2019 年就注意到了这个问题，当时 GPT-2 刚问世，我基于它进行 SFT/RL 来解决基于文本的游戏——[CALM](https://arxiv.org/abs/2010.02903) 是世界上第一个通过预训练语言模型构建的智能体。但它需要数百万次的 RL 步骤才能在一个游戏中取得进展（hillclimb a single game），并且无法迁移到新的游戏中。尽管这正是 RL 的特点，对 RL 研究者来说并不奇怪，但我发现这很奇怪，因为我们人类可以轻松地玩一个新游戏，并且在零样本（zero-shot）的情况下表现得明显更好。然后，我迎来了人生中第一个“尤里卡时刻”——我们之所以能够泛化，是因为我们可以选择做更多的事情，而不仅仅是“去2号柜子”、“用1号钥匙打开3号宝箱”或“用剑杀死地牢怪物”，我们还可以选择思考诸如“地牢很危险，我需要一把武器来战斗。这里没有可见的武器，也许我需要在锁着的盒子或箱子里找找。3号箱子在2号柜子里，让我先去那里把它解锁”之类的事情。

![推理](https://ysymyth.github.io/images/second_half/reasoning.png)

思考，或者说推理，是一种**奇怪**的行动——它不直接影响外部世界，然而推理的空间是开放式的、组合起来是无限的——你可以思考一个词、一个句子、一整段话，或者 10000 个随机的英文单词，但你周围的世界并不会立即改变。在经典的 RL 理论中，这是一种糟糕的交易，使得决策变得不可能。想象一下，你需要从两个盒子中选择一个，只有一个盒子里有 100 万美元，另一个是空的。你的期望收益是 50 万美元。现在想象我加入了无限个空盒子。你的期望收益将变为零。但是，通过将推理加入到任何 RL 环境的行动空间中，我们利用了语言预训练的先验知识来实现泛化，并且我们有能力为不同的决策提供灵活的测试时计算。这真的是一件**神奇**的事情，我很抱歉在这里没有完全解释清楚，我可能需要再写一篇博客文章来专门讨论它。欢迎阅读 [ReAct](https://arxiv.org/abs/2210.03629) 了解关于智能体推理的原始故事，并感受我当时的一些想法。就目前而言，我的直观解释是：即使你加入了无限个空盒子，但你在生活中各种游戏中都见过它们，选择这些（思考的）“盒子”能让你为任何给定的游戏更好地选择那个装有钱的（真正行动的）盒子。我的抽象解释是：**语言通过智能体中的推理实现泛化**。

一旦我们有了正确的 RL 先验（语言预训练）和 RL 环境（将语言推理添加为行动），事实证明 RL 算法可能是最微不足道的部分。因此，我们有了 o 系列模型、R1、深度研究（deep research）、计算机使用智能体（computer-using agent）以及更多即将到来的成果。这是多么讽刺性的转折！长久以来，RL 研究者关心算法远胜于环境，而且没有人关注先验——所有的 RL 实验基本上都是从零开始。但我们花费了数十年的弯路才意识到，也许我们的优先级本应完全颠倒。

但正如史蒂夫·乔布斯所说：你无法预先把点点滴滴串联起来；只有在回顾时才能将它们联系起来。

## 下半场

这个配方正在彻底改变游戏规则。回顾一下上半场的游戏：

-   我们开发新颖的训练方法或模型，以提升基准（hillclimb benchmarks）表现。
-   我们创建更难的基准，并继续这个循环。

这个游戏正在被颠覆，因为：

-   这个配方基本上已经将提升基准表现的过程标准化和工业化了，不再需要太多新的想法。随着配方的规模化和泛化能力的增强，你针对特定任务的新方法可能只能带来 5% 的提升，而下一个 o 系列模型可能在没有明确针对该任务的情况下就能提升 30%。
-   即使我们创建了更难的基准，它们很快（而且越来越快）就会被这个配方解决。我的同事 Jason Wei 制作了一张漂亮的图表来很好地展示了这一趋势：

![进展](https://ysymyth.github.io/images/second_half/progress.jpeg)

那么，下半场还有什么可玩的呢？如果不再需要新颖的方法，而更难的基准只会越来越快地被解决，我们该怎么办？

我认为**我们应该从根本上重新思考评估**。这不仅仅意味着创建新的、更难的基准，而是要从根本上质疑现有的评估**设置（setups）**并创建新的评估设置，从而迫使我们发明超越现有有效配方的新方法。这很困难，因为人类有惰性，很少质疑基本假设——你只是把它们视为理所当然，而没有意识到它们是假设，而不是定律。

为了解释惰性，假设你发明了[历史上基于人类考试的最成功的评估之一](https://arxiv.org/abs/2009.03300)。在 2021 年，这是一个极其大胆的想法，但 3 年后它已经饱和了。你会怎么做？很可能创建[一个更难的考试](https://agi.safe.ai/)。或者假设你解决了[简单的编码任务](https://arxiv.org/pdf/2107.03374)。你会怎么做？很可能寻找[更难的编码任务](https://arxiv.org/pdf/2502.06807v1)来解决，直到达到 IOI 金牌水平。

惰性是自然的，但问题在于：人工智能已经在国际象棋和围棋比赛中击败了世界冠军，在 SAT 和律师资格考试中超越了大多数人类，并在 IOI 和 IMO 中达到了金牌水平。但世界并没有发生太大变化，至少从经济和 GDP 来看是这样。

我称之为**实用性问题（utility problem）**，并认为这是人工智能最重要的问题。

也许我们很快就能解决实用性问题，也许不能。无论哪种方式，这个问题的根本原因可能简单得具有欺骗性：**我们的评估设置在许多基本方面与现实世界的设置不同**。举两个例子：

-   **评估“应该”自动运行**，因此通常智能体接收任务输入，自主执行操作，然后接收任务奖励。但在现实中，智能体必须在整个任务过程中与人互动——你不会只给客服发一条超长信息，等 10 分钟，然后期望得到一个详细的回复来解决所有问题。通过质疑这种设置，新的基准被发明出来，将真实人类（例如 [Chatbot Arena](https://lmarena.ai/)）或用户模拟（例如 [tau-bench](https://arxiv.org/abs/2406.12045)）纳入评估循环中。
    ![tau](https://ysymyth.github.io/images/second_half/tau.png)
-   **评估“应该”独立同分布（i.i.d.）运行**。如果你有一个包含 500 个任务的测试集，你会独立运行每个任务，对任务指标取平均，得到一个整体指标。但在现实中，你是按顺序解决任务，而不是并行解决。一个谷歌软件工程师（SWE）在解决 google3 的问题时，会随着对代码库越来越熟悉而做得越来越好，但一个 SWE 智能体在同一个代码库中解决许多问题时，却无法获得这种熟悉度。我们显然需要长期记忆方法（[这里](https://arxiv.org/pdf/2409.07429)和[这里](https://yitaoliu17.com/assets/pdf/ICLR_2025_CER.pdf)有一些），但学术界没有合适的基准来证明这种需求，甚至没有足够的勇气去质疑作为机器学习基础的 i.i.d. 假设。

这些假设“一直”都是这样的，在上半场，在这些假设下开发基准是可以的，因为**当智能水平较低时，提高智能通常会提高实用性**。但现在，通用的配方保证能在这些假设下工作。因此，玩转下半场新游戏的方式是：

-   我们为现实世界的实用性开发新颖的评估设置或任务。
-   我们用现有配方解决它们，或者用新颖的组件增强配方。继续这个循环。

这场游戏很难，因为它不熟悉。但它令人兴奋。上半场的玩家解决的是视频游戏和考试，而下半场的玩家则可以通过将智能转化为有用的产品来建立价值数十亿甚至数万亿美元的公司。上半场充满了渐进式的方法和模型，而下半场在某种程度上会过滤掉它们。通用的配方会碾压你的渐进式方法，除非你创造出打破该配方的新假设。那时，你才能进行真正改变游戏规则的研究。

欢迎来到下半场！

## 致谢

这篇博客文章基于我在斯坦福 224N 课程和哥伦比亚大学所做的演讲。我使用了 OpenAI 的深度研究（deep research）功能来阅读我的幻灯片并撰写初稿。