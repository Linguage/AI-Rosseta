
#  最优传输 (Optimal Transport), Part 1- Marco Cuturi

[Optimal Transport, part 1 - Marco Cuturi - MLSS 2020, Tübingen](https://www.youtube.com/watch?v=jgrkhZ8ovVc&list=PLUnQ4mNHOUWU0IvAiPhIq6hNrMF1DnviQ&index=2)


### 导言

本文档整理了 Marco Cuturi 在 MLSS 2020 上所做讲座“Optimal Transport, Part 1”的演讲实录。最优传输（Optimal Transport, OT）作为一个历史悠久且理论深刻的数学分支，近年来在机器学习领域获得了显著的关注。本次讲座是系列的第一部分，旨在为听众，特别是机器学习背景的研究者和学生，构建一个关于最优传输基本概念和理论框架的坚实基础。

讲座从最优传输的起源入手，分别介绍了经典的蒙日问题和更为现代且应用广泛的坎托罗维奇问题，阐述了它们各自的直观背景、数学表述以及两者之间的联系。随后，内容深入到最优传输的数学形式化层面，重点探讨了坎托罗维奇对偶性理论及其推导，这是理解许多现代OT应用（如Wasserstein GAN）的关键。讲座还引入了基于OT定义的Wasserstein距离，并解释了其作为概率测度空间上“自然”度量的重要性。此外，讲座也提及了OT理论中的核心成果，如布雷尼耶定理及其与凸分析的深刻联系。最后，讲座从几何视角审视了最优传输，讨论了它如何赋予测度空间结构并引出测地线插值的概念，并指出了其在机器学习中作为损失函数应用的潜力。

整体而言，这部分讲座侧重于理论铺垫和概念理解，通过历史溯源、数学推导和直观示例，为后续深入探讨最优传输的计算方法和具体应用打下基础。

### 内容纲要

```
Optimal Transport, Part 1 - Marco Cuturi - MLSS 2020
├── 引言与动机
│   ├── 讲者介绍与最优传输 (OT) 概述
│   ├── 为何关注最优传输？ (跨学科性、历史渊源、当前热度)
│   ├── 最优传输在数据科学中的核心思想与应用领域概览
│   └── 讲座内容大纲 (理论基础、计算、统计)
├── 最优传输问题的起源与定义
│   ├── 蒙日问题 (Monge Problem, 1781)
│   │   ├── 直观解释 (搬土问题)
│   │   ├── 数学定义 (寻找映射 T)
│   │   └── 核心挑战 (质量守恒约束 T#μ = ν)
│   ├── (穿插 Q&A: OT 流行原因、求解器、正则化)
│   └── 坎托罗维奇问题 (Kantorovich Problem, ~1940s)
│       ├── 直观解释 (军队调配问题)
│       ├── 数学定义 (寻找耦合 P)
│       └── 与蒙日问题的关系 (松弛、更易处理)
├── 最优传输的数学形式化与理论基础
│   ├── 不同视角看 OT (物理粒子运动、生物细胞演化)
│   ├── (穿插 Q&A: 扩散距离 vs W距离, 自然界最优性假设, 经济学应用)
│   ├── 蒙日问题的深入探讨
│   │   ├── 布雷尼耶定理 (Brenier's Theorem) 及其与凸分析的关系
│   │   └── 蒙日-安培方程 (Monge-Ampère Equation)
│   ├── 坎托罗维奇松弛：耦合 (Coupling) 的概念与存在性
│   ├── 坎托罗维奇问题的形式化定义 (线性规划)
│   ├── 推导坎托罗维奇对偶性 (Kantorovich Duality)
│   │   ├── 对偶问题形式
│   │   └── (穿插 Q&A: KL散度 vs OT, 梯度/次梯度, 线性规划 vs OT)
│   ├── Wasserstein距离 (Wasserstein Distances, Wp)
│   │   └── 基于坎托罗维奇问题的定义
│   ├── C-变换 (c-transforms) 与半对偶形式
│   ├── W1 的对偶性 (坎托罗维奇-鲁宾斯坦对偶性)
│   └── 蒙日问题与坎托罗维奇问题的联系
│       └── 最优耦合与最优映射的关系
├── 最优传输的几何视角与机器学习应用
│   ├── 最优传输几何 (赋予概率测度空间几何结构)
│   ├── 测地线 (Geodesics) 与插值 (Displacement Interpolation)
│   ├── (穿插 Q&A: 流形上的 OT)
│   └── 机器学习中的变分最优传输问题 (OT 作为损失函数)
└── (预告) 最优传输的计算
    └── (穿插 Q&A: 与归一化流关系, 与压缩/k-means关系, 非平衡 OT)
```


## 引言与动机 (Introduction & Motivation)

**主持人:** 大家好，欢迎来到MLSS 2020第四天的下午场。我是 Rashmi Ritu，将主持这场关于最优传输的会议。我很荣幸介绍本场的演讲者，Marco Cuturi。Marco于2005年在巴黎矿业大学获得应用数学博士学位。他曾在东京的统计数理研究所担任博士后研究员。在普林斯顿大学运筹学与金融工程系短暂任教后，他加入京都大学担任副教授，并于2013年获得终身教职。自2016年起，他加入了法国国家统计与经济管理学院，至今仍在那里兼职。自2018年起，他加入了Google Brain巴黎办公室担任研究科学家。Marco近期提出的使用熵正则化解决最优传输问题的方案，重新点燃了机器学习社区对最优传输和Wasserstein距离的兴趣。他近期的工作主要集中于将这种损失函数应用于涉及概率分布的问题，例如主题模型、文本和图像的字典学习、生成模型的度量推断、使用Wasserstein损失的回归，以及词语的概率嵌入。他与Gabriel Peyré合著的书籍《Computational Optimal Transport》已成为最优传输理论实际应用的主要参考文献之一。我们都非常期待您的讲座，Marco，舞台交给您。

**Marco Cuturi:** 非常感谢介绍。非常高兴能在图宾根的这个暑期学校做这次讲座。无论大家身在何处，希望你们一切安好。请随时提问，即使在讲座结束后，关于最优传输这个话题，我都很乐意回答。

对我来说，能在MLSS演讲是一个非常激动人心的时刻，因为几年前我有组织MLSS的经历，看到这个社区仍在成长，感觉非常好。我真心感谢组织者们为组织这次活动付出的努力。

我的时间不多，就直接进入主题吧。这次讲座将关于最优传输（Optimal Transport）。如果你还不熟悉它，最优传输是近年来进入机器学习领域的一个数学分支，我认为这背后有几个充分的理由。这两次讲座将深入探讨这个领域。我将主要关注基础知识，也就是计算方面、统计方面和形式化方面，而不会过多涉及应用，因为目前的应用实在太多了，无法在两次讲座中涵盖。

### 为何关注最优传输？

为什么？为什么这个领域现在受到人们的关注？它是机器学习中那些似乎能联合多个科学领域的领域之一。这包括经济学，我们必须感谢经济学家们大约八十年前开创了这个领域。它同样受到应用数学家和纯粹数学家的喜爱。现在，我认为可以公平地说，它也是机器学习社区感兴趣的一个主题。大约一年前，我和Gabriel Peyré合写了一本书，可以在线获取，书中总结了一些计算方面的进展。这更多是关于方法论的部分。但我也非常兴奋地看到近年来，或许就是这一年左右，涌现出大量工作，现在正将应用扩展到生物学等领域。所以，对于一个数学家来说，看到诸如《单细胞基因表达的最优传输分析》这样的论文标题是非常令人兴奋的。还有其他一些论文也开始出现“最优传输”这个关键词，出现在并非专门关于数据科学的领域。

这个理论，也许这就是它令人兴奋的原因，它包罗万象。它本身有一些可以独立存在的成果，你可以真正归功于最优传输。但它也在不同领域有分支，基本上包括优化、偏微分方程（PDEs）、概率论、统计学。从这个统一的视角来看，最优传输的单一统一性非常有趣。

我认为最优传输的另一个有趣之处在于，它建立在非常庞大的数学基础上，可以说是最高等级的数学。这个领域的历史通常归功于蒙日（Monge）。我们会回到这一点。蒙日是两个多世纪前的一位数学家，他提出了这个直觉。我们不得不等待相当长的时间，大约150年，才看到这些直觉转化为实际的数值方法乃至应用。这里我放了三位过去对这个理论做出著名贡献的人物的照片：我们有（列昂尼德·）坎托罗维奇（Leonid Kantorovich），然后右边是（佳林·）库普曼斯（Tjalling Koopmans），他们都在1975年因其工作被授予诺贝尔经济学奖。或许可以说，乔治·丹齐格（George Dantzig）也应该因其在线性规划方面的工作而共同获得那枚奖章。然后有几年，这个思想基本上停留在数学规划和优化的范畴内，然后慢慢地转向偏微分方程（PDEs）和更偏向纯粹数学的方面。右边是贡献者的名单，可能最著名的两位是（塞德里克·）维拉尼（Cédric Villani）和（阿莱西奥·）菲加利（Alessio Figalli），他们都因其工作获得了菲尔兹奖。

当我说这个话题有点热门时，这里我绘制了一条谷歌趋势曲线，显示了随着时间的推移，对这个理论的兴趣似乎在增长。我认为更可靠的图景是统计在NIPS或ICML上，标题中以某种方式提到最优传输或Wasserstein的论文数量，确实有不少。根据我自己的统计，去年至少有大约100篇投稿的标题中提到了Wasserstein、最优传输、Sinkhorn或其任何变体。我猜还有更多论文使用了这些方法。

### 最优传输在数据科学中的核心思想与应用领域概览

如果我必须用一张幻灯片来总结最优传输理论，那就是：**它是概率测度的自然几何**。这里的“自然”可能没有太多实际意义，但本质上，最优传输提供了一个非常物理的视角，来看待你期望两个概率分布如何相互关联。就好比，你在某个时刻拍下某物的快照——比如一堆质量、气体、液体等等，然后在另一个时间点拍下另一张照片。最优传输可以帮助你解开这期间发生了什么的谜题：谁与谁相关？谁最有可能变成了这个人、这个细胞或这个词等等？

这里列出了一些你可能会遇到这类问题的例子：你在某个时刻有一堆东西处于某种配置，稍后在另一个不同的点看到类似的东西处于另一种配置，然后你想在两者之间建立联系，弄清楚中间可能发生了什么。

例如：
-   **分布比较**：事物以某种相关分布形式分布，这是概率密度的定义。因此，在统计模型中，使用最优传输来比较两个分布非常有意义。
-   **细胞群体追踪**（例如基因组学）：也许更令人惊讶的是，我希望最后能稍微谈谈这个。基本上，当你看到一群细胞处于某个状态，也许稍后看到它们处于不同的状态，最优传输能帮助你解决这个问题：哪个细胞变成了哪种类型的细胞？我们看到在基因组学中的应用越来越多。
-   **文本分析**（词袋模型）：当你将文本视为词袋时，你再次面临一个场景：质量被分配给某些词语。你想能够判断这个点云（point cloud）与另一个点云有多大不同。
-   **大脑活动**：你可能会想到，例如，这是两个不同的人在两种不同刺激下的两次大脑活动。你想量化它们有多相似，并将这些活动视为质量——某些区域被点亮了。你想推断如何从这里的这个配置（左图）转变为那个配置（右图）。
-   **生成模型**：它也一直是机器学习中与生成模型相关的一个重要主题。你可能听说过Wasserstein生成对抗网络（Wasserstein GANs）、最优传输GANs等等。在这种情况下，最优传输也有助于解决问题。
-   **颜色直方图比较**：也许以一种更直观、更容易想象的方式，你也可以用最优传输来比较颜色直方图。

然而，所有这些比较的基础有一个重要的思想：你需要能够量化将质量从一个观测点移动到另一个观测点的**成本**有多高。我的意思是，如果你要比较两个词语的点云（word clouds），在最优传输的背景下，你需要能够量化这两个分布支撑集中的每一对元素有多么不同。换句话说，**要用最优传输比较两个概率分布（例如两个词云），你必须首先能够比较单个实例（例如两个词）**。所以，如果你有办法比较两个观测值，最优传输就给了你比较这些观测值的两个分布的方法。

对于大脑活动的例子也是一样。如果你有办法说大脑的这个区域离那个区域非常远或不太远，那么你就可以使用最优传输。这非常重要。

这是生成模型案例的另一个例证。当你使用生成模型试图将概率分布参数化为一个流形（manifold）时，重要的事情将是量化你的真实数据与你生成的流形有多么不同。

### 讲座内容大纲

在这些讲座中，我将主要讨论三件事。我不认为我会有时间讲到选定的应用，但我已经把它们加到了幻灯片里，这样你们可以自己看看。基本上，我真正想在这两次讲座中关注的有三件事：

1.  **对最优传输的直观介绍**：正如我之前所说，不同的数学家对最优传输的想象或看法可能不同。有些人会强调映射（map），有些人会强调传输计划（transportation plan）或矩阵，有些人会有更动态的观点。我会尝试简要描述这些。
2.  **计算**：因为这是一个机器学习暑期学校，我们最终需要计算东西。我会非常简要地描述，如果你严格遵循几个世纪前提出的定义，你实际上如何计算最优传输。然而，正如大家所知，在机器学习中，我们通常需要走些捷径，以使事情具有可扩展性。我将在这次讲座中论证，这实际上不仅从计算角度看很重要——这样我们才能真正扩大规模——而且从**统计角度**来看也很重要。
3.  **统计方面**：这与计算的可扩展性密切相关。

然后我想这部分就结束了，我会把剩下的幻灯片放出来。

## 最优传输问题的起源与定义 (Origins & Definitions of OT Problems)

现在让我进入关于最优传输的介绍部分。在这一节中，我将提供两个我认为很直观的例子，说明你如何将最优传输想象成一个工具，以及它在尝试计算和解决问题时做了什么。在文献中，通常会讲到两个问题来介绍：一个是所谓的**蒙日问题 (Monge Problem)**，另一个是**坎托罗维奇问题 (Kantorovich Problem)**。你会看到它们是相关的，但并不完全相同。因此，区分它们的差异是很重要的。然后我会尝试解释，一旦这些问题被提出，我们能从最优传输中获得什么。最优传输本身是一个优化问题，但仅仅解决它并不是最终目的。我们可以使用最优传输问题的解（最优值）或解本身（最优传输计划或映射）。

### 蒙日问题 (Monge Problem, 1781)

故事始于1781年的加斯帕尔·蒙日（Gaspard Monge）。蒙日是法国一位著名的数学家，他在多个方面做出了贡献，对社会产生了影响，在这里创办了几所大学。如果你来巴黎，很容易看到这一点，因为你会注意到有一个以他名字命名的广场（Place Monge）和地铁站。有好几个地方都提醒着他在法国数学界的印记和遗产。

蒙日在1781年实际提出了这个问题，他并没有解决它，他只是通过提问开辟了一个领域。他说（这是法语原文的大意转述）：“当你必须将土从一个地方运到另一个地方时，你必须问自己如何做才对。”想象一下，你有一堆沙子堆在某个地方——这里我用实线上的一个测度 $\mu$ 来表示它——还有一个体积相同的坑在地上。蒙日简单地问：**将所有这些质量（物质）从它现在的位置移动，以达到我想要的另一种配置（填满那个坑），什么是一种有效的方式？** 在这里，目标很简单，就是移动所有红色的沙子，填满右边的坑。

当然，在21世纪，如果你问一个孩子或者任何人这个问题，他们可能没有动力去优化任何东西。你只需用一台机器，把给定的全部质量推到你需要放置的地方。但你需要想到，在1781年，这样的机器并不存在。所以我们必须更仔细地思考实际上该如何移动质量。

蒙日说，好吧，想象你有一个工人，配备了一把铲子。工人会被要求在某个位置 $x$（由实线上的位置索引）铲起沙子或土。工人在那个位置会找到一个无穷小量的质量，即该点的密度 $\mu(x)$。现在，工人需要知道，或者你需要指示工人，应该把这些质量卸在哪里？所以，你可以把这定义为一个**函数** $T$。我们要为每个有质量的位置 $x$ 提供一个目标位置 $y$，即 $y = T(x)$。你想要做的就是简单地把质量从 $x$ 移动到 $T(x)$（这里是 $y$）。这样做的时候，假设总成本与工人需要搬运质量从 $x$ 走到 $T(x)$ 的**距离**相关，这是合理的。

在这个位移中，有一个重要的部分：工人不仅需要从 $x$ 走到 $T(x)$，还需要携带数量为 $\mu(x)$ 的质量。所以蒙日说，一个合理的量化这个过程成本的方式应该是**质量乘以距离**（$cost \approx \mu(x) \times distance(x, T(x))$）。当然，这只是一个提议。你应该看看最近最优传输的文献，有更通用的表述。成本可能不仅仅与距离有关，可能是更一般的东西，比如距离的平方，这会使计算更容易。所以当然有很多变体，但这是最优传输问题的第一个实例。

当蒙日定义这个问题时，有一件事他还没说，而这可以说是最重要的事：这不仅仅是定义一个函数，把任意 $x$ 带到任意 $y$ 的问题。你需要保证这个函数解决了**质量守恒 (mass conservation)** 的问题。我的意思是，最终，如果我提出了一个函数 $T$（记住这是一个从 $\mathbb{R}$ 到 $\mathbb{R}$ 的函数），它需要做的是填满右侧的无穷小或更大的坑。想象一下，在右侧的目标区域的某个区间 $B$ 里，我可以计算有多少来自红色测度 $\mu$ 的质量最终到达了这个区间。如果我们找到了 $T$，我们可以定义 $B$ 在映射 $T$ 下的原像集合（inverse set），即 $T^{-1}(B) = \{x \in \mathbb{R} \mid T(x) \in B\}$。假设这个原像集合 $T^{-1}(B)$ 由三个区间 $A_1, A_2, A_3$ 组成。我们所说的质量守恒意味着以下内容：在区间 $A_1, A_2, A_3$ 中可用的沙子总量，必须等于你需要填入区间 $B$ 的沙子总量。换句话说， $A_1, A_2, A_3$ 的测度（质量），即 $\mu(A_1 \cup A_2 \cup A_3)$，必须等于目标测度 $\nu$ 在区间 $B$ 的测度 $\nu(B)$。

在数学上，这需要对所有可能的集合（这里我只展示了区间，但对任何集合都成立）都成立。这基本上意味着，你希望对于每一个集合 $B$，目标测度 $\nu(B)$ 必须等于原像集合 $T^{-1}(B)$ 在输入测度 $\mu$ 下的测度 $\mu(T^{-1}(B))$。用数学符号可以很容易地写成：$\mu$ 在 $T$ 作用下的**推前测度 (push-forward measure)** 等于 $\nu$，记作 $T_{\#}\mu = \nu$。就好比你将 $T$（一个从 $\mathbb{R}$ 到 $\mathbb{R}$ 的映射）应用于 $\mu$ 支撑集中的每个元素 $x$，得到某个 $y=T(x)$。如果你对所有 $x$ 都这样做，最终你会得到右侧的测度 $\nu$。

蒙日问的问题是：这已经相当有挑战性了——如何找到这样一个函数？但蒙日问得更多：**在所有能够将质量从 $\mu$ 移动到 $\nu$ 的函数 $T$ 中，哪一个能最小化总成本？** 这里，蒙日的假设是总成本是所有这些无穷小位移成本的积分（或总和）：$\int_{\mathbb{R}} \text{distance}(x, T(x)) \mu(x) dx$ （更一般地是 $\int c(x, T(x)) d\mu(x)$）。

这就是**蒙日问题**。它定义得很优美，也很直观。但如果你对优化有所了解，有一点应该会让你有点警惕：这个**约束 ($T_{\#}\mu = \nu$) 实际上是一个非常棘手的约束**。很容易证明它甚至不是凸约束。如果你有一个满足约束的函数 $T_1$ 和另一个满足约束的函数 $T_2$，那么它们的平均 $(T_1 + T_2)/2$ 通常不会满足这个约束。所以，这是一个概念上简单但实际上在优化方面相当麻烦的问题。

### 坎托罗维奇问题 (Kantorovich Problem)

（技术中断和恢复...）
好的，我们回到正轨，现在是PDF格式，没有动画了。

(问答环节开始)

**问:** 为什么最优传输现在变得流行起来？是因为优化器变得更好了还是其他原因？

**Marco Cuturi:** 当然，当然。这显然是你刚才提到的原因。我认为有几个因素。其中之一当然是**求解器（solvers）的扩展性更好了**。我认为原因之一是我们也更好地理解了一个事实——我稍后会回到这一点——如果你想严格按照几个世纪前数学家设计的方式来解决最优传输问题，你会遇到一些问题。而实际上，**进行正则化**——这在我们机器学习者看来可能有点像“脏活”，到处加正则化项——**从统计学的角度来看也是非常有益的**。所以，通过一些“hack”的方式绕过难题实际上在理论上也是很好的。这就是为什么现在流行起来的原因：因为有了这些“hack”，事情变得更具扩展性。另一方面，这个理论本身仍然具有很大的魅力，并且对于解释复杂事物具有很大的价值。

(问答环节结束)

现在我将转向最优传输的第二种表述，这与坎托罗维奇有关。尽管可以说有几个人可以声称拥有这个想法的一部分，特别是苏联数学家托尔斯泰（Tolstoy）在30年代，以及在美国的希区柯克（Hitchcock），他们在40年代初提出了完全相同的问题。但我将沿用坎托罗维奇的名字来介绍以他命名的表述。

想象一下，我们现在是1942年，第二次世界大战期间，这是东线战场。假设我们这里有一条前线（蓝色线条）。我们是红军将军，面临一个问题：我们需要将驻扎在兵营里的部队调动到前线的指定位置。这里我假设…（幻灯片再次崩溃，切换到PDF后继续）。

好的，假设你有这些士兵，他们现在正在右边的兵营里休息。你希望他们去往前线。如果你看一下这里的数量，假设我们有6万、9万和15万士兵分别在三个兵营里。而在前线的三个位置，我们分别需要12万、9万和9万士兵。（总数是匹配的，30万）。

如果你想用一种**朴素 (naive)** 的方式来完成这件事，将这些红色士兵运送到蓝色前线，一个非常简单的方法本质上是**按比例分配任务**。例如，如果你在前线的三个位置分别需要12万、9万、9万士兵，这意味着总需求比例是 12:9:9，即 4:3:3。那么对于第一个兵营的6万士兵，你可以按4:3:3的比例分配，即派出 $(60k \times 4/10) = 24k$ 到第一个前线点，$(60k \times 3/10) = 18k$ 到第二个点，$(60k \times 3/10) = 18k$ 到第三个点。对其他两个兵营也做同样的操作。这个解决方案是可行的。但它有一个问题：**它甚至没有考虑问题的几何结构**。它只是说，如果你这里有一个测度（兵营分布），那里有另一个测度（前线需求分布），你只需根据你这里拥有的总质量，按照目标测度的比例来分割和发送。这种简单的想法，我们稍后会看到，实际上是**最大均值差异 (Maximum Mean Discrepancy, MMD)** 或能量距离 (energy distances) 背后的思想，你们有些人可能熟悉。然而，问题在于这会导致大量的部队移动。基本上，你把所有部队向所有方向移动。所以，这个答案并没有真正考虑到问题的几何结构。

最优传输和坎托罗维奇的想法是寻找一种更好的移动质量的方式。坎托罗维奇基本上将问题分解为两部分：

1.  **几何 (Geometry)**：问题中涉及的所有位置信息被浓缩成一个**成本矩阵 (cost matrix)** $C$。矩阵的元素 $C_{ij}$ 就是从第 $i$ 个兵营（源点）到第 $j$ 个前线位置（目标点）的**距离**（或者说成本）。
2.  **质量 (Masses)**：我们只关心每个源点有多少质量（$60k, 90k, 150k$）和每个目标点需要多少质量（$120k, 90k, 90k$）。这些构成了**边际约束 (marginal constraints)**。

坎托罗维奇现在问的问题是：如果你是红军将军，你唯一想知道的是，对于第一个兵营的6万士兵，你应该派多少人去往前线的三个点中的每一个？对于第二个和第三个兵营也是如此。

我们在左边称之为**运输矩阵 (transportation matrix)** 或 **运输计划 (transport plan)** $P$。这是一个指令集。矩阵中的元素 $P_{ij}$ 表示从第 $i$ 个源点移动到第 $j$ 个目标点的质量数量。右边是距离矩阵 $C$，它本质上是问题的几何信息。

这三样东西——源边际（行和）、目标边际（列和）以及距离矩阵——就是你需要的全部要素。坎托罗维奇要求做的是，用数值填满这个运输矩阵 $P$，这些数值必须满足一个非常简单的约束，即**质量守恒约束**：
-   矩阵 $P$ 的每一**行**的和必须等于对应源点的总质量（例如，第一行的和 $P_{11} + P_{12} + P_{13}$ 必须等于6万）。
-   矩阵 $P$ 的每一**列**的和必须等于对应目标点的需求量（例如，第一列的和 $P_{11} + P_{21} + P_{31}$ 必须等于12万）。

所以，红军将军需要填入这些 $P_{ij}$ 值，并且要注意满足这些行和与列和的约束。

在满足这些约束的前提下，坎托罗维奇、库普曼斯和希区柯克都提出，我们将通过查看它所引发的**总运输成本**来评估你的移动质量方案 $P$ 的好坏。这里的总成本就是：**所有 $i, j$ 对的 $(P_{ij} \times C_{ij})$ 的总和**。即 $\sum_{i,j} P_{ij} C_{ij}$。你应该能清楚地看到这与蒙日问题的类比。

**坎托罗维奇问题**就是：在所有满足边际约束的有效运输矩阵 $P$ 中，找到使这个总成本最小化的那一个。

在这种情况下，解决方案可能看起来是这样的（示例）：也许第一个兵营的士兵只被分配到前两个点（例如30k去第一个点，30k去第二个点），第二个兵营的9万士兵全部去了第一个点（使得第一个点总共接收30k+90k=120k），第三个兵营的150k士兵被分配到第二和第三个点（例如60k去第二个点，90k去第三个点），从而满足所有需求。

坎托罗维奇的表述与蒙日的观点非常不同，因为你看，我们没有考虑函数，我们已经在尝试在**离散世界**中解决这个问题。这就是为什么社区中有点分歧：当你只考虑连续对象，比如密度时，你会看到蒙日和坎托罗维奇这两种解释之间存在联系。而如果你处理的是离散测度——这是我们机器学习从业者大多数时候必须处理的——那么你会更多地考虑坎托罗维奇问题。两者之间存在联系，我稍后会再谈。

## 最优传输的数学形式化与理论基础 (Mathematical Formalism & Theoretical Foundations)

让我现在稍微结束一下这个引言，通过另一个视角来看待这个问题。

### OT：自然移动粒子的方式

这次不是关于搬土，也不是关于调动士兵和分配他们，而是关于更物理的现象。这是一个如果你抬头看云彩就能想到的例子。假设你现在正在看一朵云，你拍下它的照片。一朵云由无数粒子组成，但假设只有1000个。云在空间和时间中移动，对吧？你在某个时刻拍下这1000个粒子的快照。你无法真正区分一个粒子和另一个粒子，对吧？很难识别一朵粒子云中的单个粒子。但你看到的是一个**群体 (population)**。五秒钟后，你再次拍摄**同一朵云**的照片。你知道之前在这里的所有粒子现在都在那里了。但是因为你无法单独识别每个粒子，你唯一能做的就是观察这里的一堆东西和那里的一堆东西。你无法真正追踪谁去了哪里。

最优传输提供了一种方法，来解开这期间发生了什么、谁去了哪里的谜题。它是通过解决一个最优传输问题来实现的。它基本上是说：“我将做出一个假设，即**自然是以最优的方式移动的**”。因此，如果这群粒子稍后呈现为这种配置，这意味着一些粒子必须移动。但我将假设它们的移动方式不是成本那么高的，意味着这里的这个粒子并没有移动到非常远的地方去。

这就是这种观点，你可以将这种观点应用于几个问题，其中之一是**生物学**。我之前引用的一些论文就涉及这种情况。想象一下，你现在有一个培养皿，里面有一堆细胞。也许六小时后，你观察其他的细胞。当然，因为细胞会演化，它们可能会生长得不同，它们会有不同的特征，也许它们的基因表达谱会随时间变化。它们变化的方式在这里没有精确描述，但你可以看到细胞基本上随时间具有不同的特征。你正在观察这个群体，六小时后再次观察它。你期望这里的东西是那里的起源，这里的每一个东西都演变成了右边的某个东西。

在生物学中，我们这样做的方式，本质上是为每个细胞在某个特征空间中提出特征描述符。这有点像我之前展示的点云，只不过不是在三维空间，而可能是在5000维空间（对应5000个描述符）。你观察发生了什么，六小时后再次观察那些特征描述符。现在你会说：“这些点（左边）必须移动才能到达它们现在的位置（右边）”。我无法精确地标记一个细胞并观察它变成了什么，因为我是在观察一个群体，而不是单独隔离一个细胞。但我能做的，基本上是使用最优传输。我会把它们放在同一个空间里，然后观察移动这些东西的**成本最低**的方式是什么。

当然，你可以回到士兵的比喻：这可能是士兵的兵营和前线；这可能是给定某种配置的土，而你希望它处于另一种配置。但基本思想是，你必须移动东西，事情发生了，你想用最优传输来猜测发生了什么。

一旦你有了最优传输（的解），你基本上就有了一种方式，知道这里的这个（细胞/点），很可能就是后来负责产生这里的那个（细胞/点）的原因。

### 蒙日问题 (Monge Problem) 的形式化

(问答环节开始)

**问:** 扩散距离（diffusion distance）近似于Wasserstein距离，这是否正确？如果是，是否存在解析联系？

**Marco Cuturi:** 扩散距离和Wasserstein距离之间存在联系，但这有点复杂。这取决于你具体称什么为“扩散距离”。

**问:** 我们是否知道自然界是否真的以最低成本的方式移动粒子？

**Marco Cuturi:** 哈哈哈，这确实是一个“最小能量原理”的问题。这是一个合理的观点。我现在还没有例子表明，人们能够确切地观察到谁在哪里以及去了哪里，并将其与最优传输解决方案告诉你的进行比较。所以，它**需要一个先验（prior）**，这是一个**强先验**。你问的问题在某种程度上引发了关于**反问题 (inverse problems)** 的思考。例如，如果你确实观察到了一些运动，并且你知道谁去了哪里，你能否将其描述为某个给定成本函数下的最优传输解？这就引出了更复杂、更有趣的问题。

**问:** 能否提供一些最优传输在经济学中应用的例子？毕竟它最早是在那里使用的。

**Marco Cuturi:** 好的。很多人开始使用最优传输，实际上是在**真实的交通运输**领域。如果你看五六十年代的论文，很大一部分文献是关于交通运输的。比如移动真实的人，他们可能从家去工作场所，你有人口数据，然后你开始思考，如果人们都非常聪明，他们应该会以最优的方式在群体间移动。如果再往前追溯到三十年代，我提到的托尔斯泰的参考文献实际上是关于苏联的铁路系统。他们有资源分布在某些地方，需要被运输到其他地方。所以在三十年代，这就是一个问题。另一个有趣的轶事是，在五十年代，最优传输的应用之一恰恰是计算出苏联运输网络的“最优传输”，目的是为了找出在哪些区域进行轰炸会造成最大损害。这是五十年代的一篇海军论文，它实际上在说：“好吧，如果人们以最优方式移动东西，我应该轰炸哪个环节、哪条铁路，才能造成最大的破坏？”

(问答环节结束)

让我用数学术语更精确地描述一下。我们有一个可测空间 $\Omega$，一个成本函数 $c: \Omega \times \Omega \to \mathbb{R}^+$，以及定义在该空间上的两个概率测度 $\mu$ 和 $\nu$。

**蒙日问题 (Monge Problem, 1781)** 就是寻找一个映射 $T: \Omega \to \Omega$，使得总运输成本最小化：
$$ \min_{T: \Omega \to \Omega} \int_{\Omega} c(x, T(x)) d\mu(x) $$
约束条件是 $T$ 必须满足**质量守恒**，即 $T$ 将 $\mu$ 推前为 $\nu$：
$$ T_{\#}\mu = \nu $$
这里的 $T_{\#}\mu(B) = \mu(T^{-1}(B))$ 对所有可测集 $B \subseteq \Omega$ 成立。

有趣的是，我们等了两个世纪才得到这里的一个重要结果。这是1781年，而这个结果来自1987年（布雷尼耶，Brenier）。蒙日问题的解对于数学家来说长期以来一直是个难题，原因我刚才提到了：推前约束太棘手了，很难从中得到什么。更增加了难度的是，蒙日选择的成本类型通常是最难的——蒙日的成本通常是**距离**本身 $d(x, y)$，而事实证明这在数学上是最具挑战性的。

布雷尼耶的突破在于这个**定理 (Brenier's Theorem, 1987)**，这是一个非常深刻的定理，因为它将两个看似不那么相关的领域联系起来：一端是最优传输，另一端是**凸分析 (convex analysis)**。

定理内容如下：
如果空间 $\Omega = \mathbb{R}^d$，成本函数是**平方欧氏距离** $c(x, y) = \|x - y\|^2_2$（这大概是机器学习告诉我的90%的情况），并且测度 $\mu, \nu$ 行为良好（比如，它们关于勒贝格测度绝对连续，即有密度函数），那么**最优的传输映射 $T^*$ （最小化蒙日问题的那个 $T$）必然是某个凸函数 $\phi$ 的梯度形式**：
$$ T^*(x) = \nabla \phi(x) $$
这里，$\phi: \mathbb{R}^d \to \mathbb{R}$ 是一个凸函数，它的梯度 $\nabla \phi$ 是一个从 $\mathbb{R}^d$ 到 $\mathbb{R}^d$ 的映射。这正好符合我们寻找的映射类型。

令人惊奇的是，无论 $\mu$ 和 $\nu$ 长什么样（只要它们是连续且行为良好的），最优的传输方式——从分布 $\mu$ 移动到分布 $\nu$ 的最佳方式——必然是某个凸函数的梯度。

这个定理甚至更深刻一点：它表明，**只要你有一个凸函数 $\phi$，那么它的梯度 $\nabla \phi$ 就是在任意测度 $\mu$ 与其推前测度 $\nu = (\nabla \phi)_{\#}\mu$ 之间的最优传输映射**。换句话说，取一个测度 $\mu$，取任意一个凸函数 $\phi$（它的梯度是一个向量场），将这个梯度作用于测度 $\mu$ 得到推前测度 $\nu$。那么你就知道，要从输入的 $\mu$ 移动到你刚刚产生的输出 $\nu$，没有比采用 $\nabla \phi$ 作为传输映射更好的方法了。

也许需要一点时间来消化，但它基本上是说，**当你对一个测度应用一个作为凸函数梯度的推前映射时，你就是在以最优的方式做事**。

这个结果是基础性的。有趣的是，直到2015、16年左右，这对于机器学习社区来说可能有点过于数学化，没什么兴趣。但这正在发生一点变化。事实证明，这个结果现在正在机器学习中找到应用。

### 蒙日-安培方程 (Monge-Ampère Equation)

为什么如果你开始深入研究最优传输文献，会看到偏微分方程（PDEs）冒出来？本质上是因为布雷尼耶定理提供了一种用PDE方法来预测最优传输的途径。我不会过多涉及细节，但如果你有这个结果（$\Omega = \mathbb{R}^d$, 成本是二次方距离 $c(x,y) = \|x-y\|^2$, $\mu, \nu$ 有密度 $f, g$ 且行为良好），那么映射 $T$ 将 $\mu$ 推前为 $\nu$（即 $T_\#\mu = \nu$），这等价于（通过变量变换公式）以下关系：
$$ f(x) = g(T(x)) \det(\nabla T(x)) $$
其中 $\nabla T(x)$ 是 $T$ 在 $x$ 处的雅可比矩阵（Jacobian matrix）。我想熟悉归一化流（normalizing flows）的人已经多次看到这个公式了，这只是变量变换公式。

现在，让我们将 $T$ 替换为某个凸函数 $\phi$ 的梯度 $T = \nabla \phi$，因为根据布雷尼耶定理，如果我们谈论的是最优传输，必然存在这样的 $\phi$。如果你替换 $T = \nabla \phi$，你会得到 $g(\nabla \phi(x))$。而雅可比矩阵 $\nabla T(x)$ 就是 $\phi$ 的海森矩阵（Hessian matrix） $\nabla^2 \phi(x)$，由于 $\phi$ 是凸的，这个海森矩阵是半正定的。所以我们得到：
$$ f(x) = g(\nabla \phi(x)) \det(\nabla^2 \phi(x)) $$
这个偏微分方程被称为**蒙日-安培方程 (Monge-Ampère equation)**。你可以通过求解蒙日-安培方程来解决最优传输问题。这是一个著名的PDE。

但为什么这对数据科学视角有用，却仍然不那么容易操作呢？原因之一是存在很多**退化 (degeneracies)** 情况。我首先应该澄清的是，在大多数情况下，特别是当你处理**离散测度**时，蒙日问题实际上是**没有定义的 (ill-defined)**。直观上很容易理解为什么：想象一下我的第一个测度 $\mu$ 是一个狄拉克质量（Dirac mass），即集中在一个点 $x_0$ 上。我试图从这个点 $x_0$ 出发，移动到右边一个连续的密度 $\nu$。你可以想象，如果你是蒙日问题中的工人，试图接到指令把 $x_0$ 处的质量移动到右边的某个地方，并且想要恢复出 $\nu$ 这个密度，这是不可能的。因为如果输入是一个狄拉克测度，蒙日映射唯一能做的就是把那个狄拉克测度移动到别处，变成另一个狄拉克测度 $T(x_0)$。没有办法将质量扩散开来。

### 坎托罗维奇松弛 (Kantorovich Relaxation)

这就是为什么坎托罗维奇的表述——就是我之前用兵营和前线例子介绍的那个——能够奏效的原因。坎托罗维奇基本上是蒙日问题的一个**推广 (generalization)** 或 **松弛 (relaxation)**。坎托罗维奇的观点是，**与其寻找一个从 $\Omega$ 到 $\Omega$ 的映射 $T$，不如转换视角，寻找一个耦合 (coupling)**。

什么是耦合 $P$？首先，你可以将耦合看作是大量条件概率密度（conditional densities）的结果。与其假设 $x$ 处的质量被确定性地传输到**某个点** $T(x)$，且只有一个 $T(x)$，我们不如说，在 $x$ 处的任何质量，我**允许将其根据某个概率分布进行分配**。如果我对所有 $x$ 都这样做，将所有这些概率分布（以 $\mu(x)$ 加权）收集起来，我就得到了一个**耦合**。

在这种情况下，一个耦合 $P$ 将是定义在乘积空间 $\Omega \times \Omega$ 上的一个**联合概率测度 (joint probability measure)**。它需要满足正确的**边际约束 (marginal constraints)**：
-   对于任何集合 $A \subseteq \Omega$，耦合测度 $P$ 在 $A \times \Omega$ 上的值必须等于第一个边际测度 $\mu$ 在 $A$ 上的值，即 $P(A \times \Omega) = \mu(A)$。
-   对于任何集合 $B \subseteq \Omega$，耦合测度 $P$ 在 $\Omega \times B$ 上的值必须等于第二个边际测度 $\nu$ 在 $B$ 上的值，即 $P(\Omega \times B) = \nu(B)$。

我们用 $\Pi(\mu, \nu)$ 表示所有满足这两个边际约束的 $\Omega \times \Omega$ 上的概率测度的集合。

让我更具体地举例说明。如果 $\Omega$ 是实线 $\mathbb{R}$，那么我想要的是，如果左边有一个测度 $\mu$，右边有一个测度 $\nu$，一个耦合 $P$ 将是乘积空间 $\mathbb{R}^2$ 上的一个（概率）密度，使得如果我把所有质量往 $y$ 轴方向投影（积分掉 $y$），我恢复得到 $\mu$；如果我把所有质量往 $x$ 轴方向投影（积分掉 $x$），我恢复得到 $\nu$。

关于耦合的好处是，**它们总是存在的**。无论你给我什么样的测度 $\mu$ 和 $\nu$，总会存在一个耦合。例如，一个始终存在的耦合就是**独立耦合 (product coupling)** $P = \mu \otimes \nu$。即使 $\mu$ 是一个狄拉克测度，$\nu$ 是两个狄拉克测度的混合，我仍然可以取这两个测度的乘积 $\mu \otimes \nu$，它给出一个有效的耦合。

### 坎托罗维奇问题 (Kantorovich Problem) 的形式化

总结一下，你在文献中会看到的定义最优传输的两种方式：
-   要么你是**蒙日 (Monge)** 的思路，直接考虑那个棘手的、难以处理的推前约束 $T_\#\mu = \nu$，但这样你的目标函数会有一个简单且非常直观的积分形式：$\int c(x, T(x)) d\mu(x)$。优化这样一个关于特殊函数的积分是很有趣的，问题很有意思，但在实践中很难解决。你需要做些什么来摆脱这个约束。
-   要么你是**坎托罗维奇 (Kantorovich)** 的思路。这是一个**线性规划 (Linear Program)** 问题，这就是为什么我们处于一个更安全、更容易的世界。问题是：已知两个测度 $\mu, \nu$，考虑所有满足边际约束 $\mu, \nu$ 的**耦合** $P \in \Pi(\mu, \nu)$ （我们知道至少存在一个，即乘积耦合 $\mu \otimes \nu$）。在所有这些耦合中，找到那个具有最小**平均成本**的耦合。这里的成本就是对成本函数 $c(x, y)$ 关于耦合 $P$ 进行积分：
$$ \min_{P \in \Pi(\mu, \nu)} \int_{\Omega \times \Omega} c(x, y) dP(x, y) $$
在优化问题方面，这只是一个线性问题。

(问答环节开始)

**问:** 我们能否将KL散度表示为某个最优传输问题的解的成本？

**Marco Cuturi:** 据我所知，不能。因为KL散度是一个 f-散度 (f-divergence)，它本质上是通过对两个密度进行积分，并查看它们之间的差异。如果我有 $\mu$ 和 $\nu$，它基本上是关于 $\int \mu(x) \log(\mu(x)/\nu(x)) dx$ 的积分。这意味着你只是在看每个 $x$，就好像它们是完全孤立的，然后比较 $\mu(x)$ 和 $\nu(x)$。有一种情况，你可以将总变差距离 (Total Variation distance) 视为一个最优传输问题，那种情况下成本是一个非常朴素的成本，即0-1成本。但对于KL散度，通常不行。

**问:** 由于凸函数 $\phi$ 不一定是 $C^1$ 光滑的，它的梯度映射 $\nabla \phi$ 是否可能是集值的（set-valued）？例如，是次梯度（subgradient）？

**Marco Cuturi:** 这是一个极好的问题。这基本上就是**正则性理论 (regularity theory)** 关注的内容。这是卡法雷利（Caffarelli）等人的工作。他们的问题是：在输入测度 $\mu$ 和输出测度 $\nu$ 满足什么条件下，我可以保证最优传输映射 $T^*$ 是正则的（例如，Lipschitz连续）？映射的Lipschitz性质等价于说凸函数 $\phi$ 是光滑的。有一些条件下你可以证明传输映射是Lipschitz的，也有很多反例表明它不是。确实，我提到梯度和蒙日-安培方程的部分假设了相当多的正则性。所以确实存在一些微妙之处。是的。

**问:** 线性规划（Linear Programming）和最优传输方法之间有什么区别？

**Marco Cuturi:** 很明显，线性规划**包含了**经典的最优传输问题。两者之间有非常强的联系。如果你看历史，丹齐格对求解一般线性规划问题产生兴趣的原因之一就是最优传输问题。最优传输问题也被证明等价于一大类线性规划问题，即**网络流 (network flows)** 问题。你总是可以将一个网络流问题重写为一个运输问题。现在，有很多最优传输问题的推广，比如带有正则化项的，它们不再是线性的。也有很多线性规划问题不是最优传输问题。所以它们有一个很大的交集，但并非完全是包含关系。

(问答环节结束)

### 推导坎托罗维奇对偶性 (Deriving Kantorovich Duality)

现在人们将很多成就归功于坎托罗维奇，是因为他也是第一个强调**对偶性 (duality)** 的人。什么叫对偶性？本质上是说，这个优化问题（我们称之为**原始问题 Primal Problem**）可以有另一种表述方式。与其寻找成本最低的耦合 $P$，坎托罗维奇提出了一个替代的表述（**对偶问题 Dual Problem**）：我们将寻找一对函数，你可以称它们为测试函数 (test functions)，$\phi: \Omega \to \mathbb{R}$ 和 $\psi: \Omega \to \mathbb{R}$。你将把 $\phi$ 对 $\mu$ 积分，把 $\psi$ 对 $\nu$ 积分，然后求和 $\int \phi d\mu + \int \psi d\nu$。但这里有一个关键点，它们需要满足一个约束：对于每一对 $(x, y)$，必须有 $\phi(x) + \psi(y) \le c(x, y)$。我们的目标是在满足这个约束的所有 $(\phi, \psi)$ 对中，**最大化**这个积分和。

我现在将展示如何推导出这个结果。这需要一点推导，只是简单的对偶理论，但我认为看看它是如何运作的对我们来说是件好事。

首先，一个小谜题。想象你有两个函数 $\phi, \psi: \Omega \to \mathbb{R}$，以及一个**耦合** $P \in \Pi(\mu, \nu)$（记住，耦合是满足正确边际约束的联合测度）。我将定义一个函数 $(\phi \oplus \psi)(x, y) = \phi(x) + \psi(y)$。这有点像两个函数的张量和，它接受一对值 $(x, y)$ 作为输入，输出 $\phi(x) + \psi(y)$。这个记号让推导更简单些。

考虑这个表达式：
$$ \int \phi d\mu + \int \psi d\nu - \int (\phi \oplus \psi) dP $$
问题是：这个表达式的值是多少？实际上，很容易看出它等于 0。为什么？
记住 $P \in \Pi(\mu, \nu)$。这意味着 $P$ 的第一个边际是 $\mu$，第二个边际是 $\nu$。
$$ \int (\phi \oplus \psi) dP = \int (\phi(x) + \psi(y)) dP(x, y) = \int \phi(x) dP(x, y) + \int \psi(y) dP(x, y) $$
因为 $P$ 的第一个边际是 $\mu$，所以 $\int \phi(x) dP(x, y) = \int \phi(x) d\mu(x)$。
因为 $P$ 的第二个边际是 $\nu$，所以 $\int \psi(y) dP(x, y) = \int \psi(y) d\nu(y)$。
因此，
$$ \int (\phi \oplus \psi) dP = \int \phi d\mu + \int \psi d\nu $$
所以原表达式 $\int \phi d\mu + \int \psi d\nu - \int (\phi \oplus \psi) dP = 0$。这仅是因为 $P$ 具有正确的边际 $\mu$ 和 $\nu$。

这提供了一种很好的方式来量化某个联合测度是否具有正确的边际。假设现在我有一个**任意**的 $\Omega \times \Omega$ 上的正测度 $P$（不一定满足边际约束）。考虑刚才的表达式：
$$ \int \phi d\mu + \int \psi d\nu - \int (\phi \oplus \psi) dP $$
通过类似的推导，它可以写成：
$$ \int \phi (d\mu - dP_x) + \int \psi (d\nu - dP_y) $$
其中 $P_x$ 和 $P_y$ 分别是 $P$ 的第一个和第二个边际测度。
通常情况下，这个值不会是0。它只在 $P_x = \mu$ 且 $P_y = \nu$ 时（即 $P \in \Pi(\mu, \nu)$ 时）对所有的 $\phi, \psi$ 都恒等于0。

利用这一点，我们可以定义集合 $\Pi(\mu, \nu)$ 的**示性函数 (indicator function)** $\mathcal{I}_{\Pi(\mu, \nu)}(P)$。我们可以说：
$$ \mathcal{I}_{\Pi(\mu, \nu)}(P) = \sup_{\phi, \psi} \left\{ \int \phi d\mu + \int \psi d\nu - \int (\phi \oplus \psi) dP \right\} $$
根据我们之前的推导：
-   如果 $P \in \Pi(\mu, \nu)$，那么花括号里的值总是0，所以上确界是 0。
-   如果 $P \notin \Pi(\mu, \nu)$（比如 $P_x \neq \mu$），那么我们可以选择一个 $\phi$ 使得 $\int \phi (d\mu - dP_x)$ 非常大（或者如果 $P_y \neq \nu$，选择 $\psi$ 使 $\int \psi (d\nu - dP_y)$ 非常大），所以这个上确界将是 $+\infty$。

所以，这个 $\sup$ 表达式确实是 $\Pi(\mu, \nu)$ 的示性函数（当 $P$ 是概率测度时，值为0或$+\infty$）。

现在回到坎托罗维奇的原始问题：
$$ \min_{P} \left\{ \int c(x, y) dP(x, y) \quad \text{s.t.} \quad P \in \Pi(\mu, \nu) \right\} $$
这是一个约束优化问题。使用示性函数的经典技巧（在凸分析中常用），我们可以将其重写为一个无约束（在所有 $\Omega \times \Omega$ 上的正测度空间上）的优化问题：
$$ \min_{P \ge 0} \left\{ \int c(x, y) dP(x, y) + \mathcal{I}_{\Pi(\mu, \nu)}(P) \right\} $$
因为如果 $P \notin \Pi(\mu, \nu)$，示性函数为 $+\infty$，这使得该 $P$ 不可能是最小值点（除非最小值本身是$+\infty$）。如果 $P \in \Pi(\mu, \nu)$，示性函数为0，我们就回到了原始的目标函数。

现在，将示性函数的 $\sup$ 表达式代入：
$$ \min_{P \ge 0} \left\{ \int c dP + \sup_{\phi, \psi} \left( \int \phi d\mu + \int \psi d\nu - \int (\phi \oplus \psi) dP \right) \right\} $$
我们可以将所有项都放到 $\sup$ 里面（因为 $\int c dP$ 不依赖于 $\phi, \psi$），但这似乎不是标准做法。正确的做法是写成 Lagrange 对偶。不过这里用的是 minimax 的思路：
$$ \inf_{P \ge 0} \sup_{\phi, \psi} \left\{ \int c dP + \int \phi d\mu + \int \psi d\nu - \int (\phi(x) + \psi(y)) dP(x, y) \right\} $$
重新整理一下，把与 $P$ 相关的项放在一起：
$$ \inf_{P \ge 0} \sup_{\phi, \psi} \left\{ \int (c(x, y) - \phi(x) - \psi(y)) dP(x, y) + \int \phi d\mu + \int \psi d\nu \right\} $$
现在，应用（比如Sion's）minimax 定理，我们可以交换 $\inf$ 和 $\sup$ 的顺序（因为这是一个线性规划问题，强对偶性通常成立）：
$$ \sup_{\phi, \psi} \inf_{P \ge 0} \left\{ \int (c(x, y) - \phi(x) - \psi(y)) dP(x, y) + \int \phi d\mu + \int \psi d\nu \right\} $$
现在观察内部的 $\inf_{P \ge 0}$ 部分。$\int \phi d\mu + \int \psi d\nu$ 这一项不依赖于 $P$，可以提到 $\inf$ 外面。我们关注：
$$ \inf_{P \ge 0} \int (c(x, y) - \phi(x) - \psi(y)) dP(x, y) $$
考虑被积函数 $f(x, y) = c(x, y) - \phi(x) - \psi(y)$。
-   如果**存在**某个 $(x_0, y_0)$ 使得 $f(x_0, y_0) < 0$，那么我们可以让 $P$ 是一个在 $(x_0, y_0)$ 处有巨大质量的测度（比如 $M \delta_{(x_0, y_0)}$），这样积分值 $\int f dP = M f(x_0, y_0)$ 就可以趋向于 $-\infty$。因此，这种情况下 $\inf$ 的值是 $-\infty$。
-   如果对于**所有** $(x, y)$，都有 $f(x, y) \ge 0$，即 $c(x, y) - \phi(x) - \psi(y) \ge 0$（或者说 $\phi(x) + \psi(y) \le c(x, y)$），那么因为 $P$ 是正测度，积分 $\int f dP$ 总是非负的。要使其达到最小值，我们应该选择 $P = 0$（零测度），此时 $\inf$ 的值是 0。

所以，内部的 $\inf_{P \ge 0}$ 本质上是在检查约束 $\phi(x) + \psi(y) \le c(x, y)$ 是否对所有 $(x, y)$ 都成立。如果成立，$\inf$ 值为0；如果不成立，$\inf$ 值为 $-\infty$。

将这个结果代回 $\sup_{\phi, \psi}$ 表达式：
$$ \sup_{\phi, \psi} \left\{ (\text{0 if } \phi(x) + \psi(y) \le c(x, y) \forall x, y; \text{ else } -\infty) + \int \phi d\mu + \int \psi d\nu \right\} $$
为了最大化这个表达式，我们显然只对那些满足约束 $\phi(x) + \psi(y) \le c(x, y)$ 的 $(\phi, \psi)$ 对感兴趣（因为其他情况结果是 $-\infty$）。在这些满足约束的对中，我们想要最大化 $\int \phi d\mu + \int \psi d\nu$。

因此，我们恢复得到了**坎托罗维奇对偶问题 (Kantorovich Dual Problem)**：
$$ \sup_{\phi, \psi} \left\{ \int_{\Omega} \phi(x) d\mu(x) + \int_{\Omega} \psi(y) d\nu(y) \quad \text{s.t.} \quad \phi(x) + \psi(y) \le c(x, y) \quad \forall x, y \in \Omega \right\} $$
这就是坎托罗维奇原始问题的对偶形式。

### Wasserstein距离 (Wasserstein Distances)

现在我们已经涵盖了坎托罗维奇原始问题及其对偶问题。我们可以开始定义一些东西了。当成本函数 $c(x, y)$ 等于某个**距离** $d(x, y)$ 的 $p$ 次方时，即 $c(x, y) = d(x, y)^p$，其中 $p \ge 1$，并且 $d$ 是 $\Omega$ 上的一个度量（metric）—— 我提醒一下，度量需要满足对称性 $d(x, y) = d(y, x)$，同一性 $d(x, y) = 0 \iff x = y$，以及三角不等式 $d(x, z) \le d(x, y) + d(y, z)$。

那么，你可以证明，下面这个表达式（它是最优传输问题的值，使用了成本 $d(x, y)^p$，然后再取 $p$ 次根）实际上定义了概率分布之间的一个**距离**：
$$ W_p(\mu, \nu) := \left( \inf_{P \in \Pi(\mu, \nu)} \int_{\Omega \times \Omega} d(x, y)^p dP(x, y) \right)^{1/p} $$
这个 $W_p(\mu, \nu)$ 通常被称为 $p$-**Wasserstein距离 (p-Wasserstein distance)** 或 $p$-**Wasserstein度量 (p-Wasserstein metric)**。可以证明它满足距离的三个性质（非负性与同一性、对称性、三角不等式）。

通常人们在处理时会去掉 $1/p$ 指数，而是在左边写 $W_p^p$，即：
$$ W_p(\mu, \nu)^p = \inf_{P \in \Pi(\mu, \nu)} \int_{\Omega \times \Omega} d(x, y)^p dP(x, y) $$
这表明 $p$-Wasserstein距离的 $p$ 次方等于一个线性规划问题的最优值。

### 坎托罗维奇对偶性再探

我将更多地讨论坎托罗维奇对偶性，因为如果你看近期的论文，你会发现它被越来越多地利用。不仅是（Wasserstein）GAN 的论文，比如 Arjovsky 等人的工作，这是他们依赖的基础，还有很多其他论文也使用了坎托罗维奇对偶性。

我们从这个原始表述（红色）和对偶表述（蓝色）之间到底获得了什么？
**原始问题**: $\min_{P \in \Pi(\mu, \nu)} \int c(x, y) dP(x, y)$
**对偶问题**: $\sup_{\phi, \psi: \phi(x)+\psi(y) \le c(x, y)} \int \phi d\mu + \int \psi d\nu$

对偶性有点像一个魔术，有时它为你提供了一个看待同一问题的不同方式，这种方式可能非常方便。它本质上做的是将目标函数和约束条件上下翻转。

在**原始问题**中，变量是耦合 $P$，而约束 $\Pi(\mu, \nu)$ 中，输入的测度 $\mu$ 和 $\nu$ 扮演了重要角色。耦合可以是任何东西，但它必须满足由输入 $\mu, \nu$ 给定的约束。而成本函数 $c(x, y)$ 出现在目标函数中。

在**对偶问题**中，变量是势函数 $\phi, \psi$。输入的测度 $\mu, \nu$ 只出现在目标函数中。而在这里的约束 $\phi(x) + \psi(y) \le c(x, y)$ 中，起关键作用的是**成本函数** $c(x, y)$。

所以，成本 $c$ 和测度 $\mu, \nu$ 这两个问题中都重要的东西，它们扮演的角色互换了：一个原来在约束里，现在到了目标函数；一个原来在目标函数里，现在到了约束里。

从哲学的角度来看，如果你想：
-   如果你对**改变成本函数 $c$** 感兴趣，通常原始问题更适合你的研究，因为它出现在目标函数中，而改变目标函数通常比改变约束更容易。
-   如果你对**改变测度 $\mu, \nu$** 感兴趣（例如，在优化 $\mu$ 或 $\nu$ 时），那么对偶问题通常更容易处理，因为如果成本 $c$ 不变，只有 $\mu, \nu$ 改变，那么你只是在改变目标函数。

这就是对偶性的用处。

### C-变换 (c-transforms)

现在让我进一步探讨坎托罗维奇对偶性提供给我们的工具。首先，我们可以说，当你使用对偶表述时，你是在**函数空间**上进行优化，而不是在耦合空间上。仅仅在参数化方面，优化函数空间可能比优化耦合空间更容易。

我将展示，实际上，到目前为止我为每个测度都携带了两个势函数（一个 $\phi$ 对应 $\mu$，一个 $\psi$ 对应 $\nu$），这有点冗余。我们实际上可以将其简化为**只需要一个函数**。这就是坎托罗维奇对偶性的意义所在。

让我试着说服你，这确实有点多余。想象我选择了一个函数 $\phi$。我将从 $\phi$ 开始，你可能会问，找到一个好的 $\psi$ 的答案可能是什么？记住，在对偶问题中，我们是在取上确界 ($\sup$)。所以，我将某种程度上**固定 $\phi$**，然后考虑我能为 $\psi$ 提出什么样的好候选者。

事实上，回答什么是与 $\phi$ 匹配的**最佳 $\psi$** 非常容易。原因如下：
你唯一需要关心的是 $\phi, \psi$ 必须满足约束 $\phi(x) + \psi(y) \le c(x, y)$ 对所有 $x, y$ 成立。另一方面，你对 $\psi$ 的激励是什么？你的激励是使 $\int \psi d\nu$ 尽可能大，因为你在取 $\sup$。由于 $d\nu$ 是一个正测度，这本质上意味着你需要让 $\psi(y)$ 本身尽可能大，但要受到约束的限制。

对于一个固定的 $y$，你需要确保 $\psi(y) \le c(x, y) - \phi(x)$ 对**所有** $x$ 都成立。你想让 $\psi(y)$ 尽可能大，但又不能违反这个对所有 $x$ 的约束。这意味着，对于给定的 $y$，你能考虑的**最好的 $\psi(y)$** 就是：
$$ \psi(y) = \inf_{x \in \Omega} \{ c(x, y) - \phi(x) \} $$
这个量 $\psi(y)$ 描述了你能得到的最佳 $\psi$。这就是为什么我们引入这个记号（**c-变换** 或 **c-共轭**）：
如果你给我一个函数 $\phi$（红色函数），我想找到与之配对的最佳 $\psi$（蓝色函数），我能得到的不会比这个称为 **$\phi$ 的 c-变换** $\phi^c$ 更好：
$$ \phi^c(y) := \inf_{x \in \Omega} \{ c(x, y) - \phi(x) \} $$
我使用了红色函数 $\phi$，通过这个变换将它变成了与之配对的最佳蓝色函数 $\psi = \phi^c$。

它看起来有点像你可能熟悉的勒让德变换（Legendre transform）。你实际上可以将其视为勒让德变换的推广。因为如果成本 $c(x, y) = \|x-y\|^2_2$ （$p=2$），那么这个 $\phi^c$ 实际上变成了勒让德变换（可能差一个符号和变量变换）。

这给我们带来了什么呢？好吧，实际上，如果对于每个红色函数 $\phi$，没有比 $\psi = \phi^c$ 更好的蓝色函数可以考虑，这意味着当我们在寻找 $(\phi, \psi)$ 对时，我们可以简单地只寻找一个红色函数 $\phi$，积分 $\int \phi d\mu$，然后既然我们知道对于第二个测度 $\nu$，我们无法做得比使用 $\psi = \phi^c$ 更好，那么我们可以将关于 $\psi$ 的参数化移除，只关注 $\phi$。所以现在我有一个问题是这样的：
$$ \sup_{\phi} \left\{ \int_{\Omega} \phi(x) d\mu(x) + \int_{\Omega} \phi^c(y) d\nu(y) \right\} $$
我只需要寻找最佳的红色函数 $\phi$，知道我将用它对第一个测度积分，而当考虑第二个测度时，我将使用它的 c-变换 $\phi^c$ 来积分。这通常被称为**半对偶问题 (semi-dual problem)**。它甚至可以独立发挥作用。

你可能会问，为什么停在这里？对蓝色测度 $\nu$ 和蓝色函数 $\psi$ 没有理由不公平。我们可以提出相反的变换：给定一个蓝色函数 $\psi$，将其转换为最佳的红色函数 $\phi$。确实，这就是进行 c-变换的思想，但方向相反。给定一个蓝色函数 $\psi$，我们可以定义它的 c-变换 $\psi^c$：
$$ \psi^c(x) := \inf_{y \in \Omega} \{ c(x, y) - \psi(y) \} $$
这将是一个红色函数 $\phi = \psi^c$，它接受 $x$ 作为输入。

现在我们可以开始玩了，比如把红色转蓝色，再转红色，再转蓝色等等。例如，应用两次 c-变换会怎样？即 $(\phi^c)^c$ 是什么？

事实证明，这个迭代并不总是有用。因为可以证明，对于任何函数 $\phi$（不一定是最优的），如果你玩这个游戏两次，即计算 $(\phi^c)^c$，你通常不会得到比 $\phi$ 本身“更好”的东西。你会陷入这样一个事实：$(\phi^c)^c \ge \phi$。这有点像多次应用勒让德变换，最终会卡住。

然而，这有助于我们以一种稍微更精确和受约束的方式来表述问题。回想一下，当我表述坎托罗维奇对偶问题时，我基本上是在说，在所有 $(\phi, \psi)$ 对上取 $\sup$。这里我们本质上是说，我们期望最优的函数 $(\phi^*, \psi^*)$ 会某种程度上满足这个游戏：如果我用 c-变换将一个转换为另一个，我应该得到另一个，即 $\psi^* = (\phi^*)^c$ 并且 $\phi^* = (\psi^*)^c$。这意味着最优的 $\phi^*$ 应该满足 $\phi^* = ((\phi^*)^c)^c$。

我们将说 $\phi$ 是一个 **c-凹 (c-concave)** 函数，如果存在某个（蓝色）函数 $\psi$ 使得 $\phi = \psi^c$。这等价于说，如果你对 $\phi$ 应用两次 c-变换，你得到的就是 $\phi$ 本身： $\phi = (\phi^c)^c$。

你可能会觉得这有点乏味，但这基本上是WassersteinGAN论文背后的思想。思路是：我将**把我的搜索范围限制在 c-凹函数** $\phi$ 上。然后我只关注 $\int \phi d\mu + \int \phi^c d\nu$ 的积分。

### $W_1$ 的对偶性 (坎托罗维奇-鲁宾斯坦)

为什么这很有力？因为如果成本函数 $c(x, y)$ 是一个**距离** $d(x, y)$ 且 $p=1$（即成本就是距离本身），那么存在一个等价关系：
说 $\phi$ 是 **d-凹 (d-concave)**（即 $\phi = (\phi^d)^d$）等价于说 $\phi$ 是 **1-Lipschitz** 的，并且它的 d-变换 $\phi^d$ 等于 $-\phi$ 本身。

我将跳过证明的细节，它在幻灯片里。如果你感兴趣，我建议你看一下。如果你在玩GAN，记住这个Wasserstein对偶性（特别是$W_1$）的来源总是好的。

但底线本质上是：如果你处理的成本是**距离** $d(x, y)$（即 $p=1$），那么在所有 $(\phi, \psi)$ 对上取 $\sup$ 且满足 $\phi(x) + \psi(y) \le d(x, y)$，就等价于只在**一个**满足 d-凹性的函数 $\phi$ 上取 $\sup$，然后计算 $\int \phi d\mu + \int \phi^d d\nu$。
而根据刚才提到的结果，在 d-凹函数上取 $\sup$ 等价于在所有 **1-Lipschitz** 函数 $\phi$ 上取 $\sup$。并且因为对于 1-Lipschitz 函数 $\phi$，它的 d-变换 $\phi^d$ 是 $-\phi$，所以对偶问题变成了：
$$ \sup_{\phi: \|\phi\|_{\text{Lip}} \le 1} \left\{ \int \phi d\mu + \int (-\phi) d\nu \right\} = \sup_{\phi: \|\phi\|_{\text{Lip}} \le 1} \int \phi (d\mu - d\nu) $$
这就是 **坎托罗维奇-鲁宾斯坦 (Kantorovich-Rubinstein) 对偶公式**，它给出了 $W_1(\mu, \nu)$ 的表达。这就是 $W_1$ 对偶性的来源。

### 蒙日问题与坎托罗维奇问题的联系

结束理论方面的内容，我曾告诉过你们蒙日问题和坎托罗维奇问题之间存在联系。本质上，如果你深入研究数学——这也是纯粹数学家感兴趣的地方——有几个结果证明：如果成本函数 $c$ 行为良好，并且测度 $\mu, \nu$ 本身也行为良好（例如，$\mu$ 绝对连续），那么：
1.  一个最优的蒙日映射 $T^*$ **必须存在**。（这已经被布雷尼耶定理暗示了）。
2.  坎托罗维奇问题求解得到的最优运输**耦合** $P^*$ 与这个最优映射 $T^*$ 密切相关，其形式为：**耦合 $P^*$ 必须集中在映射 $T^*$ 的图 (graph) 上**。
    更精确地说，最优耦合 $P^*$ 是由 $(\text{identity}, T^*)$ 这个映射将 $\mu$ 推前得到的测度：
    $$ P^* = (\text{id}, T^*)_{\#}\mu $$
    这意味着 $P^*$ 的支撑集包含在 $\{(x, T^*(x)) \mid x \in \text{supp}(\mu)\}$ 中。基本上是说，对于 $\mu$ 中每个点 $x$，最优耦合 $P^*$ 将质量 $\mu(x)$（如果是离散的话）或 $d\mu(x)$（如果是连续的话）全部放在联合空间中的点 $(x, T^*(x))$ 上。

换句话说，如果我回到之前的图示... （翻页）... 一般来说，当你计算最优耦合时，你可能会考虑一族密度，它们可能是像这样的非常规则的测度。但事实证明，如果输入测度行为良好，那么最优耦合将是那个空间上的一个非常**细的脊 (thin ridge)**。它本质上是说，对于每个点 $x$，我只在点对 $(x, y)$ 处分配质量，其中 $y = T^*(x)$。

这大致说明了两者之间的关系。这些结果基本上可以追溯到大约20到30年前。这些是蒙日表述和坎托罗维奇表述之间的联系。

## 最优传输的几何视角与机器学习应用 (Geometric Viewpoint & ML Applications)

(问答环节开始 - 承接上一部分)

**问:** 假设我想移动流形上的密度。仅仅使用流形上的距离作为成本就足够了吗？还是有更多需要做的？

**Marco Cuturi:** 不，实际上最... 在理论上有很多关于在黎曼流形上进行最优传输的工作。这基本上是2000年代的工作。布雷尼耶定理到那个场景的推广已经由麦肯（McCann）等人完成了。所以，不，确实... 甚至在应用领域，在网格（meshes）或图（graphs）上进行最优传输也是很常见的。这是图形学中一些应用的基础。例如，如果你想到大脑，大脑是一个流形，你有信号，你可以计算信号在不同大脑区域之间的传输。通常会使用流形上的测地线距离（shortest path distance）作为成本。我的一些学生，比如Shamir Jannati，在这方面做了一些非常好的工作。

(问答环节结束)

### 最优传输几何

我想强调的是，距离本身 $W_p(\mu, \nu)$ 作为量化两个测度有多不同的方式是有用的。但它也将用于另一个有用的目的：**只要你有一个度量（距离），你就可以定义测地线 (geodesic) 的概念**。你可以有测地空间。这意味着，你现在可以找到两个测度之间的**插值 (interpolation)**。

所以，Wasserstein距离的意义不再仅仅是衡量两个测度有多不同，它也是一种**在它们之间进行插值的方法**。我将展示这有很好的推广，比如到重心 (Barycenters) 等等。

稍微强调一下，这有点回到之前关于KL散度是否可以写成最优传输问题的那个问题：**使用Wasserstein距离得到的插值类型，与使用经典欧几里得插值得到的插值非常非常不同**。
如果你有一个蓝色测度 $\mu_0$ 和一个红色测度 $\mu_1$，经典的（欧几里得）插值基本上是**线性混合 (additive mixing)** 它们： $t\mu_1 + (1-t)\mu_0$。这会给你一个介于两者之间的东西，像这样（展示混合图）。
而**最优传输插值 (displacement interpolation)** 是指将质量从输入测度 $\mu_0$ 沿着“最优路径”移动到目标测度 $\mu_1$ 的中间状态。你可以在三维中看到这一点，你可以开始想象这些最优传输插值可能看起来像什么。（指向一个演示链接）注意，这应该在你的PDF中作为一个链接工作，你应该能够点击它并看到一个地球的例子。

### 机器学习中的变分最优传输问题

这是我想说明的第一点。因此，我们正在慢慢地摆脱“最优传输仅用于计算测度间距离”的范式。我们现在可以转换并走向下一个更令人兴奋的事情，那就是**将最优传输用作损失函数 (loss function)**。

我们都知道，在机器学习中，当我们计算平方差时，目的不主要是计算距离，而是**最小化**它们。这就是我们在回归、自编码器等中所做的——我们总是在最小化平方欧氏距离。

所以，现在最优传输中令人兴奋的部分是：直到2010年左右，大多数对最优传输及其应用感兴趣的人会尝试计算距离本身 $W_p(\mu, \nu)$。这是一个**终极目标 (end goal)**。（这有点回到之前被问到的问题：为什么是现在？现在它有什么特别之处？）而现在，我们更感兴趣的是尝试**最小化**这个距离，其中一个（或两个）测度是由参数 $\theta$ 参数化的，例如 $\mu_\theta$。即解决 $\min_\theta W_p(\mu_\theta, \nu_{data})$ 这样的问题。
这意味着你需要某种方式来计算Wasserstein距离关于输入（例如参数 $\theta$ 或测度 $\mu_\theta$ 的位置/权重）的**梯度**。这就是人们现在感到兴奋的地方。

## (预告) 最优传输的计算 (Computing OT)

(问答环节开始 - 承接上一部分)

**问:** 最优传输和归一化流（Normalizing Flows）之间有联系吗？能否评论一下？

**Marco Cuturi:** 当然有联系。可以说，归一化流是一族映射（通常是可逆的、易于计算雅可比行列式的映射的组合），它们从一个（通常简单的）测度变换到另一个（通常复杂的）测度。大多数时候，重点并不在于**最优地**从一个测度移动到另一个测度，而仅仅是**到达**那里。所以你只是试图构建一系列小的、容易微分的操作，把你带到你想要的地方。从这个意义上说，这更多是关于“传输东西”，它是一种很好的参数化传输方式。当你引入最优传输时，你基本上是试图**有效地**做到这一点。有几篇论文试图通过在归一化流的优化中加入一些正则化项来引入这种最优性。我最近看到一篇名为TrajectoryNet的论文（即将在ICML发表）就是这样做的，他们在生物学的背景下使用了它。

**问:** 最优传输可以与压缩（compression）联系起来吗？

**Marco Cuturi:** 是的，嗯，是的。在比如 **k-means** 和最优传输之间有非常深刻的联系。本质上，k-means在做的是：当你有一个测度（即欧氏空间 $\mathbb{R}^d$ 中的数据点云 $\mu = \frac{1}{n}\sum \delta_{x_i}$）时，试图找到 $k$ 个**质心 (centroids)** $\{c_1, ..., c_k\}$，以及相应的权重 $\{w_1, ..., w_k\}$（通常 $w_j = 1/k$ 或与簇大小成比例），形成一个只有 $k$ 个点的测度 $\nu = \sum_{j=1}^k w_j \delta_{c_j}$，来最好地代表原始数据。最小化k-means的目标函数（点到其最近质心的平方距离之和）实际上等价于一个最优传输问题：你在所有支撑集最多为 $k$ 个原子的测度 $\nu$ 中，最小化 $\mu$ 和 $\nu$ 之间的**平方Wasserstein距离** $W_2^2(\mu, \nu)$。所以，当然有联系，更准确地说是与**量化 (quantization)** 的联系。

**问:** 在实际情况中，通常士兵比兵营多（或者反过来，质量不守恒）。是否有自然的扩展来处理这种情况？

**Marco Cuturi:** 这是一个非常令人兴奋的问题。这次讲座很短，所以我无法涉及所有细节。但确实，我的主要信息是：蒙日提出了一个运输问题，坎托罗维奇定义了另一个。现在，正如我们都知道的，没有什么东西是神圣不可侵犯的，无论是在目标函数还是在约束中，你都可以开始改变任何你想要的东西。唯一的问题是这是否有趣。**非平衡最优传输 (unbalanced optimal transport)** 的表述确实存在，并且已经存在了一段时间，可以说可以追溯到2000年代早期Benamou等人的工作。但我认为在过去五年左右发生了非常好的、令人兴奋的发展。所以，确实存在有趣的非平衡最优传输表述。是的。

(问答环节结束)

**Marco Cuturi:** 谢谢大家。

**主持人:** 好的，现场似乎没有更多问题了。感谢大家参与本次会议。感谢Marco精彩的讲座。我们将在30分钟后在另一个zoom会话中进行圆桌讨论，欢迎大家参加。谢谢大家。哦，我说明天见... 不对，下一次讲座见。再见。非常棒的讲座。