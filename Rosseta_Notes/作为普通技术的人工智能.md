


> - 原文链接：[AI as Normal Technology](https://www.aisnakeoil.com/p/ai-as-normal-technology?utm_source=multiple-personal-recommendations-email&utm_medium=email&triedRedirect=true)
> - 一篇我们将扩展为下一本书的新论文
> - 作者：Arvind Narayanan 和 Sayash Kapoor
> - 日期2025年4月15日


### 文章导言

人工智能（AI）正以前所未有的速度渗透到我们生活的方方面面，引发了从根本性社会变革到生存威胁的各种讨论。在众多激动人心或令人忧虑的预测中，本文旨在提供一个更为审慎和贴近现实的视角：将AI视为一种“普通技术”。这并非贬低其潜在影响力，而是强调它与其他改变历史的通用技术（如电力或互联网）一样，其发展和融入社会的过程遵循着可循的规律，而非某种神秘力量的降临。

本文认为，当前围绕AI，特别是“超级智能”的许多讨论，往往脱离了技术发展的实际路径和社会采纳的复杂现实。我们将探讨为何AI带来的经济和社会变革可能比预期更为缓慢，需要数十年时间；在一个人与AI共存的未来，人类的角色将如何演变为对AI的控制和引导；以及如何从“普通技术”的角度重新审视AI风险，将焦点从科幻式的“失控”场景，更多地转向事故、滥用以及加剧现有社会问题的系统性风险。最后，基于这一视角，我们将讨论更具韧性和适应性的政策方针，主张在不确定性中优先考虑稳健性、减少极端干预，并积极创造条件以公平地实现AI的潜在益处。希望通过这种冷静、务实的分析，为理解和应对AI的未来提供一个更坚实的基础。

### 内容纲要

```
人工智能万灵药 (AI Snake Oil)
├── 引言：将AI视为普通技术的愿景
│   ├── 定义：描述、预测与规范
│   ├── 核心观点：AI是工具，应由人控制，拒绝技术决定论
│   └── 文章结构概述 (Part I-IV)
├── Part I: 进步的速度
│   ├── AI方法、应用、采纳与扩散的区别与时间尺度
│   ├── AI在安全关键领域的扩散缓慢 (原因：安全限制)
│   ├── 扩散受人类、组织和制度变革速度限制 (历史对比：电气化)
│   ├── 外部世界对AI创新设置速度限制 (能力-可靠性差距、隐性知识、实验成本)
│   ├── 基准测试不能衡量现实世界效用 (结构效度问题)
│   ├── 经济影响可能是渐进的 (反馈循环、价值下降、目标转移)
│   └── AI方法进步本身也存在速度限制 (羊群效应、硬件/成本、基准局限)
├── Part II: 拥有先进AI的世界可能是什么样子
│   ├── 重新定义核心概念：从“智能”到“能力”与“权力”
│   ├── 人类能力并非受限于生物学 (技术增强视角)
│   ├── 游戏提供了关于超级智能的误导性直觉 (速度 vs 其他、不可约误差)
│   ├── 控制的多样性：超越对齐与人机回圈 (审计、监控、系统安全、网络安全原则等)
│   └── 预测：人类工作转向AI控制与任务规范 (卡车司机例子，市场与监管驱动)
├── Part III: 风险
│   ├── 风险分类：事故、军备竞赛、滥用、失控、系统性风险
│   ├── 事故：主要责任在部署者/开发者，市场与监管可缓解
│   ├── 军备竞赛：历史常见，行业 специфичн, 可通过监管解决 (公司/国家层面分析)
│   ├── 滥用：主要防御应在下游，模型对齐效果有限 (情境依赖性)
│   │   └── AI对防御亦有助益 (攻防平衡视角)
│   ├── 灾难性失控 (Misalignment)：推测性风险 (下游防御、现实部署过滤器、欺骗性对齐是工程问题)
│   └── 系统性风险：若AI为普通技术则更重要 (偏见、就业、不平等、权力集中等)
├── Part IV: 政策
│   ├── 政策制定的挑战：不确定性与世界观分歧 (超级智能 vs 普通技术)
│   │   └── 对妥协与成本效益分析的批判 (概率问题、量化困难、正当性原则)
│   ├── 政策目标：减少不确定性 (研究资助、监控、证据指导、证据收集)
│   ├── 核心政策方法：韧性 (Resilience)
│   │   ├── 定义与目标
│   │   └── 四类韧性策略 (社会韧性、先决条件、无悔干预、促进竞争/多中心治理)
│   ├── 对不扩散 (Nonproliferation) 的批判
│   │   ├── 执行困难、导致单点故障与脆弱性
│   │   └── 不扩散作为一种有害心态及其干预措施
│   └── 政策目标：实现AI的益处
│       ├── 扩散需要实验与灵活监管
│       ├── 监管可促进扩散 (法律确定性)
│       ├── 投资于自动化的补充品 (素养、数据、基础设施)
│       ├── 关注公平分配与补偿
│       └── 公共部门需谨慎平衡采纳速度与风险
└── 最终思考
    └── 重申“AI作为普通技术”的世界观及其构成要素，呼吁相互理解
```



## 作为普通技术的人工智能 (AI as Normal Technology)

我们阐述了一种将人工智能（AI）视为普通技术的愿景。将AI视为普通技术并非低估其影响——即使是像电力和互联网这样具有变革性的通用技术，在我们的概念中也是“普通的”。但这与关于AI未来的乌托邦和反乌托邦愿景形成对比，后者普遍倾向于将其视为一个独立的物种，一个高度自主、可能具有超级智能的实体。¹

“AI是普通技术”这一论断包含三层含义：对当前AI的**描述**，对AI可预见未来的**预测**，以及关于我们应如何对待它的**规范**。我们将AI视为一种我们可以并且应该保持控制的工具，并认为实现这一目标并不需要激进的政策干预或技术突破。我们认为，将AI视为类人智能在当前既不准确，也无助于理解其社会影响，在我们对未来的设想中也不太可能如此。²

普通技术的框架关乎技术与社会的关系。它拒绝技术决定论，尤其是AI自身作为决定其未来的能动者的观念。它借鉴了过去技术革命的经验教训，例如技术采纳和扩散的缓慢而不确定的特性。它还强调了AI在社会影响方面过去与未来轨迹的连续性，以及制度在塑造这一轨迹中的作用。

在**第一部分**，我们解释了为什么我们认为变革性的经济和社会影响将是缓慢的（以几十年为时间尺度），并对AI方法、AI应用和AI采纳做出了关键区分，认为这三者发生在不同的时间尺度上。

在**第二部分**，我们讨论了在一个拥有先进AI（但并非我们认为通常概念化下不连贯的“超级智能”AI）的世界中，人类与AI之间潜在的劳动分工。在这个世界中，控制权主要掌握在人和组织手中；实际上，人们工作中越来越大的比例是AI控制。

在**第三部分**，我们审视了将AI视为普通技术对AI风险的影响。我们分析了事故、军备竞赛、滥用和失控（misalignment），并认为与将AI视为类人智能相比，将AI视为普通技术会导致关于风险缓解措施的根本不同结论。

当然，我们无法确定我们的预测，但我们的目标是描述我们认为的中间结果。我们没有尝试量化概率，但我们试图做出能够告诉我们AI是否表现得像普通技术的预测。

在**第四部分**，我们讨论了对AI政策的影响。我们主张将减少不确定性作为首要政策目标，并将韧性（resilience）作为应对灾难性风险的 overarching 方法。我们认为，基于控制超级智能AI的困难而采取的激进干预措施，如果AI最终证明是普通技术，实际上会使情况变得更糟——其负面影响可能类似于先前在资本主义社会中部署的技术（如不平等）所带来的影响。³

我们在第二部分描述的世界中，AI比今天先进得多。我们并非声称AI的进步——或人类的进步——将在那时停止。之后会发生什么？我们不知道。考虑这个类比：在第一次工业革命初期，尝试思考工业化世界会是什么样子以及如何准备是有用的，但尝试预测电力或计算机会是徒劳的。我们这里的做法类似。由于我们拒绝“快速起飞”的场景，我们不认为设想一个比我们尝试的更遥远的世界是必要或有用的。如果（并且当）我们在第二部分描述的场景成为现实时，我们将能够更好地预测和准备接下来可能发生的事情。

---

## 第一部分：进步的速度

**图 1**：与其他通用技术一样，AI的影响不是在方法和能力改进时显现，而是在这些改进转化为应用并扩散到经济的生产部门时才得以实现。⁴ 每个阶段都有速度限制。

AI的进步会是渐进的，允许人和机构随着AI能力和采纳的增加而适应，还是会出现导致大规模颠覆甚至技术奇点的跳跃？我们处理这个问题的方法是将高风险任务与低风险任务分开分析，并首先分析AI的采纳和扩散速度，然后再回到创新和发明的速度。

我们使用**发明**（invention）指代新的AI方法（如大型语言模型）的发展，这些方法提高了AI执行各种任务的能力。**创新**（innovation）指代利用AI开发消费者和企业可以使用的产品和应用。**采纳**（adoption）指个体（或团队、公司）决定使用一项技术，而**扩散**（diffusion）指更广泛的社会过程，通过这个过程采纳水平得以提高。对于足够颠覆性的技术，扩散可能需要改变公司和组织的结构，以及社会规范和法律。

### AI在安全关键领域的扩散缓慢
在论文《反对预测性优化》（Against Predictive Optimization）中，我们编制了一份包含约50种预测性优化应用的综合列表，即使用机器学习（ML）通过预测个体未来行为或结果来做出关于个体的决策。⁵ 这些应用中的大多数，如犯罪风险预测、保险风险预测或虐待儿童预测，被用于做出对人们有重要后果的决策。

虽然这些应用激增，但存在一个关键的细微差别：在大多数情况下，使用的是几十年前的统计技术——简单的、可解释的模型（主要是回归）和相对较小的人工特征集。更复杂的机器学习方法，如随机森林，很少使用，而现代方法，如Transformer，则无处可寻。

换句话说，在这一广泛的领域集合中，AI的扩散比创新落后了几十年。一个主要原因是安全——当模型更复杂且更难理解时，很难在测试和验证过程中预见到所有可能的部署条件。一个很好的例子是Epic的败血症预测工具，尽管在内部验证时看似具有高准确性，但在医院中的表现却差得多，漏掉了三分之二的败血症病例，并因错误警报而使医生不堪重负。⁶

Epic的败血症预测工具之所以失败，是因为当模型复杂且特征集不受约束时，很难捕捉到错误。⁷ 特别是，用于训练模型的一个特征是医生是否已经开了抗生素——用于治疗败血症。换句话说，在测试和验证期间，模型使用了未来的特征，依赖于一个与结果有因果关系的变量。当然，这个特征在部署期间是不可用的。可解释性和审计方法无疑会改进，使我们能更好地捕捉这些问题，但我们目前还没有达到那个水平。

就生成式AI而言，即使是事后看来极其明显的失败，在测试期间也没有被发现。一个例子是早期的必应聊天机器人“Sydney”，在长时间对话中失控；开发者显然没有预料到对话可能持续超过几轮。⁸ 同样，Gemini图像生成器似乎从未在历史人物上进行过测试。⁹ 幸运的是，这些都不是高风险的应用。

需要更多的实证工作来理解各种应用中的创新-扩散滞后及其原因。但是，就目前而言，我们在先前工作中分析的证据与以下观点一致：在高风险任务中，已经存在极其强大的与安全相关的速度限制。这些限制通常通过法规强制执行，例如FDA对医疗设备的监管，以及更新的立法，如欧盟AI法案，该法案对高风险AI提出了严格要求。¹⁰ 事实上，存在（可信的）担忧，即现有对高风险AI的监管如此繁重，可能导致“失控的官僚主义”。¹¹ 因此，我们预测，缓慢扩散将继续是高风险任务的常态。

无论如何，当出现可以使用AI处理高风险事务的新领域时，我们可以而且必须对其进行监管。一个很好的例子是2010年的“闪电崩盘”，其中自动高频交易被认为起了一定作用。这导致了新的交易限制，如熔断机制。¹²

### 扩散受到人类、组织和制度变革速度的限制
即使在安全关键领域之外，AI的采纳也比普遍的说法要慢。例如，一项研究因发现2024年8月有40%的美国成年人使用生成式AI而成为头条新闻。¹³ 但是，由于大多数人使用频率不高，这仅转化为0.5%-3.5%的工作时间（以及劳动生产率提高0.125-0.875个百分点）。

甚至不清楚今天的扩散速度是否比过去更快。上述研究报告称，美国生成式AI的采纳速度快于个人电脑（PC）的采纳速度，40%的美国成年人在首个大众市场产品发布后两年内采纳了生成式AI，而PC则是在三年内达到20%。但这种比较没有考虑到采纳强度（使用小时数）的差异，也没有考虑到购买PC的高成本与访问生成式AI的低成本。¹⁴ 根据我们衡量采纳的方式，生成式AI的采纳很可能比PC采纳慢得多。

技术采纳速度未必在加快的说法可能看起来令人惊讶（甚至明显错误），因为数字技术可以同时触达数十亿设备。但重要的是要记住，采纳关乎软件*使用*，而非*可用性*。即使一个新的基于AI的产品立即在网上免费发布给任何人使用，人们也需要时间来改变他们的工作流程和习惯，以利用新产品的好处并学会避免风险。

因此，扩散的速度固有地受到个体、组织和机构适应技术速度的限制。这是我们在过去的通用技术中也看到的趋势：扩散发生在几十年而不是几年内。¹⁵

例如，Paul A. David对电气化的分析表明，生产力的好处花了数十年才完全实现。¹⁶ 在爱迪生第一个中央发电站建成后的近40年里，电动发电机“无处不在，唯独不在生产力统计数据中”。¹⁷ 这不仅仅是技术惯性；工厂主发现电气化并没有带来显著的效率提升。

最终实现效益提升的是围绕生产线逻辑重新设计整个工厂布局。除了工厂建筑的改变，扩散还需要改变工作场所组织和过程控制，这只能通过跨行业的实验来发展。工人们因此获得了更多的自主权和灵活性，这也需要不同的招聘和培训实践。

### 外部世界对AI创新设置了速度限制
诚然，AI的技术进步一直很快，但当我们区分AI方法和应用时，情况就远不那么明朗了。

我们将AI方法的进展概念化为一个**通用性阶梯**（ladder of generality）。¹⁸ 这个阶梯上的每一步都建立在下面的基础上，并反映了向更通用计算能力的迈进。也就是说，它减少了让计算机执行新任务所需的程序员工作量，并增加了在给定程序员（或用户）工作量下可以执行的任务集合；见图2。例如，机器学习通过省去程序员为解决每个新任务而设计逻辑的需要，仅需要收集训练示例，从而提高了通用性。

人们很容易得出结论，随着我们构建更多阶梯层级，开发特定应用所需的工作量将持续减少，直到我们达到通用人工智能（AGI），后者通常被概念化为一个开箱即用、能够完成所有事情的AI系统，从而完全省去了开发应用的需要。

在某些领域，我们确实看到了应用开发工作量减少的趋势。在自然语言处理中，大型语言模型使得开发语言翻译应用变得相对简单。或者考虑游戏：AlphaZero可以通过自我对弈学会下诸如国际象棋之类的游戏，并且比任何人类都强，只需要游戏规则的描述和足够的计算能力——这与过去开发游戏程序的方式相去甚远。

**图 2**：计算中的通用性阶梯。对于某些任务，更高的阶梯层级需要更少的程序员工作量来让计算机执行新任务，并且可以用给定的程序员（或用户）工作量执行更多的任务。¹⁹

然而，在高风险、难以模拟且错误代价高昂的现实世界应用中，这种趋势并未出现。考虑自动驾驶汽车：在许多方面，其发展轨迹类似于AlphaZero的自我对弈——技术改进使其能在更真实的条件下驾驶，这使得能够收集更好和/或更真实的数据，进而导致技术改进，完成反馈循环。但这个过程花费了二十多年，而不是AlphaZero的几个小时，因为安全考虑限制了该循环每次迭代相对于前一次的扩展程度。²⁰

这种“能力-可靠性差距”（capability-reliability gap）反复出现。它一直是构建能够自动化现实世界任务的有用AI“代理”（agents）的主要障碍。²¹ 需要明确的是，许多设想使用代理的任务，如预订旅行或提供客户服务，其风险远低于驾驶，但其代价仍然足够高，以至于让代理从现实世界经验中学习并非易事。

非安全关键应用中也存在障碍。总的来说，组织中的许多知识是隐性的，没有被记录下来，更不用说以可以被动学习的形式存在。这意味着这些发展反馈循环必须在每个行业中发生，对于更复杂的任务，甚至可能需要在不同组织中分别进行，从而限制了快速并行学习的机会。并行学习可能受限的其他原因包括隐私问题：组织和个人可能不愿与AI公司分享敏感数据，法规也可能限制在医疗等情境下可以与第三方共享的数据类型。

AI中的“惨痛教训”（bitter lesson）是，利用计算能力增长的通用方法最终会大幅超越利用人类领域知识的方法。²² 这是关于**方法**的有价值的观察，但常常被误解为涵盖了**应用开发**。在基于AI的产品开发背景下，“惨痛教训”从未接近过真实情况。²³ 考虑社交媒体上的推荐系统：它们由（日益通用的）机器学习模型驱动，但这并未消除手动编写业务逻辑、前端和其他组件的需要，这些组件加起来可能有百万行代码级别。

当我们不仅需要AI从现有的人类知识中学习，还需要推动知识边界时，会遇到进一步的限制。²⁴ 我们最有价值的知识类型之一是科学和社会科学知识，它们通过技术和大规模社会组织（如政府）推动了文明的进步。AI要推动这类知识的边界需要什么？很可能需要与人或组织互动，甚至进行实验，范围从药物测试到经济政策。在这里，由于实验的社会成本，知识获取的速度存在硬性限制。社会可能不会（也不应该）允许为AI开发而快速扩大实验规模。

### 基准测试不能衡量现实世界的效用
方法与应用的区分对于我们如何衡量和预测AI进展具有重要意义。AI基准测试对于衡量**方法**的进展很有用；不幸的是，它们常常被误解为衡量**应用**的进展，这种混淆是关于即将到来的经济转型的大量炒作的驱动因素。

例如，虽然据报道GPT-4在律师资格考试中取得了前10%的成绩，但这几乎不能说明AI从事法律实践的能力。²⁵ 律师资格考试过分强调学科知识，而低估了在标准化、计算机管理的格式中难以衡量的现实世界技能。换句话说，它恰恰强调了语言模型擅长的方面——检索和应用记忆的信息。

更广泛地说，那些会对法律行业产生最显著改变的任务，也是最难评估的任务。对于像按法律领域对法律请求进行分类这样的任务，评估很简单，因为有明确的正确答案。但对于涉及创造力和判断的任务，如准备法律文件，没有唯一的正确答案，理性的人们可能会在策略上存在分歧。正是这些后一类任务，如果自动化，将对该行业产生最深远的影响。²⁶

这种观察绝不限于法律领域。另一个例子是，AI在独立编码问题上表现出色，与其在现实世界软件工程中的影响（难以衡量但似乎不大）之间存在差距。²⁷ 即使是备受推崇的、超越玩具问题的编码基准测试，为了量化和使用公开数据进行自动评估，也必须忽略现实世界软件工程的许多维度。²⁸

这种模式反复出现：一项任务越容易通过基准测试来衡量，它就越不可能代表定义专业实践的那种复杂的、情境化的工作。通过严重依赖能力基准来理解AI进展，AI社区持续高估了该技术的现实世界影响。

这是一个“结构效度”（construct validity）的问题，指的是测试是否真正衡量了它打算衡量的内容。²⁹ 衡量潜在应用现实世界效用的唯一可靠方法是实际构建该应用，然后在现实场景中与专业人士一起测试（根据预期用途，替代或增强他们的劳动）。这类“提升”（uplift）研究通常确实表明，许多职业的专业人士从现有AI系统中受益，但这种好处通常是适度的，更多是关于**增强**而非**替代**，这与基于考试等静态基准得出的结论截然不同³⁰（少数职业如文案撰稿人和翻译员确实经历了大量失业³¹）。

总之，虽然基准测试对于跟踪AI方法的进展很有价值，但我们应该关注其他类型的指标来跟踪AI的影响（图1）。在衡量采纳时，我们必须考虑AI使用的**强度**。应用的类型也很重要：增强与替代，以及高风险与低风险。

确保结构效度的困难不仅困扰着基准测试，也困扰着预测，这是人们试图评估（未来）AI影响的另一种主要方式。确保有效预测，避免模棱两可的结果极其重要。预测界实现这一点的方式是通过定义相对狭窄技能方面的里程碑，例如考试成绩。例如，Metaculus上关于“人机智能对等”（human-machine intelligence parity）的问题是根据在数学、物理和计算机科学考试题上的表现来定义的。基于这个定义，预测者预测到2040年实现“人机智能对等”的可能性为95%也就不足为奇了。³²

不幸的是，这个定义被稀释得如此厉害，以至于对于理解AI的影响意义不大。正如我们上面看到的法律和其他专业基准测试一样，AI在考试上的表现结构效度如此之低，甚至无法让我们预测AI是否会取代专业工作者。

### 经济影响很可能是渐进的
认为AI发展可能产生突然、剧烈的经济影响的一个论点是，通用性的提高可能导致经济中大量任务变得可自动化。这与通用人工智能（AGI）的一个定义有关——一个能够执行所有具有经济价值任务的统一系统。

根据普通技术的观点，这种突然的经济影响是不太可能的。在前几节中，我们讨论了一个原因：AI方法的突然改进当然是可能的，但不会直接转化为经济影响，后者需要创新（在应用开发的意义上）和扩散。

创新和扩散发生在一个反馈循环中。在安全关键应用中，这个反馈循环总是缓慢的，但即使在安全之外，也有许多原因使其可能缓慢。对于过去的通用技术，如电力、计算机和互联网，各自的反馈循环持续了几十年，我们应该预期AI也会发生同样的情况。

另一个支持渐进经济影响的论点是：一旦我们自动化某事物，其生产成本和价值，与人力成本相比，往往会随着时间的推移而急剧下降。随着自动化程度的提高，人类会适应，并将专注于尚未自动化的任务，也许是今天不存在的任务（在第二部分我们描述了这些任务可能是什么样子）。

这意味着AGI的目标会随着自动化程度的提高而不断 redefining 哪些任务具有经济价值而持续后移。即使人类今天所做的每一项任务有朝一日都可能被自动化，这并不意味着人类劳动将变得多余。

所有这些都表明，在某个特定时刻自动化经济的绝大部分是不太可能的。这也意味着强大的AI将在不同行业的不同时间尺度上产生影响。

### AI方法进步的速度限制
我们关于AI影响缓慢性的论点基于创新-扩散反馈循环，并且即使AI方法的进展可以任意加速也适用。我们认为好处和风险主要源于AI的**部署**而非**开发**；因此，AI方法的进展速度与影响问题并不直接相关。尽管如此，讨论同样适用于方法开发的速度限制仍然值得。

AI研究的产出一直在指数级增长，arXiv上AI/ML论文的发表率呈现出不到两年的倍增时间。³³ 但这种数量的增加如何转化为进展尚不清楚。衡量进展的一个指标是核心思想的更替率。不幸的是，在其整个历史中，AI领域表现出高度的围绕流行思想的**羊群效应**，以及对非流行思想的探索（事后看来）不足。一个显著的例子是神经网络研究被边缘化了数十年。

当前时代是否不同？虽然思想以越来越快的速度增量累积，但它们是否正在推翻已确立的思想？Transformer架构在过去十年大部分时间里一直是主导范式，尽管其众所周知的局限性。通过分析241个学科的超过10亿次引用，Johan S.G. Chu和James A. Evans表明，在论文数量较多的领域，新思想突破**更难**，而不是更容易。这导致了“经典的僵化”（ossification of canon）。³⁴ 或许这种描述适用于当前AI方法研究的状态。

许多其他速度限制也是可能的。历史上，深度神经网络技术部分受到硬件不足的阻碍，特别是图形处理单元（GPU）。计算和成本限制继续与新范式相关，包括推理时扩展。新的减速可能会出现：最近的迹象表明，行业内开放知识共享的文化正在发生转变。

AI进行AI研究是否能提供缓解还有待观察。也许方法的递归自我改进是可能的，从而导致方法的无限加速。但请注意，AI开发已经严重依赖AI。更有可能的是，我们将继续看到自动化在AI开发中的作用逐渐增加，而不是出现实现递归自我改进的单一、不连续的时刻。³⁵

早些时候，我们认为基准测试给出了关于AI应用效用的误导性画面。但它们可以说也导致了对方法进展速度的过度乐观。一个原因是很难设计出在当前进展范围之外有意义的基准。图灵测试在几十年里一直是AI的北极星，因为假设任何通过它的系统在重要方面都将是类人的，并且我们将能够使用这样的系统来自动化各种复杂任务。既然大型语言模型可以说通过了它，却仅微弱地满足了测试背后的期望，其重要性已经减弱。³⁶

与登山的类比很贴切。每当我们解决一个基准（到达我们认为是顶峰的地方），我们就会发现基准的局限性（意识到我们处于“假顶峰”），并构建一个新的基准（将目光投向我们现在认为是顶峰的地方）。这导致了“移动球门”的指责，但考虑到基准测试的内在挑战，这正是我们应该预料到的。

AI先驱们认为AI（我们现在称为AGI）的两大挑战是（我们现在称为）硬件和软件。在构建了可编程机器之后，有一种明显的感觉，即AGI近在咫尺。1956年达特茅斯会议的组织者希望通过一次“为期2个月，10人参与”的努力，在该目标上取得重大进展。³⁷ 今天，我们在通用性阶梯上攀登了更多层级。我们经常听到，构建AGI所需要的只是扩展、通用AI代理或样本高效学习。

但值得记住的是，看起来是一步的东西可能并非如此。例如，可能不存在一个单一的突破性算法能够在所有情境下实现样本高效学习。事实上，大型语言模型中的上下文学习已经是“样本高效”的，但仅适用于有限的任务集。³⁸

---

## 第二部分：拥有先进AI的世界可能是什么样子
我们认为，依赖“智能”和“超级智能”这些模糊的概念，已经模糊了我们清晰思考拥有先进AI世界的能力。通过将智能分解为不同的基础概念——**能力**（capability）和**权力**（power），我们反驳了在拥有“超级智能”AI的世界中人类劳动将变得多余的观点，并提出了一个替代愿景。这也为我们在第三部分讨论风险奠定了基础。

### 人类能力不受生物学限制
AI能否超越人类智能？如果能，能超越多少？根据一种流行的观点，其程度将难以想象。这通常通过在智能谱系上比较不同物种来描绘。

**图 3**：通过递归自我改进AI实现的智能爆炸是一个常见的担忧，通常用类似这样的图表来描绘。图表重绘。³⁹

然而，这张图存在概念和逻辑上的缺陷。在概念层面上，智能——尤其是作为不同物种之间的比较——没有明确定义，更不用说在单一维度上衡量了。⁴⁰

更重要的是，对于分析AI的影响而言，重要的属性不是**智能**，而是**权力**——改变环境的能力。为了清晰地分析技术（特别是日益通用的计算技术）的影响，我们必须研究技术如何影响人类的权力。当我们从这个角度看问题时，一个完全不同的画面出现了。

**图 4**：分析技术对人类权力的影响。我们之所以强大，不是因为我们的智能，而是因为我们用来增强能力的**技术**。

这种视角的转变阐明了人类一直使用技术来增强我们控制环境的能力。祖先人类与现代人类之间几乎没有生物学或生理学上的差异；相反，相关的差异是改进的知识和理解、工具、技术，以及确实存在的AI。从某种意义上说，能够改变地球及其气候的现代人类，相对于前技术时代的人类，就是“超级智能”的存在。不幸的是，许多分析AI超级智能风险的基础文献在使用“智能”一词时缺乏精确性。

**图 5**：关于AI能力提升导致失控的两种因果链观点。

一旦我们停止使用“智能”和“超级智能”这两个术语，事情就变得清晰多了（图5）。担忧在于，如果AI能力持续无限增长（无论它们是否类人或超人，这都无关紧要），它们可能导致AI系统拥有越来越大的权力，进而导致失控。如果我们承认能力可能会无限增长（我们承认），那么我们防止失控的选择是在两个因果步骤中的一个进行干预。

超级智能观点对图5中的第一个箭头持悲观态度——即阻止能力任意强大的AI系统获得足以构成灾难性风险的权力——而是专注于**对齐**（alignment）技术，试图阻止权力任意强大的AI系统做出违背人类利益的行为。我们的观点恰恰相反，正如我们在本文其余部分所阐述的。

### 游戏为超级智能的可能性提供了误导性的直觉
不再强调智能不仅仅是一种修辞手法：我们认为，在任何有意义的“智能”定义下，AI都不比借助AI行动的人类更智能。人类智能之所以特殊，在于我们使用工具以及将其他智能融入自身的能力，它无法被连贯地置于智能谱系上。

人类能力确实有一些重要的局限性，特别是**速度**。这就是为什么机器在国际象棋等领域显著优于人类，并且在人机团队中，人类几乎无法做得比简单地听从AI更好。但在大多数领域，速度限制是无关紧要的，因为不需要高速顺序计算或快速反应时间。

在少数需要超人速度的现实世界任务中，例如核反应堆控制，我们擅长构建范围严格限定的自动化工具来完成高速部分，而人类保留对整个系统的控制。

我们基于这种对人类能力的看法提供一个预测。我们认为，现实世界中相对较少的认知任务中，人类的局限性如此明显，以至于AI能够远远超越人类表现（就像AI在国际象棋中那样）。在许多其他领域，包括一些与对AI表现寄予厚望和担忧相关的领域，我们认为存在很高的“不可约误差”（irreducible error）——由于现象固有的随机性而无法避免的误差——而人类的表现基本上接近这个极限。⁴¹

具体来说，我们提出两个这样的领域：**预测**和**说服**。我们预测AI将无法在预测地缘政治事件（比如选举）方面显著优于受过训练的人类（特别是人类团队，尤其是如果辅以简单的自动化工具）。我们对说服人们做出违背自身利益的行为这一任务做出同样的预测。

说服中的自身利益方面是一个关键问题，但常常被低估。作为一个常见模式的说明性例子，考虑研究“评估前沿模型的危险能力”，该研究评估了语言模型说服人的能力。⁴² 他们的一些说服测试对被说服的对象来说是无成本的；他们只是在与AI互动结束时被问及是否相信某个说法。其他测试有小成本，例如放弃给慈善机构的20英镑奖金（当然，捐赠给慈善机构是人们经常自愿做的事情）。因此，这些测试未必能告诉我们AI说服人们执行某些危险任务的能力。值得称赞的是，作者承认了这种生态效度的缺乏，并强调他们的研究不是“社会科学实验”，而仅仅旨在评估模型能力。⁴³ 但这样一来，就不清楚这种脱离情境的能力评估是否具有任何安全意义，然而它们通常被误解为具有安全意义。

要使我们的预测精确，需要一些谨慎——对于众所周知但次要的人类局限性，如缺乏校准（在预测的情况下）或耐心有限（在说服的情况下），应该允许多少余地尚不清楚。

### 控制有多种形式
如果我们假定存在超级智能，控制问题就会让人联想到建造一个“宇宙大脑”然后将其关在盒子里的比喻，这是一个可怕的前景。但是，如果我们是正确的，即AI系统在能力上不会显著超过借助AI行动的人类，那么控制问题就更容易处理了，特别是如果超人说服力被证明是毫无根据的担忧。

关于AI控制的讨论往往过度关注少数狭隘的方法，包括模型对齐（model alignment）和让人类参与决策（human-in-the-loop）。⁴⁴ 我们可以粗略地将这些视为两个极端：在系统运行期间完全将安全决策委托给AI，以及让一个人类复核每一个决策。这些方法有其作用，但非常有限。在第三部分，我们解释了我们对模型对齐的怀疑。我们所说的“人类参与决策”（human-in-the-loop control）是指一个系统中，每个AI的决策或行动都需要人类的审查和批准。在大多数情况下，这种方法大大削弱了自动化的好处，因此要么演变成人类扮演橡皮图章的角色，要么被安全性较低的解决方案所取代。⁴⁵ 我们强调，人类参与决策**不等于**人类对AI的监督；它是一种特定的监督模型，而且是一种极端的模型。

幸运的是，在这两个极端之间存在许多其他形式的控制，例如**审计**（auditing）和**监控**（monitoring）。审计允许在部署前和/或定期评估AI系统在多大程度上实现了其既定目标，使我们能够在灾难性故障发生前预见到它们。监控允许在系统属性偏离预期行为时进行实时监督，从而在真正需要时进行人为干预。

其他想法来自**系统安全**（system safety），这是一个专注于通过系统性分析和设计来预防复杂系统中事故的工程学科。⁴⁶ 例子包括**故障安全**（fail-safes），确保系统在发生故障时默认进入安全状态，例如预定义的规则或硬编码的操作；以及当超过预定义的安​​全阈值时自动停止操作的**熔断器**（circuit breakers）。其他技术包括关键部件的**冗余**（redundancy）和系统行为**安全属性的验证**（verification）。

其他计算领域，包括**网络安全**、**形式化验证**和**人机交互**，也是控制技术的丰富来源，这些技术已成功应用于传统软件系统，并且同样适用于AI。在网络安全中，“最小权限”原则确保行为者仅能访问其任务所需的最低资源。访问控制阻止处理敏感数据和系统的人员访问非其工作所需的机密信息和工具。我们可以在重要场景中为AI系统设计类似的保护措施。形式化验证方法确保安全关键代码按照其规范工作；它现在正被用于验证AI生成代码的正确性。⁴⁷ 从人机交互中，我们可以借鉴诸如设计系统使得状态改变的操作是**可逆的**（reversible）等想法，即使在高度自动化的系统中也能让人类保持有意义的控制。

除了将其他领域的现有思想应用于AI控制之外，技术性AI安全研究也产生了许多新想法。⁴⁸ 例子包括使用语言模型作为自动裁判来评估提议行动的安全性，开发能够根据不确定性或风险水平适当地将决策上报给人类操作员的系统，设计使得其活动对人类可见且易于理解的代理系统，以及创建由更简单、更可靠的AI系统监督更强大但可能不可靠的AI系统的分层控制结构。⁴⁹

技术性AI安全研究有时是根据模糊且不切实际的目标来评判的，即保证未来的“超级智能”AI将“与人类价值观保持一致”。从这个角度来看，它往往被视为一个未解决的问题。但从让AI系统的开发者、部署者和操作者更容易降低事故可能性的角度来看，技术性AI安全研究已经产生了大量想法。我们预测，随着先进AI的开发和采用，将会出现越来越多的创新，以寻找新的人类控制模型。

随着越来越多的体力 Tâches 和认知任务变得适合自动化，我们预测人类工作和任务中与AI控制相关的百分比将不断增加。如果这看起来很激进，请注意，这种对工作概念近乎彻底的重新定义以前也发生过。在工业革命之前，大多数工作涉及体力劳动。随着时间的推移，越来越多的体力任务被自动化，这一趋势仍在继续。在这个过程中，发明了许多不同的操作、控制和监控物理机器的方法，今天人类在工厂里所做的事情是“控制”（监控自动化装配线、编程机器人系统、管理质量控制检查点、协调对设备故障的响应）和一些需要机器尚未具备的认知能力或灵巧性的任务的结合。

Karen Levy描述了这种转变在AI和卡车司机案例中已经如何展开：

> 卡车司机的日常工作远不止驾驶卡车。卡车司机监控他们的货物，在冷藏卡车中保持食物在合适的温度，将货物牢固地固定在平板拖车上。他们每天进行两次强制性安全检查。他们负责保护贵重物品。他们维护和修理卡车——有些是常规的，有些则不然。当卡车司机到达终点站或交货点时，他们不只是卸下东西就离开：有些人装卸货物；他们与客户交谈；他们处理文书工作；他们可能会花数小时进行“场内移动”（等待可用的卸货平台并移动到那里，就像飞机在繁忙机场所做的那样）。这些任务中的一些能否被智能系统消除？当然有些可以并且将会——但工作的这些组成部分比高速公路驾驶更难自动化，并且会晚得多。⁵⁰

除了AI控制，**任务规范**（task specification）很可能成为人类工作内容的更大部分（取决于我们对控制的理解范围有多广，规范可以被视为控制的一部分）。任何尝试过外包软件或产品开发的人都知道，明确无误地说明期望什么，结果证明是整个工作中令人惊讶的一大部分。因此，人类劳动——规范和监督——将在执行不同任务的AI系统之间的边界上运作。消除一些效率瓶颈，让AI系统自主地“端到端”完成更大的任务将是一个永恒的诱惑，但这会增加安全风险，因为它会降低可理解性和控制力。这些风险将成为防止让渡过多控制权的自然制约。

我们进一步预测，这种转变将主要由**市场力量**驱动。控制不佳的AI将过于容易出错，以至于在商业上不可行。但**监管**可以并且应该加强组织保持人类控制的能力和必要性。

---

## 第三部分：风险
我们考虑五种类型的风险：**事故**（accidents）、**军备竞赛**（arms races，导致事故）、**滥用**（misuse）、**失控/未对齐**（misalignment）和**非灾难性但系统性的风险**。

我们已经在上面讨论过事故。我们的观点是，就像其他技术一样，部署者和开发者应该对减轻AI系统中的事故负主要责任。他们这样做的有效性取决于他们的激励机制，以及缓解方法的进展。在许多情况下，市场力量将提供足够的激励，但安全监管应该填补任何空白。至于缓解方法，我们回顾了AI控制研究如何迅速发展。

这种乐观评估可能不成立的原因有几个。首先，可能存在军备竞赛，因为AI的竞争优势如此之大，以至于它们成为通常模式的例外。我们将在下面讨论这一点。

其次，部署AI的公司或实体可能如此庞大和强大，以至于知道如果它对事故缓解持不良态度最终会倒闭也无济于事——它可能会拖垮整个文明。例如，控制几乎所有消费设备的AI代理的不当行为可能导致灾难性的广泛数据丢失。虽然这当然是可能的，但这种权力集中比AI事故的可能性是一个更大的问题，这正是为什么我们的政策方法强调韧性和去中心化（第四部分）。

最后，也许即使是一个相对不起眼的部署者造成的AI控制失败也可能导致灾难性风险——比如因为一个AI代理“逃脱”，复制自身等等。我们认为这是一种失控风险，并在下面讨论。

在第三部分的其余部分，我们通过将AI视为普通技术的视角来考虑四种风险——军备竞赛、滥用、失控和非灾难性但系统性的风险。

### 军备竞赛是一个老问题
AI军备竞赛是指两个或多个竞争者——公司、不同国家的政策制定者、军队——部署日益强大的AI，而监督和控制不足。危险在于，更安全的参与者将被风险更高的参与者淘汰。基于上述原因，我们不太担心AI方法开发中的军备竞赛，而更担心AI应用的**部署**。

一个重要的提醒：我们明确将**军事AI**排除在分析之外，因为它涉及机密能力和独特的动态，需要更深入的分析，这超出了本文的范围。

让我们先考虑公司。在安全方面“竞相逐底”（race to the bottom）在历史上各行各业都极为常见，并已被广泛研究；它也非常适合采用众所周知的监管干预措施。例子包括美国服装业的消防安全（20世纪初）、美国肉类加工业的食品安全和工人安全（19世纪末和20世纪初）、美国蒸汽船行业（19世纪）、采矿业（19世纪和20世纪初）以及航空业（20世纪初）。

这些竞赛之所以发生，是因为公司能够将其不良安全行为的**成本外部化**，导致市场失灵。消费者很难评估产品安全（工人也很难评估工作场所安全），因此在没有监管的情况下，市场失灵很常见。但一旦监管迫使公司内化其安全实践的成本，竞赛就会消失。有许多潜在的监管策略，包括侧重于流程（标准、审计和检查）、结果（责任）和纠正信息不对称（标签和认证）的策略。

AI也不例外。自动驾驶汽车提供了一个关于安全与竞争成功之间关系的良好案例研究。考虑四家安全实践各不相同的 主要公司。据报道，Waymo拥有强大的安全文化，强调保守部署和自愿透明度；它也是安全结果方面的领导者。⁵¹ Cruise在部署方面更为激进，安全结果较差。特斯拉也一直很激进，并经常被指责将其客户当作测试者。最后，Uber的自动驾驶部门以其 notoriously 松懈的安全文化而闻名。

市场成功与安全密切相关。Cruise定于2025年关闭，而Uber则被迫出售其自动驾驶部门。⁵² 特斯拉正面临诉讼和监管审查，其安全态度将给公司带来多大代价还有待观察。⁵³ 我们认为这些相关性是**因果关系**。Cruise的牌照被吊销是其落后于Waymo的一个重要原因，安全也是Uber自动驾驶失败的一个因素。⁵⁴

监管发挥了微小但有益的作用。联邦和州/地方层面的政策制定者都预见到了该技术的潜力，并采取了轻触式和多中心（多个监管机构而非一个）的监管策略。总的来说，他们专注于监督、标准制定和证据收集，牌照吊销这一始终存在的威胁对公司的行为起到了制约作用。

同样，在航空业，AI的整合一直遵循现有的安全标准，而不是为了激励AI采用而降低标准——这主要是因为监管机构有能力惩罚未能遵守安全标准的公司。⁵⁵

简而言之，AI军备竞赛可能会发生，但它们是**特定于行业的**，应通过特定行业的法规来解决。

作为一个与自动驾驶汽车或航空业情况不同的领域案例研究，考虑社交媒体。生成内容源的推荐算法是一种AI。它们被指责造成了许多社会弊病，而社交媒体公司在这些算法系统的设计和部署中可以说低估了安全性。也存在明显的军备竞赛动态，TikTok给竞争对手施加压力，使其信息流更加依赖推荐。⁵⁶ 可以说，市场力量不足以使收入与社会效益保持一致；更糟糕的是，监管机构行动迟缓。原因何在？

社交媒体和交通运输之间的一个显著区别是，当发生危害时，将其归因于产品故障在交通运输案例中相对直接，并且公司会立即遭受声誉损害。但在社交媒体案例中，归因极其困难，甚至研究仍然没有定论且存在争议。领域之间的第二个区别是，我们已经有超过一个世纪的时间来制定交通安全的标准和期望。在汽车的早期几十年里，安全并不被认为是制造商的责任。⁵⁷

AI足够广泛，其未来的一些应用将更像交通运输，而另一些则更像社交媒体。这表明在新兴的AI驱动行业和应用中，**主动收集证据和保持透明度**的重要性。我们在第四部分讨论这个问题。这也显示了“**预期性AI伦理**”（anticipatory AI ethics）的重要性——在 新兴技术生命周期的早期识别伦理问题，制定规范和标准，并利用这些来积极塑造技术的部署，最大限度地减少军备竞赛的可能性。⁵⁸

安全监管在AI案例中可能更困难的一个原因是，如果采纳速度如此之快，以至于监管机构无法在为时已晚之前进行干预。到目前为止，我们还没有看到在高风险任务中AI被快速采纳的例子，即使在没有监管的情况下也是如此，我们在第一部分提出的反馈循环模型或许可以解释原因。新AI应用的采纳率将仍然是一个需要跟踪的关键指标。

与此同时，即使未来扩散速度不加快，监管步伐缓慢也是一个问题。我们在第四部分讨论这个“步调问题”（pacing problem）。

现在让我们考虑国家之间的竞争。政府是否会面临竞争压力，要求其对AI安全采取放任自流的态度？

再次强调，我们的信息是，这不是一个新问题。创新与监管之间的权衡是监管国家反复出现的困境。到目前为止，我们看到了截然不同的方法，例如欧盟强调预防性方法（通用数据保护条例、数字服务法案、数字市场法案和AI法案），而美国则倾向于仅在出现已知危害或市场失灵后才进行监管。⁵⁹

尽管美中军备竞赛的言论甚嚣尘上，但尚不清楚两国的AI监管是否因此放缓。⁶⁰ 在美国，仅2024年就有700项与AI相关的法案在州立法机构提出，其中数十项已经通过。⁶¹ 正如我们在前面部分指出的，大多数高风险行业都受到严格监管，无论是否使用AI都适用。那些声称AI监管是“狂野西部”的人往往过分强调狭隘的、以模型为中心的监管类型。在我们看来，监管机构强调AI的**使用**而非**开发**是适当的（除了我们下面讨论的透明度要求等例外）。

未能充分监管安全采纳将主要通过事故在**本地**产生负面影响，而不是像安全文化松懈的公司可能将安全成本外部化那样。因此，没有直接理由预期国家之间会发生军备竞赛。请注意，由于本节我们关注的是**事故**而非**滥用**，针对外国的网络攻击不在讨论范围之内。我们将在下一节讨论滥用。

与核技术的类比可以阐明这一点。AI经常被比作核武器。但除非我们谈论的是军事AI的风险（我们同意这是一个值得关注的领域，并且本文不考虑），否则这是一个错误的类比。关于部署（其他方面是良性的）AI应用可能导致事故的担忧，正确的类比是**核电**。核武器和核电之间的区别恰好说明了我们的观点——虽然存在核武器军备竞赛，但核电没有对应的竞赛。事实上，由于安全影响是局部感受到的，该技术在许多国家引发了强烈的反弹，普遍认为这严重阻碍了其潜力。

理论上，在大国冲突背景下的政策制定者可能会选择在本地承担安全成本，以确保其AI产业成为全球赢家。再次强调，关注**采纳**而非**开发**，目前没有迹象表明这种情况正在发生。美中军备竞赛的言论一直强烈关注模型开发（发明）。我们没有看到相应的仓促采纳AI的现象。安全界应继续向政策制定者施压，确保这种情况不会改变。国际合作也必须发挥重要作用。

### 针对滥用的主要防御必须位于模型下游
模型对齐（Model alignment）通常被视为防止模型滥用的主要防御手段。目前，这是通过训练后干预实现的，例如使用人类和AI反馈的强化学习。⁶² 不幸的是，使模型对齐以拒绝滥用尝试已被证明极其脆弱。⁶³ 我们认为这种局限性是固有的，不太可能被修复；因此，针对滥用的主要防御必须存在于别处。

根本问题在于，一个能力是否有害取决于**情境**——而模型通常缺乏这种情境信息。⁶⁴

考虑一个攻击者使用AI通过网络钓鱼邮件攻击一家大公司员工的场景。攻击链可能涉及许多步骤：扫描社交媒体资料以获取个人信息，识别公开个人信息的目标，制作个性化钓鱼信息，以及利用收集到的凭证入侵账户。

这些单独的任务本身都不是恶意的。使系统有害的是这些能力的**组合方式**——这些信息只存在于攻击者的编排代码中，而不在模型本身。被要求撰写有说服力邮件的模型无法知道它是被用于营销还是网络钓鱼——因此模型级别的干预将是无效的。⁶⁵

这种模式反复出现：试图制造一个无法被滥用的AI模型，就像试图制造一台不能用于做坏事的计算机一样。模型级别的安全控制要么过于严格（阻止有益用途），要么对能够将看似良性的能力重新用于有害目的的对手无效。

如果我们把AI模型看作一个我们可以将安全决策委托给它的类人系统，那么模型对齐似乎是一种自然的防御。但要使其有效，必须给予模型大量关于用户和情境的信息——例如，广泛访问用户的个人信息将使其更有可能判断用户的意图。但是，当将AI视为普通技术时，这样的架构会**降低**安全性，因为它违反了基本的网络安全原则，如最小权限，并引入了新的攻击风险，如个人数据泄露。

我们并不反对模型对齐。它在减少语言模型产生有害或带偏见的输出方面是有效的，并且在其商业部署中发挥了重要作用。对齐也可以为临时威胁行为者制造障碍。

然而，鉴于模型级别的保护不足以防止滥用，防御必须集中在恶意行为者实际部署AI系统的**下游攻击面**。⁶⁶ 这些防御通常看起来类似于现有的针对非AI威胁的保护措施，只是针对AI增强的攻击进行了调整和加强。

再次考虑网络钓鱼的例子。最有效的防御不是限制邮件撰写（这会损害合法用途），而是检测可疑模式的邮件扫描和过滤系统、针对恶意网站的浏览器级别保护、防止未经授权访问的操作系统安全功能，以及对用户的安全培训。⁶⁷

这些都不涉及对用于生成钓鱼邮件的AI采取行动——事实上，这些下游防御措施经过几十年的发展，已经能有效对抗人类攻击者。⁶⁸ 它们可以而且应该被增强以处理AI赋能的攻击，但基本方法仍然有效。

类似的模式也存在于其他领域：防御AI赋能的网络威胁需要加强现有的漏洞检测程序，而不是试图从源头限制AI能力。同样，关于AI生物风险的担忧最好在采购和筛选阶段解决，以防止制造生物武器。

### AI 对防御很有用
与其仅仅将AI能力视为风险来源，我们应该认识到它们的**防御潜力**。在网络安全领域，AI已经通过自动化漏洞检测、威胁分析和攻击面监控来加强防御能力。⁶⁹

让防御者能够使用强大的AI工具通常会使攻防平衡向有利于他们的方向倾斜。这是因为防御者可以使用AI系统地探测自己的系统，在攻击者利用漏洞之前找到并修复它们。例如，谷歌最近将语言模型整合到其用于测试开源软件的模糊测试（fuzzing）工具中，使他们能够比传统方法更有效地发现潜在的安全问题。⁷⁰

同样的模式也适用于其他领域。在生物安全领域，AI可以增强用于检测危险序列的筛选系统。⁷¹ 在内容审核方面，它可以帮助识别协同影响操作。这些防御性应用表明，为什么限制AI开发可能会适得其反——我们需要强大的AI系统在防御方来对抗AI赋能的威胁。如果我们对语言模型进行对齐，使其在这些任务（例如在关键网络基础设施中查找错误）上变得无用，防御者将失去这些强大的系统。但有动机的对手可以训练自己的AI工具进行此类攻击，导致进攻能力增强而防御能力没有相应提高。

与其仅仅根据进攻能力来衡量AI风险，我们应该关注每个领域的**攻防平衡**等指标。此外，我们应该认识到我们有能力有利地改变这种平衡，并且可以通过投资于防御性应用而不是试图限制技术本身来实现。

### 灾难性失控（Misalignment）是一种推测性风险
失控的AI（Misaligned AI）违背其开发者或用户的意图。（术语“对齐”有多种用法；我们在此排除其他定义。）与滥用场景不同，这里没有恶意用户。与事故不同，系统按设计或指令工作，但设计或指令本身由于完全且正确地指定目标的挑战而未能匹配开发者或用户的意图。与日常的失控情况（如聊天机器人中的有害输出）不同，我们在此关注的是先进AI的失控导致灾难性或生存性危害。

在我们看来，针对失控的主要防御同样位于**下游**。我们之前讨论的针对滥用所需的防御措施——从加固关键基础设施到改善网络安全——也将作为防范潜在失控风险的保护。

在将AI视为普通技术的观点中，灾难性失控是（迄今为止）我们讨论的风险中最具**推测性**（speculative）的一种。但什么是推测性风险——难道所有风险不都是推测性的吗？区别在于两种类型的不确定性，以及相应不同的概率解释。

在2025年初，当天文学家评估小行星YR4在2032年与地球相撞的概率约为2%时，该概率反映了**测量的不确定性**。在这种情况下，实际撞击（在没有干预的情况下）的几率要么是0%，要么是100%。进一步的测量解决了YR4案例中的这种“认知不确定性”（epistemic uncertainty）。相反，当分析师预测未来十年核战争的风险（比如）为10%时，这个数字主要反映了源于未来如何展开的不可知性的“随机不确定性”（stochastic uncertainty），并且相对不太可能通过进一步观察来解决。

我们所说的推测性风险，是指那些存在**认知不确定性**，即不确定真实风险是否为零的风险——这种不确定性可能通过进一步的观察或研究来解决。小行星YR4撞击的影响是一种推测性风险，而核战争则不是。

为了说明为什么灾难性失控是一种推测性风险，考虑一个著名的思想实验，其初衷是为了展示失控的危险。它涉及一个“回形针最大化器”（paperclip maximizer）：一个目标是制造尽可能多回形针的AI。⁷² 担忧在于AI会按字面意思理解目标：它会意识到获取世界上的权力和影响力并控制所有资源将有助于实现其目标。一旦它变得无所不能，它可能会征用世界上所有的资源，包括人类生存所需的资源，来生产回形针。

担心AI系统可能灾难性地误解指令的恐惧，依赖于关于技术如何在现实世界中部署的可疑假设。在一个系统被授予访问重要决策的权限之前很久，它需要在不那么关键的情境中证明其可靠的性能。任何过于字面地解释指令或缺乏常识的系统都会在这些早期测试中失败。

考虑一个更简单的案例：一个机器人被要求“尽快从商店取回形针”。一个字面解释这个指令的系统可能会无视交通法规或试图盗窃。这种行为将导致立即关停和重新设计。**采纳的路径本身就要求在日益重要的情境中展示适当的行为。** 这不是侥幸，而是组织如何采纳技术的一个基本特征。

对此担忧的一个更复杂的版本基于**欺骗性对齐**（deceptive alignment）的概念：这指的是一个系统在评估或部署早期阶段表现出对齐，但在获得足够权力后释放有害行为。在领先的AI模型中已经观察到某种程度的欺骗现象。⁷³

根据超级智能的观点，欺骗性对齐是一个定时炸弹——由于具有超级智能，该系统将能够轻易地挫败任何人类试图检测其是否真正对齐的尝试，并将伺机而动。但是，在普通技术的观点中，欺骗仅仅是一个工程问题，尽管是一个重要的问题，需要在开发过程中和整个部署过程中解决。事实上，它已经是强大的AI模型安全评估的标准部分。⁷⁴

关键的是，AI在这个过程中是有用的，AI的进步不仅使欺骗成为可能，而且也**改进了欺骗的检测**。就像在网络安全领域一样，防御者拥有许多不对称优势，包括能够检查目标系统的内部（这种优势的用处取决于系统的设计方式以及我们在可解释性技术上的投入程度）。另一个优势是**纵深防御**（defense in depth），许多不仅针对滥用，而且针对失控AI的防御将位于AI系统的下游。

对失控的担忧常常假定AI系统将自主运行，在没有人类监督的情况下做出高风险决策。但正如我们在第二部分所论证的，**人类控制**将仍然是AI部署的核心。围绕重要决策的现有制度控制——从财务控制到安全法规——为防止灾难性失控创建了多层保护。

某些技术设计决策比其他决策更容易导致失控。一个以此臭名昭著的场景是使用强化学习来优化单一目标函数（可能被意外地未充分说明或错误说明）在一个长期时间范围内。游戏代理中有一长串有趣的例子，比如一个赛艇代理学会了无限期地在一个区域盘旋以击中相同的目标并得分，而不是前进到终点线。⁷⁵ 需要重申的是，我们认为在开放式的现实世界场景中，以这种方式设计的代理将更多的是**无效**而非**危险**。无论如何，研究不易受规范博弈（specification gaming）影响的替代设计范式是一个重要的研究方向。⁷⁶

简而言之，关于回形针最大化器场景存在非零风险的论点，基于可能成立也可能不成立的假设，并且有理由认为研究可以让我们更好地了解这些假设对于正在构建或设想的AI系统类型是否成立。基于这些原因，我们称之为“推测性”风险，并在第四部分考察这种观点的政策含义。

### 历史表明普通AI可能引入多种系统性风险
虽然上面讨论的风险有可能造成灾难性或生存性后果，但还有一长串AI风险低于此级别，但仍然是大规模和系统性的，超越了任何特定AI系统的直接影响。这些包括系统性地固化偏见和歧视、特定职业的大规模失业、劳动条件恶化、不平等加剧、权力集中、社会信任侵蚀、信息生态系统污染、自由媒体衰落、民主倒退、大规模监控以及助长威权主义。

如果AI是普通技术，那么这些风险变得远比上面讨论的灾难性风险**更重要**。这是因为这些风险源于人和组织利用AI来推进自身利益，而AI仅仅充当了我们社会中现有不稳定因素的放大器。

在变革性技术的历史上，这类社会政治动荡有大量先例。值得注意的是，工业革命导致了快速的大规模城市化，其特点是恶劣的工作条件、剥削和不平等，催生了工业资本主义以及社会主义和马克思主义的兴起作为回应。⁷⁷

我们建议的重点转变大致对应于Kasirzadeh区分的**决定性**（decisive）和**累积性**（accumulative）生存风险。决定性生存风险涉及“公开的AI接管路径，以不可控的超级智能等场景为特征”，而累积性生存风险指的是“关键的AI诱发威胁的逐渐积累，如严重的脆弱性和经济政治结构的系统性侵蚀”。⁷⁸ 但存在重要区别：Kasirzadeh对累积风险的描述在很大程度上仍然依赖于网络攻击者等威胁行为者，而我们的担忧仅仅是关于当前资本主义的路径。而且我们认为这类风险不太可能是生存性的，但仍然极其严重。

---

## 第四部分：政策
AI不同未来——普通技术与潜在不可控的超级智能——之间的分歧给政策制定者带来了一个两难境地，因为针对一种风险的防御措施可能会使另一种风险恶化。我们提供了一套指导原则来应对这种不确定性。更具体地说，政策制定者应重点关注的策略是**韧性**（resilience），即现在采取行动以提高我们应对未来意外发展的能力。政策制定者应拒绝**不扩散**（nonproliferation），因为它违反了我们概述的原则，并降低了韧性。最后，扩散面临的阻力意味着实现AI的好处并非必然，需要政策制定者的行动。

关于AI治理已经有很多论述。我们的目标不是提出一个全面的治理框架；我们仅仅强调将AI视为普通技术的观点的政策含义。

### 不确定性下的政策制定挑战
今天的AI安全讨论的特点是世界观存在深刻分歧。我们认为这些分歧不太可能消失。根深蒂固的阵营已经形成：AI安全联盟早已建立，而那些对灾难性风险持更怀疑态度的人则在2024年凝聚起来，尤其是在关于加州AI安全法案的辩论过程中。⁷⁹ 同样，AI安全阵营的思想根源要古老得多，而采用普通技术范式的学术研究正在逐渐形成；我们自己大部分工作的目标，包括这篇论文，就是将“普通主义”（normalist）思维置于更坚实的思想基础上。⁸⁰

我们支持减少社区两极分化和碎片化的呼吁。⁸¹ 但即使我们改善了讨论的基调，我们很可能仍然面临世界观和认知实践上的差异，这些差异不太可能通过经验得到解决。⁸² 因此，“专家”之间关于AI风险的共识不太可能达成。两个阵营所设想的AI风险场景的性质截然不同，商业行为者抵制这些风险的能力和动机也大相径庭。面对这种不确定性，政策制定者应如何进行？

政策制定中一个自然的倾向是**妥协**。这不太可能奏效。一些干预措施，如提高透明度，对于风险缓解是无条件有益的，不需要妥协（或者更确切地说，政策制定者将不得不平衡行业和外部利益相关者的利益，这基本上是一个正交的维度）。⁸³ 其他干预措施，如**不扩散**，可能有助于遏制超级智能，但通过增加市场集中度而加剧与普通技术相关的风险。⁸⁴ 反之亦然：诸如通过培育开源AI来增强韧性的干预措施将有助于治理普通技术，但有释放失控超级智能的风险。

这种紧张关系是不可避免的。防御超级智能需要人类团结起来对抗一个共同的敌人（可以这么说），集中权力和对AI技术实施中央控制。但我们更担心的是源于人们利用AI实现自身目的的风险，无论是恐怖主义、网络战、破坏民主，还是仅仅——也是最常见的——放大不平等的榨取性资本主义实践。⁸⁵ 防御这类风险需要通过防止权力和资源集中来**增强韧性**（这通常意味着让强大的AI更广泛可用）。

应对不确定性的另一种诱人方法是估计各种结果的概率，然后应用**成本效益分析**。AI安全界在很大程度上依赖于灾难性风险，特别是生存风险的概率估计来指导政策制定。想法很简单：如果我们认为一个结果具有U的主观价值或效用（可以是正或负），并且它有（比如）10%的发生概率，我们可以把它当作确定发生且价值为0.1 * U来处理。然后我们可以将我们可用的每个选项的成本和收益加总，并选择使成本减去收益（即“预期效用”）最大化的选项。

在最近的一篇文章中，我们解释了为什么这种方法不可行。⁸⁶ AI风险概率缺乏有意义的认知基础。有根据的概率估计可以是归纳性的，基于类似过去事件的参考类别，例如汽车保险定价中的车祸。或者它可以是演绎性的，基于对所讨论现象的精确模型，如扑克。不幸的是，在AI风险方面，既没有有用的参考类别，也没有精确的模型。实际上，风险估计是“主观的”——预测者的个人判断。⁸⁷ 由于缺乏任何基础，这些估计往往差异巨大，通常相差几个数量级。

除了概率之外，计算的其他组成部分——各种政策选择的后果，包括不作为的后果——也受到巨大的不确定性影响，不仅在量级上，而且在方向上。没有可靠的方法来量化由于限制AI可用性的政策而放弃的好处，并且我们将在下面论证不扩散可能使灾难性风险变得更糟。

此外，我们赋予某些结果的效用可能取决于我们的道德价值观。例如，有些人可能认为灭绝具有难以想象的巨大负效用，因为它排除了未来可能存在的所有人类生命，无论是物理的还是模拟的。⁸⁸ （当然，涉及无穷大的成本效益分析往往会导致荒谬的结论）。

另一个例子是限制自由（例如要求开发某些AI模型需要许可证）和不限制自由（例如增加开发防御AI风险的资金）的政策之间的不对称性。某些类型的限制违反了自由民主的核心原则，即国家不应基于理性人可以拒绝的有争议的信念来限制人民的自由。**正当性论证**（Justification）对于政府的合法性和权力行使至关重要。⁸⁹ 如何量化违反这一原则的成本尚不清楚。

正当性论证的重要性当然可以在规范层面上争论，但从经验上看，它似乎在迄今为止的AI政策中得到了证实。如前所述，加州的AI安全法规导致了反对该法案的人的凝聚。反对阵营的一些成员是自利的公司，但另一些则是学者和进步倡导者。根据我们的经验，在许多情况下，第二组人的驱动动机是政府被认为超越了其合法权限的界限，因为对于那些不认同该法案未明言前提的人来说，所提供的理由是多么不令人信服。

价值观和信念上不可避免的差异意味着政策制定者必须采取**价值多元主义**（value pluralism），倾向于那些能被持有广泛价值观的利益相关者接受的政策，并试图避免那些可能被利益相关者合理拒绝的自由限制。他们还必须优先考虑**稳健性**（robustness），倾向于那些即使其关键假设被证明不正确，仍然有益，或至少无害的政策。⁹⁰

### 将减少不确定性作为政策目标
虽然由于上述原因无法消除不确定性，但可以减少不确定性。然而，这个目标不应留给专家；政策制定者可以而且应该发挥积极作用。我们推荐五种具体方法。

**图 6**：可以增强关于AI使用、风险和失败的公共信息的几类政策概述。⁹¹

*   **对风险研究进行战略性资助**。当前的AI安全研究过分关注有害能力，并未接纳普通技术的观点。对技术能力下游的问题关注不足。例如，关于威胁行为者实际如何使用AI的知识惊人地缺乏。像AI事件数据库（AI Incident Database）这样的努力存在且有价值，但数据库中的事件来源于新闻报道而非研究，这意味着它们经过了此类事件成为新闻的选择性和有偏见的过程的过滤。⁹²
*   幸运的是，研究资助是一个妥协有益的领域；我们主张增加对风险（和收益）研究的资助，以解决在普通技术观点下更相关的问题。其他可能减少或至少澄清不确定性的研究类型包括证据综合（evidence synthesis）工作以及持有不同世界观的研究人员之间的对抗性合作（adversarial collaborations）。
*   **监控AI的使用、风险和失败**。虽然研究资助可以帮助监控野外的AI，但也可能需要监管和政策——即“寻求证据的政策”（evidence-seeking policies）。⁹³ 我们在图6中建议了一些此类政策。
*   **关于不同类型证据价值的指导**。政策制定者可以为研究界提供关于哪些类型的证据是有用且可操作的更好理解。例如，各种政策制定者和咨询机构已经指出了“边际风险”（marginal risk）框架在分析开源权重模型和专有模型相对风险方面的有用性，这有助于研究人员指导未来的研究。⁹⁴
*   **将证据收集作为首要目标**。到目前为止，我们讨论了专门旨在产生更好证据或减少不确定性的行动。更广泛地说，对证据收集的影响可以被视为评估任何AI政策的一个因素，与最大化收益和最小化风险的影响并列。例如，支持开源权重和开源模型的一个原因可能是推进对AI风险的研究。相反，支持专有模型的一个原因可能是对其使用和部署的监视可能更容易。

### 韧性（Resilience）的理由
Marchant和Stevens描述了治理新兴技术的四种方法；见图7。⁹⁵ 两种是事前方法（ex ante），风险分析和预防（precaution），另外两种是事后方法（ex post），责任（liability）和韧性（resilience）。这些方法各有优缺点，可以相互补充；尽管如此，某些方法显然比其他方法更适合某些技术。

Marchant和Stevens认为（我们也同意），事前方法不适合AI，因为在部署前难以确定风险。责任制表现更好，但也有重要局限性，包括因果关系的不确定性以及它可能对技术发展产生的寒蝉效应。

**图 7**：基于Marchant和Stevens的四种治理新兴技术方法的总结。

他们将韧性定义如下：

> 韧性，其最简单的形式，是系统应对损害的能力。[脚注省略] 韧性方法不一定试图维持稳定或平衡。相反，它认识到复杂系统中的变化是不可避免的，并试图以保护和保存原始系统核心价值和功能的方式来管理和适应这种变化。因此，韧性是“一个系统在经历冲击时仍能基本保持相同功能、结构、反馈，并因此保持其身份的能力”。⁹⁶ 韧性被描述为一种策略，以确保在重大外部冲击或破坏造成损害后实现“软着陆”。⁹⁷

在AI背景下，损害可能源于特定部署系统中的事件，无论这些事件是事故还是攻击。也存在可能导致也可能不导致损害的冲击，包括进攻能力的突然增加（例如使生物恐怖分子能够行动）和能力的突然扩散，例如通过发布开源权重模型或专有模型权重的被盗。在我们看来，韧性既需要**在损害发生时最小化其严重性**，也需要**在冲击发生时最小化损害的可能性**。

韧性结合了事前和事后方法的元素，包括在损害发生前采取行动，以便在损害确实发生时处于更有利的位置来限制损害。许多基于韧性的治理工具有助于缓解“步调问题”（pacing problem），即传统治理方法无法跟上技术发展的速度。

许多针对AI的韧性策略已被提出。它们可以分为四大类。前三类包括“无悔”（no regret）政策，无论AI的未来如何，这些政策都将有所帮助。

1.  **广泛的社会韧性**：加倍努力保护民主基础至关重要，特别是那些被AI削弱的基础，如自由媒体和公平的劳动力市场。AI的进步并非现代社会面临的唯一冲击，甚至不是唯一的技术冲击，因此无论AI的未来如何，这些政策都将有所帮助。
2.  **有效技术防御和政策制定的先决条件**：这些干预措施通过加强技术和制度能力来支持下一类干预措施。例子包括资助更多关于AI风险的研究、对高风险AI系统开发者的透明度要求、在AI社区建立信任和减少碎片化、增加政府的技术专长、加强AI国际合作以及提高AI素养。⁹⁸ 这些将有助于建立技术和制度能力来缓解AI风险，即使我们对AI当前或未来影响的判断是错误的。
3.  **无论AI未来如何都有帮助的干预措施**：这些包括开发早期预警系统、开发针对已识别AI风险的防御措施、激励防御者（例如网络攻击背景下的软件开发者）采用AI、对研究人员的法律保护、不良事件报告要求和举报人保护。⁹⁹
4.  **促进韧性的干预措施，如果AI是普通技术则有帮助，但可能使控制潜在的超级智能AI变得更困难**：例如促进竞争，包括通过开放模型发布；确保AI广泛用于防御；以及**多中心治理**（polycentricity），即要求监管机构多样化，理想情况下在它们之间引入竞争，而不是让一个监管机构负责所有事情。¹⁰⁰

我们希望即使在对AI风险和未来轨迹持有截然不同信念的专家和利益相关者之间，也能就前三类达成共识。我们建议，目前政策制定者也应谨慎推行最后一类的干预措施，但同时也应提高他们在AI轨迹发生变化时改变方向的准备程度。

### 不扩散（Nonproliferation）难以执行且导致单点故障
不扩散政策试图限制能够获得强大AI能力的参与者数量。例子包括旨在限制国家构建、获取或操作强大AI能力的硬件或软件出口管制、要求获得构建或分发强大AI的许可证，以及禁止开源权重AI模型（因为它们的进一步扩散无法控制）。¹⁰¹

如果我们视未来AI为超级智能，不扩散似乎是一个有吸引力的干预措施，甚至可能是必要的。如果只有少数参与者控制强大的AI，政府可以监控他们的行为。

不幸的是，构建有能力AI模型所需的技术知识已经广泛传播，许多组织分享了他们完整的代码、数据和训练方法。对于资金充足的组织和民族国家来说，即使是训练最先进模型的高昂成本也微不足道；因此，不扩散将需要前所未有的国际协调水平。¹⁰² 此外，算法改进和硬件成本降低不断降低了进入门槛。

强制执行不扩散存在严重的实际挑战。恶意行为者可以简单地忽略许可要求。建议监视训练模型的数据中心，随着训练成本的降低而变得越来越不切实际。¹⁰³ 随着能力的普及，维持有效的限制将需要越来越严厉的措施。

不扩散引入了新的风险：它将减少竞争并增加AI模型市场的**集中度**。当许多下游应用依赖于同一个模型时，该模型的漏洞可能在所有应用中被利用。软件单一文化（monoculture）的网络安全风险的一个典型例子是2000年代针对微软Windows的蠕虫病毒的扩散。¹⁰⁴

依赖不扩散会在面临冲击时产生**脆弱性**，例如模型权重泄露、对齐技术失败或对手获得训练能力。它将注意力从更稳健的防御措施上移开，这些措施侧重于AI风险可能出现的下游攻击面。

不扩散带来的风险不仅仅是单点故障——当开发最先进模型所需的专业知识仅限于少数几家公司时，只有他们的研究人员才能获得安全研究所需的深入访问权限。

许多潜在的AI滥用被用来主张不扩散，包括化学、生物和核威胁，以及网络攻击。

生物武器的风险是真实存在的。由于大型语言模型是通用技术，它们很可能会被生物恐怖分子找到某些用途，就像它们在大多数领域找到用途一样。但这并不使生物恐怖成为**AI风险**——就像它不是**互联网风险**一样，考虑到关于生物武器的信息在网上广泛可用。¹⁰⁵ 无论我们采取何种防御措施来应对现有的生物恐怖主义风险（例如限制获取危险材料和设备），这些措施也将有效对抗AI赋能的生物恐怖主义。

在网络安全领域，正如我们在第三部分讨论的那样，自动化漏洞检测的进步往往**有利于防御者**而非攻击者。除非这种攻防平衡发生变化，否则试图限制这些能力的扩散将适得其反。

长期以来一直有人认为，政府在许多文明风险领域（如大流行病预防）的投资严重不足。如果坏人利用AI利用这些现有漏洞的可能性增加了解决这些问题的紧迫性，那将是一个好结果。但将现有风险重新定义为AI风险，并优先考虑针对AI的缓解措施，将是**极其适得其反**的。

**不扩散是一种心态，而不仅仅是一种政策干预。**¹⁰⁶ 这种心态可以被模型和下游开发者、部署者以及个人采纳。它不仅涉及对技术的访问集中化，还涉及对其的控制集中化。考虑AI系统控制权位置的层级（从集中到分散）：政府、模型开发者、应用开发者、部署者和最终用户。在不扩散心态下，控制在可能的最高（最集中）级别行使，而在韧性心态下，通常在可能的最低级别行使。

以下是基于不扩散的干预措施示例：

*   通过“遗忘”技术从模型中移除双重用途能力。
*   限制下游开发者微调模型的能力。
*   委托AI模型和系统本身自主做出安全决策，理由是它们被训练来遵守集中的安全策略，而部署者/用户不被信任这样做。
*   提高AI系统对情境、资源和敏感数据的访问级别，理由是这使它们能够做出更好的安全决策（例如，访问用户的网络搜索历史可能让聊天机器人更好地判断请求背后的意图是否恶意）。
*   开发由开发者控制并在传统组织之外并行运作的“AI组织”（具有高层级组织复杂性的多代理系统），而不是将AI代理整合到现有组织中。

除少数例外情况外，我们认为基于不扩散的安全措施会**降低韧性**，从而长期来看会**恶化**AI风险。¹⁰⁷ 它们导致的设计和实施选择可能在**权力**意义上促成超级智能——提高自主性、组织能力、资源访问权限等。矛盾的是，它们增加了它们本意要防御的风险。

### 实现AI的益处
普通技术观点的一个重要结果是，进步并非自动发生——AI扩散存在许多障碍。正如Jeffrey Ding所示，在整个经济中扩散创新的能力在国家之间差异很大，并对其整体实力和经济增长产生重大影响。¹⁰⁸ 作为扩散如何成为瓶颈的一个例子，回想一下上面描述的工厂电气化的例子。政策可以减轻或加剧这些障碍。

实现AI的益处需要实验和重构。对这些需求不敏感的监管有可能扼杀有益的AI采纳。监管倾向于创建或固化类别，因此可能过早地冻结商业模式、组织形式、产品类别等等。以下是一些例子：

*   将某些领域（如保险、福利裁决或招聘）归类为“高风险”可能是一个类别错误，因为一个领域内任务风险的变化可能远大于跨领域的变化。¹⁰⁹ 同一领域的任务可能从自动化决策（高风险）到光学字符识别（相对无害）。此外，AI的扩散肯定会创造出我们尚未设想的新任务，这些任务可能会被监管过早地错误分类。
*   AI供应链正在迅速变化。基础模型的兴起导致模型开发者、下游开发者和部署者（以及许多其他类别）之间的区别更加清晰。对这些区别不敏感的监管有可能使模型开发者承担与特定部署情境相关的风险缓解责任，由于基础模型的通用性和所有可能部署情境的不可知性，这将是他们无法承担的。
*   当监管对完全自动化和非完全自动化的决策做出二元区分，并且不承认监督程度时，它会抑制采用新的AI控制模型。正如我们上面讨论的，对于如何在每个决策中不让人类参与的情况下进行有效的人类监督，提出了许多新模型。将自动化决策定义得使这些方法承担与完全没有监督的系统相同的合规负担是不明智的。

需要明确的是，**监管与扩散**是一个**虚假的权衡**，就像**监管与创新**一样。¹¹⁰ 以上例子都不是反对监管的论点；它们只说明了需要细致和灵活性。

此外，监管在**促进扩散**方面可以发挥关键作用。作为一个历史例子，美国2000年的《电子签名法案》（ESIGN Act）在促进数字化和电子商务方面发挥了重要作用：确保电子签名和记录具有法律效力，有助于建立对数字交易的信任。¹¹¹

在AI领域，也有许多促进扩散的监管机会。举个例子，将新闻和媒体内容整合到聊天机器人和其他AI界面中，受到媒体组织对AI公司合理警惕的限制。迄今为止达成的许多AI与新闻业的交易，由于AI公司与出版商之间的权力不对称以及后者无法集体谈判而具有剥削性。存在各种强制性谈判并辅以监管监督的模型。¹¹² （可以说，此类监管的一个更重要原因是为了保护出版商的利益，我们将在下面重新讨论）。

在存在法律或监管不确定性的领域，监管可以促进扩散。责任法如何适用于AI通常不明确。例如，小型无人机就是这种情况，直到美国联邦航空管理局（FAA）在2016年对这个新兴行业进行了监管，建立了明确的规则和要求。由此产生的清晰度刺激了采纳，并导致注册无人机、认证飞行员以及跨不同行业用例数量的快速增长。¹¹³

除了政府作为监管者的角色，促进AI扩散的一个强有力策略是投资于**自动化的补充品**（complements of automation），即随着自动化程度提高而变得更有价值或更必要的事物。一个例子是促进AI素养以及公共和私营部门的劳动力培训。另一个例子是数字化和开放数据，特别是开放政府数据，这可以让AI用户受益于以前无法访问的数据集。私营部门可能在这些领域投资不足，因为它们是人人都能受益的公共产品。能源基础设施的改进，例如电网的可靠性，将促进AI创新和扩散，因为它有助于AI训练和推理。

政府在将AI的收益重新分配以使其更公平，以及补偿那些可能因自动化而受损的人方面也发挥着重要作用。加强社会安全网将有助于降低目前许多国家公众对AI的高度焦虑。¹¹⁴ 艺术和新闻业是重要的生活领域，已受到AI的损害。政府应考虑通过向AI公司征税来资助它们。

最后，政府应在公共部门采纳AI方面取得微妙的平衡。行动过快将导致信任和合法性的丧失，就像纽约市那个显然测试不足并因告诉企业违法而上头条的聊天机器人一样。¹¹⁵ 美国政府效率部（DOGE）对AI的使用包括许多可疑的应用。¹¹⁶ 但行动过慢可能意味着基本的政府职能被外包给私营部门，而在私营部门实施时问责制较少。¹¹⁷

例如，税收和福利等领域规则的复杂性意味着人们经常向聊天机器人寻求指导，而政府目前由于对所涉风险可以理解的谨慎而在提供此类服务方面远远落后。¹¹⁸

但行政国家对这些风险的处理方式过于谨慎，被Nicholas Bagley描述为“程序迷恋”（procedure fetish），可能导致“失控的官僚主义”（runaway bureaucracy）。¹¹⁹ 除了错失AI的好处，Bagley警告说，不称职的表现将导致政府机构失去他们试图通过强调程序和问责制来获得的合法性。

---

## 最终思考
作为普通技术的AI是一种世界观，与作为即将到来的超级智能的AI的世界观形成对比。世界观由其假设、词汇、证据解释、认知工具、预测和（可能）价值观构成。这些因素相互强化，并在每个世界观内部形成一个紧密的整体。

例如，我们假设，尽管AI与过去的技术之间存在明显差异，但它们足够相似，以至于我们应该预期既定模式（例如扩散理论）适用于AI，除非有具体的相反证据。

词汇差异可能是有害的，因为它们可能隐藏潜在的假设。例如，我们拒绝了某些对于普遍理解的超级智能概念有意义所必需的假设。

关于AI未来的分歧通常部分源于对当前证据的不同解释。例如，我们强烈反对将生成式AI的采纳描述为快速的（这强化了我们关于AI扩散与过去技术相似性的假设）。

在认知工具方面，我们不强调概率预测，而是强调在从过去推断未来时，需要分解我们所说的AI（通用性水平、方法进展与应用开发与扩散等）的含义。

我们相信，我们世界观的某种版本被广泛持有。不幸的是，它没有被明确阐述，也许是因为持有这种观点的人可能觉得它是默认的，阐述它似乎是多余的。然而，随着时间的推移，超级智能的观点在AI讨论中占据了主导地位，以至于沉浸其中的人可能无法认识到存在另一种连贯的方式来概念化AI的现在和未来。因此，可能很难认识到为什么不同的人可能真诚地对AI的进展、风险和政策持有截然不同的看法。我们希望这篇论文能在促进更深入的相互理解方面发挥一点作用，即使它不改变任何信念。

---

## 致谢
我们深深感谢Gillian Hadfield、Seth Lazar以及我们的匿名同行评审员在Knight First Amendment Institute关于AI与民主自由研讨会期间及之后对我们论文提出的详细意见。我们也感谢研讨会的参与者，包括Alex Abdo、Borhane Blili-Hamelin、Kevin Feng、Henry Farrell、Katy Glenn-Bass、Atoosa Kasirzadeh、Sydney Levine、Nik Marda、Deirdre Mulligan和Daniel Susskind。我们很幸运收到了许多其他人士对草稿的反馈，包括Shazeda Ahmed、Dean Ball、Nicholas Carlini、Alan Chan、Ajeya Cotra、Justin Curl、Jeffrey Ding、Benjamin Edelman、Jobst Heitzig、Noam Kolt、Mihir Kshirsagar、Timothy B. Lee、Steve Newman、David Robinson、Matthew Salganik、Zachary Siegel、Ollie Stephenson和Zach Vertin。我们感谢Shira Minsk和Mandy Soulsby-Bodart提供的编辑支持。最后，我们感谢澳大利亚国立大学MINT实验室成员以及普林斯顿大学“预测的局限性”课程学生的反馈。

---

## 参考文献

- [1] Nick Bostrom. 2012. The superintelligent will: Motivation and instrumental rationality in advanced artificial agents. Minds and Machines 22, 2 (May 2012), 71–85; Nick Bostrom. 2017. Superintelligence: Paths, Dangers, Strategies (reprinted with corrections). Oxford University Press, Oxford, United Kingdom; Sam Altman, Greg Brockman, and Ilya Sutskever. 2023. Governance of Superintelligence (May 2023); Shazeda Ahmed et al. 2023. Building the Epistemic Community of AI Safety. SSRN: Rochester, NY.
- [2] This is different from the question of whether it is helpful for an individual user to conceptualize a specific AI system as a tool as opposed to a human-like entity such as an intern, a co-worker, or a tutor.
- [3] Daron Acemoglu and Simon Johnson. 2023. Power and Progress: Our Thousand-Year Struggle over Technology and Prosperity. PublicAffairs, New York, NY.
- [4] Jeffrey Ding. 2024. Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition. Princeton University Press, Princeton.
- [5] Angelina Wang et al. 2023. Against predictive optimization: On the legitimacy of decision-making algorithms that optimize predictive accuracy. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA: ACM, 2023), 626–26.
- [6] Casey Ross. 2022. Epic’s Overhaul of a Flawed Algorithm Shows Why AI Oversight Is a Life-or-Death Issue. STAT.
- [7] Andrew Wong et al. 2021. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. JAMA Internal Medicine 181, 8 (August 2021), 1065–70.
- [8] Kevin Roose. 2023. A Conversation With Bing’s Chatbot Left Me Deeply Unsettled. The New York Times (February 2023).
- [9] Dan Milmo and Alex Hern. 2024. ‘We definitely messed up’: why did Google AI tool make offensive historical images? The Guardian (March 2024).
- [10] Jamie Bernardi et al. 2024. Societal adaptation to advanced AI. arXiv: May 2024; Center for Devices and Radiological Health. 2024. Regulatory evaluation of new artificial intelligence (AI) uses for improving and automating medical practices. FDA (June 2024); “Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down Harmonised Rules on Artificial Intelligence and Amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA Relevance),” June 2024.
- [11] Javier Espinoza. 2024. Europe’s rushed attempt to set the rules for AI. Financial Times (July 2024); Daniel E. Ho and Nicholas Bagley. 2024. Runaway bureaucracy could make common uses of ai worse, even mail delivery. The Hill (January 2024).
- [12] Avanidhar Subrahmanyam. 2013. Algorithmic trading, the flash crash, and coordinated circuit breakers. Borsa Istanbul Review 13, 3 (September 2013), 4–9.
- [13] Alexander Bick, Adam Blandin, and David J. Deming. 2024. The Rapid Adoption of Generative AI. National Bureau of Economic Research.
- [14] Alexander Bick, Adam Blandin, and David J. Deming. 2024. The Rapid Adoption of Generative AI. National Bureau of Economic Research.
- [15] Benedict Evans. 2023. AI and the Automation of Work; Benedict Evans, 2023; Jeffrey Ding. 2024. Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition. Princeton University Press, Princeton.
- [16] Paul A. David. 1990. The dynamo and the computer: an historical perspective on the modern productivity paradox. The American Economic Review 80, 2 (1990), 355–61; Tim Harford. 2017. Why didn’t electricity immediately change manufacturing? (August 2017).
- [17] Robert Solow as quoted in Paul A. David. 1990. The dynamo and the computer: an historical perspective on the modern productivity paradox. The American Economic Review 80, 2 (1990), Page 355; Tim Harford. 2017. Why didn’t electricity immediately change manufacturing? (August 2017).
- [18] Arvind Narayanan and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton University Press, Princeton, NJ.
- [19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems 25 (2012); Harris Drucker, Donghui Wu, and Vladimir N. Vapnik. 1999. Support vector machines for spam categorization. IEEE Transactions on Neural Networks 10, 5 (September 1999), 1048–54; William D. Smith. 1964. New I.B.M, System 360 can serve business, science and government; I.B.M. Introduces a computer it says tops output of biggest. The New York Times April 1964; Special to THE NEW YORK TIMES. Algebra machine spurs research calling for long calculations; Harvard receives today device to solve in hours problems taking so much time they have never been worked out. The New York Times (August 1944); Herman Hollerith. 1894. The electrical tabulating machine. Journal of the Royal Statistical Society 57, 4 (December 1894), 678.
- [20] Mohammad Musa, Tim Dawkins, and Nicola Croce. 2019. This is the next step on the road to a safe self-driving future. World Economic Forum (December 2019); Louise Zhang. 2023. Cruise’s Safety Record Over 1 Million Driverless Miles. Cruise (April 2023).
- [21] Arvind Narayanan and Sayash Kapoor. 2024. AI companies are pivoting from creating gods to building products. Good. AI Snake Oil newsletter.
- [22] Rich Sutton. 2019. The Bitter Lesson (March 2019).
- [23] Arvind Narayanan and Sayash Kapoor. 2024. AI companies are pivoting from creating gods to building products. Good. AI Snake Oil newsletter.
- [24] Melanie Mitchell. 2021. Why AI is harder than we think. arXiv preprint (April 2021).
- [25] Josh Achiam et al. 2023. GPT-4 technical report. arXiv preprint arXiv: 2303.08774; Peter Henderson et al. 2024. Rethinking machine learning benchmarks in the context of professional codes of conduct. In Proceedings of the Symposium on Computer Science and Law; Varun Magesh et al. 2024. Hallucination-free? Assessing the reliability of leading AI legal research tools. arXiv preprint arXiv: 2405.20362; Daniel N. Kluttz and Deirdre K. Mulligan. 2019. Automated decision support technologies and the legal profession. Berkeley Technology Law Journal 34, 3 (2019), 853–90; Inioluwa Deborah Raji, Roxana Daneshjou, and Emily Alsentzer. 2025. It’s time to bench the medical exam benchmark. NEJM AI 2, 2 (2025).
- [26] Sayash Kapoor, Peter Henderson, and Arvind Narayanan. Promises and pitfalls of artificial intelligence for legal applications. Journal of Cross-Disciplinary Research in Computational Law 2, 2 (May 2024), Article 2.
- [27] Hamel Husain, Isaac Flath, and Johno Whitaker. Thoughts on a month with Devin. Answer.AI (2025).
- [28] Ehud Reiter. 2025. Do LLM Coding Benchmarks Measure Real-World Utility?.
- [29] Deborah Raji et al. 2021. AI and the everything in the whole wide world benchmark. In Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks, vol. 1.; Rachel Thomas and David Uminsky. 2020. The problem with metrics is a fundamental problem for AI. arXiv preprint.
- [30] Ashwin Nayak et al. 2023. Comparison of history of present illness summaries generated by a chatbot and senior internal medicine residents. JAMA Internal Medicine 183, 9 (September 2023), 1026–27; Shakked Noy and Whitney Zhang. 2023. Experimental evidence on the productivity effects of generative artificial intelligence. Science 381, 6654 (July 2023), 187–92; Fabrizio Dell’Acqua et al., “Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality,” Harvard Business School Technology & Operations Mgt. Unit Working Paper, no. 24–13 (2023).
- [31] Pranshu Verma and Gerrit De Vynck. 2023. ChatGPT took their jobs. Now they walk dogs and fix air conditioners. Washington Post (June 2023).
- [32] Metaculus. 2024. Will there be human-machine intelligence parity before 2040?.
- [33] Mario Krenn et al. 2023. Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Nature Machine Intelligence 5, 11 (2023), 1326–35.
- [34] Johan S.G. Chu and James A. Evans. 2021. Slowed Canonical Progress in Large Fields of Science. Proceedings of the National Academy of Sciences 118, 41 (2021), e2021636118.
- [35] Timothy B. Lee. 2024. Predictions of AI doom are too much like Hollywood movie plots. https://www.understandingai.org/p/predictions-of-ai-doom-are-too-much
- [36] Celeste Biever. 2023. ChatGPT broke the Turing Test — The race is on for new ways to assess AI. Nature 619, 7971 (July 2023), 686–89; Melanie Mitchell. 2024. The Turing Test and our shifting conceptions of intelligence. Science 385, 6710 (2024), eadq9356.
- [37] John McCarthy, Marvin L. Minsky, Nathaniel Rochester and Claude E. Shannon. 1955. A proposal for the dartmouth summer research project on artificial intelligence.
- [38] Changmao Li and Jeffrey Flanigan. 2023. Task contamination: Language models may not be few-shot anymore. arXiv: December 2023.
- [39] Luke Muehlhauser. 2013. Plenty of room above us. In Facing the Intelligence Explosion.
- [40] Melanie Mitchell et al. 2024. Ep. 1: What is intelligence? Complexity. Santa Fe Institute; Podcast episode; Melanie Mitchell. 2019. Opinion. We shouldn’t be scared by ‘Superintelligent A.I.’ The New York Times (October 2019).
- [41] Matthew J Salganik et al. 2020. Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the National Academy of Sciences 117, 15 (2020), 8398–8403.
- [42] Mary Phuong et al. 2024. Evaluating frontier models for dangerous capabilities. arXiv: April 2024. Page 5.
- [43] Mary Phuong et al. 2024. Evaluating frontier models for dangerous capabilities. arXiv: April 2024.
- [44] Arvind Narayanan, Sayash Kapoor, and Seth Lazar. 2024. Model alignment protects against accidental harms, not intentional ones.
- [45] Raja Parasuraman and Dietrich H. Manzey. 2010. Complacency and bias in human use of automation: An attentional integration. Human Factors 52, 3 (June 2010), 381–410.
- [46] Roel I. J. Dobbe. 2022. System safety and artificial intelligence. In The Oxford Handbook of AI Governance, ed. Justin B. Bullock et al., Oxford University Press, Oxford.
- [47] CodeMetal.ai. 2024. Combining AI with formal verification for efficient migration of legacy code.
- [48] Balint Gyevnar and Atoosa Kasirzadeh. 2025. AI safety for everyone. arXiv preprint arXiv: 2502.09288.
- [49] Balint Gyevnar and Atoosa Kasirzadeh. 2025. AI safety for everyone. arXiv preprint arXiv: 2502.09288; Tinghao Xie et al. 2024. SORRY-Bench: Systematically evaluating large language model safety refusal behaviors. arXiv: June 2024; Alan Chan et al. 2024. Visibility into AI agents. arXiv preprint arXiv:2401.13138; Yonadav Shavit et al. 2023. Practices for governing agentic AI systems.
- [50] Karen Levy. 2022. Data Driven: Truckers, Technology, and the New Workplace Surveillance. Princeton University Press, Princeton, NJ.
- [51] Andrew J. Hawkins. 2024. Waymo thinks it can overcome robotaxi skepticism with lots of safety data. The Verge; Caleb Miller. 2024. General motors gives up on its cruise robotaxi dreams. Car and Driver (December 2024); Greg Bensinger. 2021. Why Tesla’s ‘Beta Testing’ Puts the Public at Risk. The New York Times (July 2021); Andrew J. Hawkins. 2020. Uber’s fraught and deadly pursuit of self-driving cars is over. The Verge.
- [52] Caleb Miller. 2024. General motors gives up on its cruise robotaxi dreams. Car and Driver (December 2024); Andrew J. Hawkins. 2020. Uber’s fraught and deadly pursuit of self-driving cars is over. The Verge.
- [53] Jonathan Stempel. 2024. Tesla must face vehicle owners’ lawsuit over self-driving claims. Reuters (May 2024).
- [54] Hayden Field. 2023. Waymo is full speed ahead as safety incidents and regulators stymie competitor cruise.
- [55] Will Hunt. 2020. The flight to safety-critical AI: Lessons in AI safety from the aviation industry. CLTC White Paper Series. UC Berkeley Center for Long-Term Cybersecurity.
- [56] Arvind Narayanan. 2023. Understanding Social Media Recommendation Algorithms. Knight First Amendment Institute.
- [57] Ralph Nader. 1965. Unsafe at Any Speed: The Designed-in Dangers of the American Automobile. Grossman Publishers, New York, NY.
- [58] Seth Lazar. 2025. Anticipatory AI ethics (manuscript, forthcoming 2025).
- [59] Alex Engler. 2023. The EU and U.S. diverge on AI regulation: A transatlantic comparison and steps to alignment.
- [60] Matt Sheehan. 2023. China’s AI regulations and how they get made.
- [61] Heather Curry, 2024. 2024 state summary on AI. BSA TechPost (October 2024).
- [62] Yuntao Bai et al. 2022. Constitutional AI: Harmlessness from AI feedback. arXiv: December 2022; Long Ouyang et al.. 2022. Training language models to follow instructions with human feedback. arXiv: March 2022.
- [63] Eugene Bagdasaryan et al. 2023. Abusing images and sounds for indirect instruction injection in multi-modal LLMs. arXiv: October 2023; Xiangyu Qi et al. 2023. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv: October 2023.
- [64] Arvind Narayanan and Sayash Kapoor. 2024. AI safety is not a model property.
- [65] Erik Jones, Anca Dragan, and Jacob Steinhardt. 2024. Adversaries can misuse combinations of safe models. arXiv: July 2024.
- [66] Arvind Narayanan and Sayash Kapoor. 2024. AI safety is not a model property.
- [67] Google. 2024. Email sender guidelines.
- [68] Craig Marcho. 2024. IE7 - Introducing the phishing filter. Microsoft Tech Community.
- [69] Jennifer Tang, Tiffany Saade, and Steve Kelly. 2024. The implications of artificial intelligence in cybersecurity: shifting the offense-defense balance.
- [70] Dongge Liu et al. 2023. AI-Powered Fuzzing: Breaking the Bug Hunting Barrier. Google Online Security Blog.
- [71] Juan Cambeiro. How AI can help prevent biosecurity disasters. Institute for Progress (July 2023).
- [72] LessWrong. 2008. Squiggle maximizer (formerly “paperclip maximizer”).
- [73] Ryan Greenblatt et al. 2024. Alignment faking in large language models.
- [74] Bowen Baker et al. 2025. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.
- [75] Victoria Krakovna. 2020. Specification gaming: The flip side of AI ingenuity. Google DeepMind (April 2020).
- [76] Simon Dima et al. 2024. Non-maximizing policies that fulfill multi-criterion aspirations in expectation. arXiv: August 2024.
- [77] Daron Acemoglu and Simon Johnson. 2023. Power and Progress. PublicAffairs.
- [78] Atoosa Kasirzadeh. 2024. Two types of AI existential risk: Decisive and accumulative. arXiv: preprint (February 2024).
- [79] Anton Leicht. 2024. AI safety politics after the SB-1047 veto.
- [80] Timothy B. Lee. 2024. Six Principles for Thinking about AI Risk.
- [81] Mary Phuong et al. 2024. Evaluating frontier models for dangerous capabilities.
- [82] Shazeda Ahmed et al. 2024. Field-building and the epistemic culture of AI safety. First Monday 29, 4.
- [83] Arvind Narayanan and Sayash Kapoor. 2024. AI existential risk probabilities are too unreliable to inform policy; Neel Guha et al. 2023. AI regulation has its own alignment problem: The technical and institutional feasibility of disclosure, registration, licensing, and auditing. SSRN (November 2023).
- [84] Christopher A. Mouton, Caleb Lucas, and Ella Guest. 2024. The operational risks of AI in large-scale biological attacks: Results of a red-team study. RAND Corporation; Ari Takanen, Jared D. Demott, and Charles Miller. 2008. Fuzzing for Software Security Testing and Quality Assurance. Fuzzing for Software Security (1st ed.). Artech House Publishers, Norwood, MA.
- [85] Sayash Kapoor and Arvind Narayanan. 2023. Licensing is neither feasible nor effective for addressing AI risks (June 2023).
- [86] Arvind Narayanan and Sayash Kapoor. 2024. AI existential risk probabilities are too unreliable to inform policy.
- [87] Richard Blumenthal and Josh Hawley. 2023. Bipartisan framework for U.S. AI act.
- [88] Sigal Samuel. 2022. Effective altruism’s most controversial idea.
- [89] Kevin Vallier. 1996. Public justification.
- [90] Jeffrey A Friedman and Richard Zeckhauser. 2018. Analytic confidence and political decision-making: Theoretical principles and experimental evidence from national security professionals. Political Psychology 39, 5 (2018), 1069–87.
- [91] Arvind Narayanan and Sayash Kapoor. 2023. Generative AI companies must publish transparency reports. Knight First Amendment Institute; Executive Office of the President. 2020. Promoting the use of trustworthy artificial intelligence in the federal government, 2020; Justin Colannino. 2021. The copyright office expands your security research rights. GitHub Blog.
- [92] AI Incident Database. https://incidentdatabase.ai/.
- [93] Stephen Casper, David Krueger, and Dylan Hadfield-Menell. 2025. Pitfalls of evidence-based AI policy.
- [94] Sayash Kapoor et al. 2024. On the societal impact of open foundation models.
- [95] Gary E. Marchant and Yvonne A. Stevens. 2017. Resilience: A new tool in the risk governance toolbox for emerging technologies. UC Davis Law Review.
- [96] Brian Walker et al. 2006. A handful of heuristics and some propositions for understanding resilience in social-ecological systems. Ecology and Society 11, 1 (2006).
- [97] Gary E. Marchant and Yvonne A. Stevens. 2017. Resilience: A new tool in the risk governance toolbox for emerging technologies. UC Davis Law Review.
- [98] Rishi Bommasani et al. 2024. A path for science- and evidence-based AI policy; Balint Gyevnar and Atoosa Kasirzadeh. 2025. AI safety for everyone; Anka Reuel et al. 2024. Position: Technical research and talent is needed for effective AI governance. In Proceedings of the 41st International Conference on Machine Learning (PMLR, 2024), 42543–57.
- [99] The National Artificial Intelligence Advisory Committee (NAIAC). 2023. Improve monitoring of emerging risks from AI through adverse event reporting. (November 2023); Shayne Longpre et al. 2024. A safe harbor for AI evaluation and red teaming (March 2024); Jamie Bernardi et al. 2025. Societal adaptation to advanced AI. Helen Toner. 2024. Oversight of AI: Insiders’ perspectives (September 2024).
- [100] Sayash Kapoor and Rishi Bommasani et al. 2024. On the societal impact of open foundation models; Rishi Bommasani et al. 2024. Considerations for Governing Open Foundation Models. Science 386, 6718 (October 2024), 151–53; Gary E. Marchant and Yvonne A. Stevens. 2017. Resilience; Noam Kolt. 2024. Algorithmic black swans. Washington University Law Review.
- [101] Richard Blumenthal and Josh Hawley. 2023. Bipartisan framework for U.S. AI act; Josh Hawley. 2025. Decoupling America’s artificial intelligence capabilities from China Act of 2025. Pub. L. No. S 321 (2025).
- [102] Sayash Kapoor and Arvind Narayanan. 2023. Licensing is neither feasible nor effective for addressing AI risks.
- [103] Eliezer Yudkowsky. 2023. Pausing AI developments isn’t enough. we need to shut it all down. (March 2023).
- [104] Reuters. 2005. New Internet worm targeting Windows. NBC News (August 2005).
- [105] Christopher A. Mouton, Caleb Lucas, and Ella Guest. 2024. The operational risks of AI in large-scale biological attacks.
- [106] Dan Hendrycks, Eric Schmidt, and Alexandr Wang. 2025. Superintelligence strategy: Expert version. arXiv: preprint arXiv:2503.05628.
- [107] Emanuel Maiberg. 2024. Apple removes nonconsensual AI nude apps following 404 Media investigation.
- [108] Jeffrey Ding. 2024. Technology and the Rise of Great Powers: How Diffusion Shapes Economic Competition. Princeton University Press, Princeton..
- [109] Olivia Martin et al. 2024, The spectrum of AI integration: The case of benefits adjudication. In Artificial Intelligence: Legal Issues, Policy & Practical Strategies, Cynthia H. Cwik (ed.).
- [110] Anu Bradford. The false choice between digital regulation and innovation. Nw. UL Rev. 119 (2024), 377.
- [111] Scott R. Zemnick. 2001. The E-Sign Act: The Means to Effectively Facilitate the Growth and Development of E-commerce. Chicago-Kent Law Review (April 2001). https://scholarship.kentlaw.iit.edu/cgi/viewcontent.cgi?article=3342&context=cklawreview.
- [112] Benjamin Brooks. 2024. AI search could break the web. MIT Technology Review (October 2024).
- [113] Drones Are Here to Stay. Get Used to It. 2018. Time (May 2018).
- [114] Ipsos. 2024. The Ipsos AI Monitor 2024: Changing attitudes and feelings about AI and the future it will bring.
- [115] Colin Lecher. 2024. NYC’s AI chatbot tells businesses to break the law. The Markup.
- [116] Courtney Kube et al. 2025. DOGE will use AI to assess the responses of federal workers who were told to justify their jobs via email. NBC News (February 2025); Dell Cameron. 2025. Democrats demand answers on DOGE’s use of AI.
- [117] Dean W. Ball. 2021. How California turned on its own citizens.
- [118] Kate Dore. 2024. ‘Proceed with caution’ before tapping AI chatbots to file your tax return, experts warn. CNBC (April 2024).
- [119] Nicholas Bagley. 2021. The procedure fetish. Niskanen Center; Daniel E. Ho and Nicholas Bagley. 2024. Runaway bureaucracy could make common uses of AI worse, even mail delivery.